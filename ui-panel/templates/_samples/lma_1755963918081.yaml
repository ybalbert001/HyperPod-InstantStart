apiVersion: sagemaker.amazonaws.com/v1
kind: HyperPodPyTorchJob
metadata:
  labels:
    app.kubernetes.io/name: HyperPod
    app.kubernetes.io/managed-by: kustomize
    app: lmf-v2
  name: lmf-v2
spec:
  nprocPerNode: "4"
  replicaSpecs:
    - name: pods
      replicas: 1
      template:
        spec:
          serviceAccountName: mlflow-service-account
          # serviceAccountName: hp-training-operator-pod-manager
          nodeSelector:
            beta.kubernetes.io/instance-type: ml.g6.12xlarge
          containers:
            - name: pytorch
              image: 633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op-v2:latest
              imagePullPolicy: Always
              ports:
                - containerPort: 8080 # HyperPodElasticAgent port
              resources:
                requests:
                  nvidia.com/gpu: 4
                  vpc.amazonaws.com/efa: 1
                limits:
                  nvidia.com/gpu: 4
                  vpc.amazonaws.com/efa: 1
              env:
              - name: LOGLEVEL
                value: "INFO"
              - name: FI_PROVIDER
                value: "efa"
              # - name: TORCH_DISTRIBUTED_DEBUG
              #   value: "DETAIL"
              # - name: TORCH_NCCL_ENABLE_MONITORING
              #   value: "1"
              # - name: TORCH_NCCL_TRACE_BUFFER_SIZE
              #   value: "20000"
              - name: NCCL_DEBUG
                value: "DEBUG"
              # - name: NCCL_SOCKET_IFNAME
              #   value: "^lo"
              - name: NCCL_SOCKET_IFNAME
                value: "eth0"
              # - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
              #   value: "1"
              # - name: HF_TOKEN
              #   value: "12345"
              - name: MLFLOW_TRACKING_URI
                value: arn:aws:sagemaker:us-west-2:633205212955:mlflow-tracking-server/pdx-mlflow
              - name: MLFLOW_EXPERIMENT_NAME
                value: lmf-v2
              - name: MLFLOW_TAG_NPROCPERNODE
                value: "4"
              - name: MLFLOW_TAG_REPLICAS
                value: "1"
              - name: MLFLOW_TAG_EFAPERNODE
                value: "1"
              - name: MLFLOW_TAG_INSTANCETYPE
                value: ml.g6.12xlarge
              - name: LMF_RECIPE_RUN_PATH
                value: /s3/train-recipes/llama-factory-project/
              - name: LMF_RECIPE_YAML_FILE
                value: qwen_full_dist_template.yaml
              command: ["bash", "/docker_workspace/lmf_recipe_dist_run.sh"]
              volumeMounts:
                - name: persistent-storage-s3
                  mountPath: /s3
                  readOnly: false
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local
                - name: inst-nvme
                  mountPath: /ckpt-path
                - name: local-cache
                  mountPath: /root/.cache
                # - name: persistent-storage
                #   mountPath: /dfsx
          volumes:
            - name: shmem
              hostPath: 
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
            - name: local-cache
              hostPath:
                path: /opt/dlami/nvme/.cache
            - name: inst-nvme
              hostPath:
                path: /opt/dlami/nvme/checkpoints/
            # - name: persistent-storage
            #   persistentVolumeClaim:
            #     claimName: fsx-claim
            - name: persistent-storage-s3
              persistentVolumeClaim:
                claimName: s3-claim
  runPolicy:
    jobMaxRetryCount: 5
    restartPolicy:
      numRestartBeforeFullJobRestart: 3
      evalPeriodSeconds: 21600
      maxFullJobRestarts: 1
    cleanPodPolicy: "All"
