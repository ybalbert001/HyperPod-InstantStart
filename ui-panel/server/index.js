const express = require('express');
const cors = require('cors');
const WebSocket = require('ws');
const { exec, spawn } = require('child_process');
const fs = require('fs-extra');
const YAML = require('yaml');
const path = require('path');
const https = require('https');
const http = require('http');

// å¼•å…¥å·¥å…·æ¨¡å—
const HyperPodDependencyManager = require('./utils/hyperPodDependencyManager');

// å¼•å…¥é›†ç¾¤çŠ¶æ€V2æ¨¡å—
const { 
  handleClusterStatusV2, 
  handleClearCache, 
  handleCacheStatus 
} = require('./clusterStatusV2');

// å¼•å…¥åº”ç”¨çŠ¶æ€V2æ¨¡å—
const {
  handlePodsV2,
  handleServicesV2,
  handleAppStatusV2,
  handleClearAppCache,
  handleAppCacheStatus
} = require('./appStatusV2');

const app = express();
const PORT = 3001;
const WS_PORT = 8081; // æ”¹ä¸º8081é¿å…ç«¯å£å†²çª

app.use(cors());
app.use(express.json());

// WebSocketæœåŠ¡å™¨ç”¨äºå®æ—¶æ›´æ–°
const wss = new WebSocket.Server({ port: WS_PORT });

// å­˜å‚¨æ´»è·ƒçš„æ—¥å¿—æµ
const activeLogStreams = new Map();

// æ—¥å¿—å­˜å‚¨é…ç½® - ç®€åŒ–è·¯å¾„ç»“æ„
const LOGS_BASE_DIR = path.join(__dirname, '..', 'logs');

// ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨ä»»åŠ¡å
function ensureLogDirectory(jobName, podName) {
  const jobLogDir = path.join(LOGS_BASE_DIR, jobName);
  if (!fs.existsSync(jobLogDir)) {
    fs.mkdirSync(jobLogDir, { recursive: true });
  }
  return path.join(jobLogDir, `${podName}.log`);
}

// å¹¿æ’­æ¶ˆæ¯ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
function broadcast(data) {
  wss.clients.forEach(client => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(JSON.stringify(data));
    }
  });
}

// ä¼˜åŒ–é”™è¯¯æ¶ˆæ¯çš„å‡½æ•°
function optimizeErrorMessage(errorMessage) {
  if (!errorMessage) return 'Unknown error';
  
  // å¦‚æœæ˜¯è·å–hyperpodpytorchjobä½†èµ„æºç±»å‹ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
  if (errorMessage.includes(`doesn't have a resource type "hyperpodpytorchjob"`)) {
    return 'No HyperPod training jobs found (HyperPod operator may not be installed)';
  }
  // å¦‚æœæ˜¯è·å–rayjobä½†èµ„æºç±»å‹ä¸å­˜åœ¨
  if (errorMessage.includes(`doesn't have a resource type "rayjob"`)) {
    return 'No RayJobs found (Ray operator may not be installed)';
  }
  // å¦‚æœæ˜¯èµ„æºä¸å­˜åœ¨ï¼Œä½¿ç”¨æ›´å‹å¥½çš„æ¶ˆæ¯
  if (errorMessage.includes('not found') || errorMessage.includes('NotFound')) {
    return 'Resource not found - this may be normal if no resources have been created yet';
  }
  // å¦‚æœæ˜¯è¿æ¥é—®é¢˜
  if (errorMessage.includes('connection refused') || errorMessage.includes('unable to connect')) {
    return 'Unable to connect to Kubernetes cluster. Please check if the cluster is accessible.';
  }
  
  return errorMessage;
}

// æ‰§è¡Œkubectlå‘½ä»¤çš„è¾…åŠ©å‡½æ•° - ç®€åŒ–ç‰ˆé”™è¯¯ä¼˜åŒ–
function executeKubectl(command, timeout = 30000) { // é»˜è®¤30ç§’è¶…æ—¶
  return new Promise((resolve, reject) => {
    console.log(`Executing kubectl command: kubectl ${command}`);
    
    const child = exec(`kubectl ${command}`, { timeout }, (error, stdout, stderr) => {
      if (error) {
        console.error(`kubectl command failed: kubectl ${command}`);
        console.error(`Error details:`, error);
        console.error(`Stderr:`, stderr);
        
        if (error.code === 'ETIMEDOUT') {
          console.error(`kubectl command timed out after ${timeout}ms: ${command}`);
          reject(new Error(`Command timed out after ${timeout/1000} seconds. The cluster may be slow to respond.`));
        } else {
          const errorMessage = error.message || stderr || 'Unknown kubectl error';
          
          // é’ˆå¯¹ç‰¹å®šæƒ…å†µä¼˜åŒ–é”™è¯¯æ¶ˆæ¯
          let optimizedMessage = errorMessage;
          
          // å¦‚æœæ˜¯è·å–hyperpodpytorchjobä½†èµ„æºç±»å‹ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
          if (command.includes('get hyperpodpytorchjob') && 
              errorMessage.includes(`doesn't have a resource type "hyperpodpytorchjob"`)) {
            optimizedMessage = 'No HyperPod training jobs found (HyperPod operator may not be installed)';
          }
          // å¦‚æœæ˜¯èµ„æºä¸å­˜åœ¨ï¼Œä½¿ç”¨æ›´å‹å¥½çš„æ¶ˆæ¯
          else if (errorMessage.includes('not found') || errorMessage.includes('NotFound')) {
            optimizedMessage = 'Resource not found - this may be normal if no resources have been created yet';
          }
          // å¦‚æœæ˜¯è¿æ¥é—®é¢˜
          else if (errorMessage.includes('connection refused') || errorMessage.includes('unable to connect')) {
            optimizedMessage = 'Unable to connect to Kubernetes cluster. Please check if the cluster is accessible.';
          }
          
          console.error(`Optimized error message: ${optimizedMessage}`);
          reject(new Error(optimizedMessage));
        }
      } else {
        console.log(`kubectl command succeeded: kubectl ${command}`);
        console.log(`Output:`, stdout);
        resolve(stdout);
      }
    });
    
    // é¢å¤–çš„è¶…æ—¶ä¿æŠ¤
    const timeoutId = setTimeout(() => {
      child.kill('SIGTERM');
      console.error(`Force killing kubectl command after ${timeout}ms: ${command}`);
    }, timeout);
    
    child.on('exit', () => {
      clearTimeout(timeoutId);
    });
  });
}

// ç®€åŒ–çš„æ¨¡å‹æ ‡ç­¾ç”Ÿæˆå‡½æ•°ï¼ˆç”¨äºæ¨¡å‹ä¸‹è½½ï¼‰
function generateModelTag(modelId) {
  if (!modelId) return 'model';
  return modelId
    .toLowerCase()
    .replace(/[^a-z0-9-]/g, '-')
    .replace(/-+/g, '-')
    .replace(/^-|-$/g, '') || 'model';
}

// ç”ŸæˆNLBæ³¨è§£çš„å‡½æ•°
function generateNLBAnnotations(isExternal) {
  if (isExternal) {
    return `
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"`;
  } else {
    return `
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"`;
  }
}

// ç®€åŒ–çš„å‘½ä»¤è§£æå‡½æ•° - ç§»é™¤GPUè‡ªåŠ¨è§£æé€»è¾‘
function parseVllmCommand(vllmCommandString) {
  // ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œå¤„ç†åæ–œæ æ¢è¡Œ
  const cleanCommand = vllmCommandString
    .replace(/\\\s*\n/g, ' ')  // å¤„ç†åæ–œæ æ¢è¡Œ
    .replace(/\s+/g, ' ')      // åˆå¹¶å¤šä¸ªç©ºæ ¼
    .trim();
  
  // åˆ†å‰²å‘½ä»¤ä¸ºæ•°ç»„
  const parts = cleanCommand.split(' ').filter(part => part.trim());
  
  // æ£€æŸ¥å‘½ä»¤æ˜¯å¦ä¸ºç©º
  if (parts.length === 0) {
    throw new Error('Command cannot be empty');
  }
  
  // æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥çš„å‘½ä»¤æ ¼å¼ï¼ˆç”¨äºæ¡†æ¶è¯†åˆ«ï¼‰
  const isVllmCommand = parts.includes('python3') && parts.includes('-m') && parts.includes('vllm.entrypoints.openai.api_server');
  const isVllmServeCommand = parts.includes('vllm') && parts.includes('serve');
  const isSglangCommand = parts.includes('python3') && parts.includes('-m') && parts.includes('sglang.launch_server');
  
  let entrypointIndex = -1;
  
  if (isVllmCommand) {
    entrypointIndex = parts.findIndex(part => part === 'vllm.entrypoints.openai.api_server');
  } else if (isVllmServeCommand) {
    entrypointIndex = parts.findIndex(part => part === 'serve');
  } else if (isSglangCommand) {
    entrypointIndex = parts.findIndex(part => part === 'sglang.launch_server');
  }
  
  const args = entrypointIndex >= 0 ? parts.slice(entrypointIndex + 1) : parts.slice(1);
  
  return {
    fullCommand: parts,
    args: args,
    commandType: (isVllmCommand || isVllmServeCommand) ? 'vllm' : (isSglangCommand ? 'sglang' : 'custom')
  };
}

// æ”¹è¿›çš„HTTPè¯·æ±‚ä»£ç†å‡½æ•°
function makeHttpRequest(url, payload, method = 'POST') {
  return new Promise((resolve, reject) => {
    try {
      const urlObj = new URL(url);
      const isHttps = urlObj.protocol === 'https:';
      const httpModule = isHttps ? https : http;
      
      const isGetRequest = method.toUpperCase() === 'GET';
      const postData = isGetRequest ? '' : JSON.stringify(payload);
      
      const options = {
        hostname: urlObj.hostname,
        port: urlObj.port || (isHttps ? 443 : 80),
        path: urlObj.pathname + urlObj.search,
        method: method.toUpperCase(),
        headers: {
          'User-Agent': 'Model-Deployment-UI/1.0'
        },
        timeout: 30000 // 30ç§’è¶…æ—¶
      };
      
      // åªæœ‰POSTè¯·æ±‚æ‰éœ€è¦Content-Typeå’ŒContent-Length
      if (!isGetRequest) {
        options.headers['Content-Type'] = 'application/json';
        options.headers['Content-Length'] = Buffer.byteLength(postData);
      }
      
      console.log(`Making HTTP ${method} request to: ${url}`);
      console.log(`Request options:`, JSON.stringify(options, null, 2));
      if (!isGetRequest) {
        console.log(`Payload:`, postData);
      }
      
      const req = httpModule.request(options, (res) => {
        let data = '';
        
        console.log(`Response status: ${res.statusCode}`);
        console.log(`Response headers:`, JSON.stringify(res.headers, null, 2));
        
        res.on('data', (chunk) => {
          data += chunk;
        });
        
        res.on('end', () => {
          console.log(`Response data:`, data);
          
          // å¤„ç†ä¸åŒçš„å“åº”çŠ¶æ€
          if (res.statusCode >= 200 && res.statusCode < 300) {
            // æˆåŠŸå“åº”
            try {
              const jsonData = JSON.parse(data);
              resolve({
                success: true,
                status: res.statusCode,
                data: jsonData
              });
            } catch (parseError) {
              // å¦‚æœä¸æ˜¯JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
              console.log('Response is not JSON, returning as text');
              resolve({
                success: true,
                status: res.statusCode,
                data: data,
                isText: true
              });
            }
          } else {
            // é”™è¯¯å“åº”
            try {
              const errorData = JSON.parse(data);
              resolve({
                success: false,
                status: res.statusCode,
                error: errorData.error || `HTTP ${res.statusCode}`,
                data: errorData
              });
            } catch (parseError) {
              resolve({
                success: false,
                status: res.statusCode,
                error: `HTTP ${res.statusCode}: ${data}`,
                data: data
              });
            }
          }
        });
      });
      
      req.on('error', (error) => {
        console.error('HTTP request error:', error);
        reject({
          success: false,
          error: `Network error: ${error.message}`
        });
      });
      
      req.on('timeout', () => {
        console.error('HTTP request timeout');
        req.destroy();
        reject({
          success: false,
          error: 'Request timeout (30s)'
        });
      });
      
      // åªæœ‰éGETè¯·æ±‚æ‰å†™å…¥payload
      if (!isGetRequest && postData) {
        req.write(postData);
      }
      req.end();
      
    } catch (error) {
      console.error('HTTP request setup error:', error);
      reject({
        success: false,
        error: `Request setup error: ${error.message}`
      });
    }
  });
}

// è·å–Pending GPUç»Ÿè®¡
app.get('/api/pending-gpus', async (req, res) => {
  try {
    const pendingPodsOutput = await executeKubectl('get pods --field-selector status.phase=Pending -o json');
    const pendingPodsData = JSON.parse(pendingPodsOutput);
    
    let pendingGPUs = 0;
    if (pendingPodsData.items && Array.isArray(pendingPodsData.items)) {
      pendingGPUs = pendingPodsData.items.reduce((sum, pod) => {
        if (pod.spec?.containers) {
          return sum + pod.spec.containers.reduce((containerSum, container) => {
            const gpuRequest = container.resources?.requests?.['nvidia.com/gpu'];
            return containerSum + (parseInt(gpuRequest) || 0);
          }, 0);
        }
        return sum;
      }, 0);
    }
    
    res.json({ pendingGPUs });
  } catch (error) {
    console.error('Error fetching pending GPUs:', error);
    res.status(500).json({ error: error.message, pendingGPUs: 0 });
  }
});

// è·å–é›†ç¾¤èŠ‚ç‚¹GPUä½¿ç”¨æƒ…å†µ - V2ä¼˜åŒ–ç‰ˆæœ¬
app.get('/api/cluster-status', handleClusterStatusV2);

// é›†ç¾¤çŠ¶æ€ç¼“å­˜ç®¡ç†API
app.post('/api/cluster-status/clear-cache', handleClearCache);
app.get('/api/cluster-status/cache-status', handleCacheStatus);

// ç»Ÿä¸€æ—¥å¿—æµç®¡ç† - é¿å…å†²çª
const unifiedLogStreams = new Map(); // ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ—¥å¿—æµ

// å¯åŠ¨ç»Ÿä¸€æ—¥å¿—æµï¼ˆæ”¯æŒè‡ªåŠ¨æ”¶é›†å’ŒWebSocketæµå¼ä¼ è¾“ï¼‰
function startUnifiedLogStream(jobName, podName, options = {}) {
  const streamKey = `${jobName}-${podName}`;
  const { ws = null, autoCollection = false } = options;
  
  // å¦‚æœå·²ç»æœ‰è¯¥podçš„æ—¥å¿—æµï¼Œæ·»åŠ WebSocketè¿æ¥ä½†ä¸é‡å¯è¿›ç¨‹
  if (unifiedLogStreams.has(streamKey)) {
    const existing = unifiedLogStreams.get(streamKey);
    if (ws && !existing.webSockets.has(ws)) {
      existing.webSockets.add(ws);
      console.log(`Added WebSocket to existing log stream for ${streamKey}`);
      
      // å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: 'log_stream_started',
          jobName: jobName,
          podName: podName,
          timestamp: new Date().toISOString()
        }));
      }
    }
    return;
  }
  
  console.log(`ğŸš€ Starting unified log stream for pod: ${podName} in job: ${jobName} (auto: ${autoCollection})`);
  
  // åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
  const logFilePath = ensureLogDirectory(jobName, podName);
  const logStream = fs.createWriteStream(logFilePath, { flags: 'a' });
  
  // å¯åŠ¨kubectl logså‘½ä»¤
  const logProcess = spawn('kubectl', ['logs', '-f', podName], {
    stdio: ['pipe', 'pipe', 'pipe']
  });
  
  // åˆ›å»ºWebSocketé›†åˆ
  const webSockets = new Set();
  if (ws) {
    webSockets.add(ws);
  }
  
  // å­˜å‚¨ç»Ÿä¸€çš„æ—¥å¿—æµä¿¡æ¯
  unifiedLogStreams.set(streamKey, {
    process: logProcess,
    logStream: logStream,
    webSockets: webSockets,
    jobName: jobName,
    podName: podName,
    autoCollection: autoCollection,
    startTime: new Date().toISOString()
  });
  
  // å¤„ç†æ ‡å‡†è¾“å‡º
  logProcess.stdout.on('data', (data) => {
    const logLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    logStream.write(`[${timestamp}] ${logLine}`);
    
    // å‘é€åˆ°æ‰€æœ‰è¿æ¥çš„WebSocket
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_data',
          jobName: jobName,
          podName: podName,
          data: logLine,
          timestamp: timestamp
        }));
      }
    });
  });
  
  // å¤„ç†é”™è¯¯è¾“å‡º
  logProcess.stderr.on('data', (data) => {
    const errorLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶
    logStream.write(`[${timestamp}] ERROR: ${errorLine}`);
    
    // å‘é€é”™è¯¯åˆ°WebSocket
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_error',
          jobName: jobName,
          podName: podName,
          error: errorLine,
          timestamp: timestamp
        }));
      }
    });
  });
  
  // å¤„ç†è¿›ç¨‹é€€å‡º
  logProcess.on('close', (code) => {
    console.log(`Unified log stream for ${podName} exited with code ${code}`);
    logStream.end();
    
    // é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_stream_closed',
          jobName: jobName,
          podName: podName,
          timestamp: new Date().toISOString()
        }));
      }
    });
    
    unifiedLogStreams.delete(streamKey);
  });
  
  // å¤„ç†è¿›ç¨‹é”™è¯¯
  logProcess.on('error', (error) => {
    console.error(`Unified log stream error for ${podName}:`, error);
    logStream.end();
    
    // é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_stream_error',
          jobName: jobName,
          podName: podName,
          error: error.message,
          timestamp: new Date().toISOString()
        }));
      }
    });
    
    unifiedLogStreams.delete(streamKey);
  });
  
  // å‘é€å¯åŠ¨æˆåŠŸæ¶ˆæ¯
  if (ws && ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify({
      type: 'log_stream_started',
      jobName: jobName,
      podName: podName,
      timestamp: new Date().toISOString()
    }));
  }
}

// ä»ç»Ÿä¸€æ—¥å¿—æµä¸­ç§»é™¤WebSocketè¿æ¥
function removeWebSocketFromLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  const stream = unifiedLogStreams.get(streamKey);
  
  if (stream) {
    stream.webSockets.delete(ws);
    console.log(`Removed WebSocket from log stream ${streamKey}, remaining: ${stream.webSockets.size}`);
    
    // å¦‚æœæ²¡æœ‰WebSocketè¿æ¥ä¸”ä¸æ˜¯è‡ªåŠ¨æ”¶é›†ï¼Œåœæ­¢æ—¥å¿—æµ
    if (stream.webSockets.size === 0 && !stream.autoCollection) {
      console.log(`No more WebSocket connections for ${streamKey}, stopping log stream`);
      stream.process.kill();
      stream.logStream.end();
      unifiedLogStreams.delete(streamKey);
    }
  }
}

// ä¸ºè®­ç»ƒä»»åŠ¡è‡ªåŠ¨å¼€å§‹æ—¥å¿—æ”¶é›†
async function startAutoLogCollectionForJob(jobName) {
  try {
    console.log(`ğŸ” Starting auto log collection for training job: ${jobName}`);
    
    // è·å–è¯¥è®­ç»ƒä»»åŠ¡çš„æ‰€æœ‰pods
    const output = await executeKubectl('get pods -o json');
    const result = JSON.parse(output);
    
    const jobPods = result.items.filter(pod => {
      const labels = pod.metadata.labels || {};
      const ownerReferences = pod.metadata.ownerReferences || [];
      
      return labels['training-job-name'] === jobName || 
             labels['app'] === jobName ||
             ownerReferences.some(ref => ref.name === jobName) ||
             pod.metadata.name.includes(jobName);
    });
    
    // ä¸ºæ¯ä¸ªè¿è¡Œä¸­çš„podå¼€å§‹è‡ªåŠ¨æ—¥å¿—æ”¶é›†
    jobPods.forEach(pod => {
      if (pod.status.phase === 'Running' || pod.status.phase === 'Pending') {
        startUnifiedLogStream(jobName, pod.metadata.name, { autoCollection: true });
      }
    });
    
    console.log(`âœ… Started auto log collection for ${jobPods.length} pods in job ${jobName}`);
  } catch (error) {
    console.error(`âŒ Failed to start auto log collection for job ${jobName}:`, error);
  }
}

// ä¿®æ”¹åŸæœ‰çš„startLogStreamå‡½æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€ç®¡ç†
function startLogStream(ws, jobName, podName) {
  startUnifiedLogStream(jobName, podName, { ws: ws });
}

// ä¿®æ”¹åŸæœ‰çš„stopLogStreamå‡½æ•°
function stopLogStream(ws, jobName, podName) {
  removeWebSocketFromLogStream(ws, jobName, podName);
  
  if (ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify({
      type: 'log_stream_stopped',
      jobName: jobName,
      podName: podName,
      timestamp: new Date().toISOString()
    }));
  }
}

// åº”ç”¨çŠ¶æ€V2 API - ä¼˜åŒ–ç‰ˆæœ¬
app.get('/api/v2/pods', handlePodsV2);
app.get('/api/v2/services', handleServicesV2);
app.get('/api/v2/app-status', handleAppStatusV2);
app.post('/api/v2/app-status/clear-cache', handleClearAppCache);
app.get('/api/v2/app-status/cache-status', handleAppCacheStatus);

// è·å–PodçŠ¶æ€
app.get('/api/pods', async (req, res) => {
  try {
    console.log('Fetching pods...');
    const output = await executeKubectl('get pods -o json');
    const pods = JSON.parse(output);
    console.log('Pods fetched:', pods.items.length, 'pods');
    res.json(pods.items);
  } catch (error) {
    console.error('Pods fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–ServiceçŠ¶æ€
app.get('/api/services', async (req, res) => {
  try {
    console.log('Fetching services...');
    const output = await executeKubectl('get services -o json');
    const services = JSON.parse(output);
    console.log('Services fetched:', services.items.length, 'services');
    res.json(services.items);
  } catch (error) {
    console.error('Services fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// ä»£ç†HTTPè¯·æ±‚åˆ°æ¨¡å‹æœåŠ¡
app.post('/api/proxy-request', async (req, res) => {
  try {
    const { url, payload, method = 'POST' } = req.body;
    
    if (!url) {
      return res.status(400).json({
        success: false,
        error: 'Missing url'
      });
    }
    
    // GETè¯·æ±‚ä¸éœ€è¦payload
    if (method.toUpperCase() !== 'GET' && payload === undefined) {
      return res.status(400).json({
        success: false,
        error: 'Missing payload for non-GET request'
      });
    }
    
    console.log(`Proxying ${method} request to: ${url}`);
    if (method.toUpperCase() !== 'GET') {
      console.log(`Payload:`, JSON.stringify(payload, null, 2));
    }
    
    const result = await makeHttpRequest(url, payload, method);
    
    console.log('Proxy result:', JSON.stringify(result, null, 2));
    res.json(result);
    
  } catch (error) {
    console.error('Proxy request error:', error);
    res.json({
      success: false,
      error: error.error || error.message || 'Request failed'
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²YAMLé…ç½® - ä»…ç”¨äºæ¨ç†éƒ¨ç½²ï¼ˆVLLMå’ŒOllamaï¼‰
app.post('/api/deploy', async (req, res) => {
  try {
    const {
      replicas,
      huggingFaceToken,
      deploymentType,
      vllmCommand,
      ollamaModelId,
      gpuCount,
      isExternal = true,
      deploymentName,  // ç”¨æˆ·è¾“å…¥çš„éƒ¨ç½²åç§°
      dockerImage = 'vllm/vllm-openai:latest'
    } = req.body;

    console.log('Inference deployment request:', { 
      deploymentType, 
      deploymentName,
      ollamaModelId, 
      replicas, 
      isExternal,
      dockerImage
    });

    // ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€æ ‡ç­¾ï¼ˆç¬¦åˆKuberneteså‘½åè§„èŒƒï¼‰
    const timestamp = new Date().toISOString()
      .replace(/[:.]/g, '-')     // æ›¿æ¢å†’å·å’Œç‚¹å·ä¸ºè¿å­—ç¬¦
      .replace('T', '-')         // æ›¿æ¢Tä¸ºè¿å­—ç¬¦
      .slice(0, 19);             // æˆªå–åˆ°ç§’çº§
    const finalDeploymentTag = deploymentName ? `${deploymentName}-${timestamp}` : `model-${timestamp}`;
    
    console.log(`Generated deployment tag: "${finalDeploymentTag}"`);

    let templatePath, newYamlContent;

    // ç”ŸæˆNLBæ³¨è§£
    const nlbAnnotations = generateNLBAnnotations(isExternal);
    console.log(`Generated NLB annotations (external: ${isExternal}):`, nlbAnnotations);

    if (deploymentType === 'ollama') {
      // å¤„ç†Ollamaéƒ¨ç½²
      templatePath = path.join(__dirname, '../templates/ollama-template.yaml');
      const templateContent = await fs.readFile(templatePath, 'utf8');
      
      // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
      newYamlContent = templateContent
        .replace(/MODEL_TAG/g, finalDeploymentTag)
        .replace(/OLLAMA_MODEL_ID/g, ollamaModelId)
        .replace(/REPLICAS_COUNT/g, replicas.toString())
        .replace(/GPU_COUNT/g, gpuCount.toString())
        .replace(/NLB_ANNOTATIONS/g, nlbAnnotations);
      
    } else {
      // å¤„ç†VLLM/SGLang/Customéƒ¨ç½²
      const parsedCommand = parseVllmCommand(vllmCommand);
      console.log('Parsed command:', parsedCommand);
      
      // æ ¹æ®å‘½ä»¤ç±»å‹ç¡®å®šæœåŠ¡å¼•æ“å‰ç¼€
      let servEngine;
      if (parsedCommand.commandType === 'sglang') {
        servEngine = 'sglang';
      } else if (parsedCommand.commandType === 'vllm') {
        servEngine = 'vllm';
      } else {
        servEngine = 'custom';  // è‡ªå®šä¹‰å‘½ä»¤ä½¿ç”¨customå‰ç¼€
      }
      console.log(`Using service engine: ${servEngine} for command type: ${parsedCommand.commandType}`);

      templatePath = path.join(__dirname, '../templates/vllm-sglang-template.yaml');
      const templateContent = await fs.readFile(templatePath, 'utf8');
      
      // ç”ŸæˆHuggingFace tokenç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæä¾›äº†tokenï¼‰
      let hfTokenEnv = '';
      if (huggingFaceToken && huggingFaceToken.trim() !== '') {
        hfTokenEnv = `
            - name: HUGGING_FACE_HUB_TOKEN
              value: "${huggingFaceToken}"`;
      }
      
      // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦ - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„GPUæ•°é‡
      newYamlContent = templateContent
        .replace(/SERVENGINE/g, servEngine)
        .replace(/MODEL_TAG/g, finalDeploymentTag)
        .replace(/REPLICAS_COUNT/g, replicas.toString())
        .replace(/GPU_COUNT/g, gpuCount.toString())
        .replace(/HF_TOKEN_ENV/g, hfTokenEnv)
        .replace(/VLLM_COMMAND/g, JSON.stringify(parsedCommand.fullCommand))
        .replace(/DOCKER_IMAGE/g, dockerImage)
        .replace(/NLB_ANNOTATIONS/g, nlbAnnotations);
    }
    
    // ä¿å­˜åˆ°é¡¹ç›®ç›®å½•ä¸­çš„deploymentsæ–‡ä»¶å¤¹
    const deploymentsDir = path.join(__dirname, '../deployments');
    if (!fs.existsSync(deploymentsDir)) {
      fs.mkdirSync(deploymentsDir, { recursive: true });
    }
    
    const accessType = isExternal ? 'external' : 'internal';
    const tempYamlPath = path.join(deploymentsDir, `${finalDeploymentTag}-${deploymentType}-${accessType}.yaml`);
    await fs.writeFile(tempYamlPath, newYamlContent);
    
    console.log(`Generated YAML saved to: ${tempYamlPath}`);
    
    // æ‰§è¡Œkubectl apply
    const applyOutput = await executeKubectl(`apply -f ${tempYamlPath}`);
    
    // å¹¿æ’­éƒ¨ç½²çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'deployment',
      status: 'success',
      message: `Successfully deployed: ${finalDeploymentTag} (${accessType} access)`,
      output: applyOutput
    });
    
    res.json({
      success: true,
      message: 'Deployment successful',
      output: applyOutput,
      yamlPath: tempYamlPath,
      generatedYaml: newYamlContent,
      deploymentType,
      deploymentTag: finalDeploymentTag,
      accessType
    });
    
  } catch (error) {
    console.error('Deployment error:', error);
    
    broadcast({
      type: 'deployment',
      status: 'error',
      message: `Deployment failed: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç»Ÿä¸€çš„è®­ç»ƒYAMLéƒ¨ç½²å‡½æ•°
async function deployTrainingYaml(recipeType, jobName, yamlContent) {
  try {
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿deployments/trainingsç›®å½•å­˜åœ¨
    const trainingsDir = path.join(__dirname, '../deployments/trainings');
    if (!fs.existsSync(trainingsDir)) {
      fs.mkdirSync(trainingsDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `${recipeType}-${jobName}-${Date.now()}.yaml`;
    const tempFilePath = path.join(tempDir, tempFileName);
    await fs.writeFile(tempFilePath, yamlContent);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆç”¨äºè®°å½•ï¼‰
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const permanentFileName = `${recipeType}_${timestamp}.yaml`;
    const permanentFilePath = path.join(trainingsDir, permanentFileName);
    await fs.writeFile(permanentFilePath, yamlContent);

    console.log(`${recipeType} training YAML saved to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½®
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`);
    console.log(`${recipeType} training kubectl apply output:`, applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    fs.unlinkSync(tempFilePath);

    // å‘é€WebSocketå¹¿æ’­
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched ${recipeType} training job: ${jobName}`,
      output: applyOutput
    });

    return {
      success: true,
      permanentFileName,
      permanentFilePath,
      applyOutput
    };

  } catch (error) {
    // å‘é€é”™è¯¯å¹¿æ’­
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `${recipeType} training launch failed: ${error.message}`
    });

    throw error;
  }
}

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPod Torchè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºTorchè®­ç»ƒ
app.post('/api/launch-torch-training', async (req, res) => {
  try {
    console.log('Raw torch training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 16,
      entryPythonScriptPath,
      pythonScriptParameters,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Torch training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      entryPythonScriptPath,
      pythonScriptParameters,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!entryPythonScriptPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry Python Script Path is required'
      });
    }

    if (!pythonScriptParameters) {
      return res.status(400).json({
        success: false,
        error: 'Python Script Parameters are required'
      });
    }

    // è¯»å–Torchè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-torch-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // å¤„ç†Pythonè„šæœ¬å‚æ•° - ç¡®ä¿å¤šè¡Œå‚æ•°åœ¨YAMLä¸­æ­£ç¡®æ ¼å¼åŒ–
    let formattedPythonParams = pythonScriptParameters;
    if (pythonScriptParameters.includes('\\')) {
      // å¦‚æœåŒ…å«åæ–œæ æ¢è¡Œç¬¦ï¼Œå°†å…¶è½¬æ¢ä¸ºå•è¡Œæ ¼å¼
      formattedPythonParams = pythonScriptParameters
        .replace(/\\\s*\n\s*/g, ' ')  // å°†åæ–œæ æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
        .replace(/\s+/g, ' ')         // åˆå¹¶å¤šä¸ªç©ºæ ¼
        .trim();
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/TORCH_RECIPE_PYPATH_PH/g, entryPythonScriptPath)
      .replace(/TORCH_RECIPE_PYPARAMS_PH/g, formattedPythonParams)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated torch training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `torch-training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `torch_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Torch training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Torch training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Torch training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched torch training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Torch training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Torch training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Torch training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPodè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºLlamaFactoryè®­ç»ƒä»»åŠ¡
app.post('/api/launch-training', async (req, res) => {
  try {
    console.log('Raw request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = 'pytorch/pytorch:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 0,
      lmfRecipeRunPath,
      lmfRecipeYamlFile,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      lmfRecipeRunPath,
      lmfRecipeYamlFile,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!lmfRecipeRunPath) {
      return res.status(400).json({
        success: false,
        error: 'LlamaFactory Recipe Run Path is required'
      });
    }

    if (!lmfRecipeYamlFile) {
      return res.status(400).json({
        success: false,
        error: 'LlamaFactory Config YAML File Name is required'
      });
    }

    // è¯»å–è®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-lmf-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/LMF_RECIPE_RUNPATH_PH/g, lmfRecipeRunPath)
      .replace(/LMF_RECIPE_YAMLFILE_PH/g, lmfRecipeYamlFile)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `lma_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ä¿å­˜LlamaFactoryé…ç½®
app.post('/api/llamafactory-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/llamafactory-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('LlamaFactory config saved:', config);
    
    res.json({
      success: true,
      message: 'LlamaFactory configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving training config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½LlamaFactoryé…ç½®
app.get('/api/llamafactory-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/llamafactory-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'lmf-v1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op-v2:latest',
        instanceType: 'ml.g6.12xlarge',
        nprocPerNode: 4,
        replicas: 1,
        efaCount: 1,
        lmfRecipeRunPath: '/s3/train-recipes/llama-factory-project/',
        lmfRecipeYamlFile: 'qwen_full_dist_template.yaml',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('LlamaFactory config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading training config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜Scripté…ç½®
app.post('/api/script-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/script-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('Script config saved:', config);
    
    res.json({
      success: true,
      message: 'Script configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving script config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½Scripté…ç½®
app.get('/api/script-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/script-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'hypd-recipe-script-1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
        instanceType: 'ml.g5.12xlarge',
        nprocPerNode: 1,
        replicas: 1,
        efaCount: 16,
        projectPath: '/s3/training_code/my-training-project/',
        entryPath: 'train.py',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('Script config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading script config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPod Scriptè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºScriptè®­ç»ƒ
app.post('/api/launch-script-training', async (req, res) => {
  try {
    console.log('Raw script training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 16,
      projectPath,
      entryPath,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Script training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      projectPath,
      entryPath,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!projectPath) {
      return res.status(400).json({
        success: false,
        error: 'Project Path is required'
      });
    }

    if (!entryPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry Script Path is required'
      });
    }

    // è¯»å–Scriptè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-script-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/SCRIPT_RECIPE_PROJECTPATH_PH/g, projectPath)
      .replace(/SCRIPT_RECIPE_ENTRYPATH_PH/g, entryPath)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated script training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `script-training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `script_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Script training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Script training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Script training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched script training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Script training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Script training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Script training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ä¿å­˜Torché…ç½®
app.post('/api/torch-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/torch-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('Torch config saved:', config);
    
    res.json({
      success: true,
      message: 'Torch configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving torch config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½Torché…ç½®
app.get('/api/torch-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/torch-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'hypd-recipe-torch-1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
        instanceType: 'ml.g5.12xlarge',
        nprocPerNode: 1,
        replicas: 1,
        efaCount: 16,
        entryPythonScriptPath: '/s3/training_code/model-training-with-hyperpod-training-operator/torch-training.py',
        pythonScriptParameters: '--learning_rate 1e-5 \\\n--batch_size 1',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('Torch config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading torch config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜verlé…ç½®
app.post('/api/verl-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/verl-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('VERL config saved:', config);
    
    res.json({
      success: true,
      message: 'VERL configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving VERL config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½verlé…ç½®
app.get('/api/verl-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/verl-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        jobName: 'verl-training-a1',
        instanceType: 'ml.g5.12xlarge',
        entryPointPath: 'verl-project/src/qwen-3b-grpo-kuberay.sh',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/hypd-verl:latest',
        workerReplicas: 1,
        gpuPerNode: 4,
        efaPerNode: 1
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('VERL config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading VERL config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²VERLè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºVERLè®­ç»ƒ
app.post('/api/launch-verl-training', async (req, res) => {
  try {
    console.log('Raw VERL training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      jobName,
      instanceType = 'ml.g5.12xlarge',
      entryPointPath,
      dockerImage,
      workerReplicas = 1,
      gpuPerNode = 4,
      efaPerNode = 1,
      recipeType
    } = req.body;

    console.log('VERL training launch request parsed:', { 
      jobName,
      instanceType,
      entryPointPath,
      dockerImage,
      workerReplicas,
      gpuPerNode,
      efaPerNode,
      recipeType
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!jobName) {
      return res.status(400).json({
        success: false,
        error: 'Job name is required'
      });
    }

    if (!entryPointPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry point path is required'
      });
    }

    if (!dockerImage) {
      return res.status(400).json({
        success: false,
        error: 'Docker image is required'
      });
    }

    // è¯»å–VERLè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/verl-training-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/JOB_NAME/g, jobName)
      .replace(/ENTRY_POINT_PATH/g, entryPointPath)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/WORKER_REPLICAS/g, workerReplicas.toString())
      .replace(/MAX_REPLICAS/g, Math.max(3, workerReplicas + 2).toString())
      .replace(/GPU_PER_NODE/g, gpuPerNode.toString())
      .replace(/EFA_PER_NODE/g, efaPerNode.toString());

    console.log('Generated VERL YAML content preview:', newYamlContent.substring(0, 500) + '...');

    // ä½¿ç”¨ç»Ÿä¸€çš„éƒ¨ç½²å‡½æ•°
    const deployResult = await deployTrainingYaml('verl', jobName, newYamlContent);

    res.json({
      success: true,
      message: `VERL training job "${jobName}" launched successfully`,
      jobName: jobName,
      templateUsed: 'verl-training-template.yaml',
      savedTemplate: deployResult.permanentFileName,
      savedTemplatePath: deployResult.permanentFilePath,
      output: deployResult.applyOutput
    });

  } catch (error) {
    console.error('VERL training launch error:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Unknown error occurred'
    });
  }
});

// è·å–æ‰€æœ‰RayJob
app.get('/api/rayjobs', async (req, res) => {
  try {
    const output = await executeKubectl('get rayjobs -o json');
    const rayjobs = JSON.parse(output);
    res.json(rayjobs.items);
  } catch (error) {
    console.error('RayJobs fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// åˆ é™¤æŒ‡å®šçš„RayJob
app.delete('/api/rayjobs/:jobName', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Deleting RayJob: ${jobName}`);
    
    const output = await executeKubectl(`delete rayjob ${jobName}`);
    console.log('RayJob delete output:', output);
    
    // å‘é€WebSocketå¹¿æ’­
    broadcast({
      type: 'rayjob_deleted',
      status: 'success',
      message: `RayJob "${jobName}" deleted successfully`,
      jobName: jobName
    });
    
    res.json({
      success: true,
      message: `RayJob "${jobName}" deleted successfully`,
      output: output
    });
  } catch (error) {
    console.error('Error deleting RayJob:', error);
    
    broadcast({
      type: 'rayjob_deleted',
      status: 'error',
      message: `Failed to delete RayJob: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// è·å–æ‰€æœ‰HyperPodè®­ç»ƒä»»åŠ¡
app.get('/api/training-jobs', async (req, res) => {
  try {
    console.log('Fetching training jobs (HyperPod PytorchJob + RayJob)...');
    
    // è·å–HyperPod PytorchJob
    let hyperpodJobs = [];
    try {
      const hyperpodOutput = await executeKubectl('get hyperpodpytorchjob -o json');
      const hyperpodResult = JSON.parse(hyperpodOutput);
      hyperpodJobs = hyperpodResult.items.map(job => ({
        name: job.metadata.name,
        namespace: job.metadata.namespace || 'default',
        creationTimestamp: job.metadata.creationTimestamp,
        status: job.status || {},
        type: 'hyperpod',
        spec: {
          replicas: job.spec?.replicaSpecs?.[0]?.replicas || 0,
          nprocPerNode: job.spec?.nprocPerNode || 0
        }
      }));
    } catch (error) {
      const optimizedMessage = optimizeErrorMessage(error.message);
      console.log('No HyperPod PytorchJobs found or error:', optimizedMessage);
      // å¯¹äºå¯¼å…¥çš„é›†ç¾¤ï¼Œè¿™æ˜¯æ­£å¸¸çš„ - ä¸è®°å½•ä¸ºé”™è¯¯
    }

    // è·å–RayJob
    let rayJobs = [];
    try {
      const rayOutput = await executeKubectl('get rayjob -o json');
      const rayResult = JSON.parse(rayOutput);
      rayJobs = rayResult.items.map(job => ({
        name: job.metadata.name,
        namespace: job.metadata.namespace || 'default',
        creationTimestamp: job.metadata.creationTimestamp,
        status: job.status || {},
        type: 'rayjob',
        spec: {
          replicas: 1, // RayJobé€šå¸¸æ˜¯å•ä¸ªä½œä¸š
          nprocPerNode: 1
        }
      }));
    } catch (error) {
      const optimizedMessage = optimizeErrorMessage(error.message);
      console.log('No RayJobs found or error:', optimizedMessage);
      // å¯¹äºå¯¼å…¥çš„é›†ç¾¤ï¼Œè¿™æ˜¯æ­£å¸¸çš„ - ä¸è®°å½•ä¸ºé”™è¯¯
    }

    // åˆå¹¶ä¸¤ç§ç±»å‹çš„ä½œä¸š
    const trainingJobs = [...hyperpodJobs, ...rayJobs];
    
    console.log(`Found ${trainingJobs.length} training jobs (${hyperpodJobs.length} HyperPod + ${rayJobs.length} Ray):`, 
                trainingJobs.map(j => `${j.name}(${j.type})`));
    
    res.json({
      success: true,
      jobs: trainingJobs
    });
  } catch (error) {
    console.error('Error fetching training jobs:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      jobs: []
    });
  }
});

// åˆ é™¤æŒ‡å®šçš„HyperPodè®­ç»ƒä»»åŠ¡
app.delete('/api/training-jobs/:jobName', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Deleting training job: ${jobName}`);
    
    const output = await executeKubectl(`delete hyperpodpytorchjob ${jobName}`);
    console.log('Delete output:', output);
    
    // å¹¿æ’­åˆ é™¤çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_job_deleted',
      status: 'success',
      message: `Training job "${jobName}" deleted successfully`,
      jobName: jobName
    });
    
    res.json({
      success: true,
      message: `Training job "${jobName}" deleted successfully`,
      output: output
    });
  } catch (error) {
    console.error('Error deleting training job:', error);
    
    broadcast({
      type: 'training_job_deleted',
      status: 'error',
      message: `Failed to delete training job: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// MLflowé…ç½®ç®¡ç†

const CONFIG_FILE = path.join(__dirname, '../config/mlflow-metric-config.json');

// ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
const configDir = path.dirname(CONFIG_FILE);
if (!fs.existsSync(configDir)) {
  fs.mkdirSync(configDir, { recursive: true });
}

// é»˜è®¤MLflowé…ç½®
const DEFAULT_MLFLOW_CONFIG = {
  tracking_uri: '',
  experiment_id: '',
  sync_configs: {}
};

// è¯»å–MLflowé…ç½®
function readMlflowConfig() {
  try {
    if (fs.existsSync(CONFIG_FILE)) {
      const configData = fs.readFileSync(CONFIG_FILE, 'utf8');
      return JSON.parse(configData);
    }
  } catch (error) {
    console.error('Error reading MLflow config:', error);
  }
  return DEFAULT_MLFLOW_CONFIG;
}

// ä¿å­˜MLflowé…ç½®
function saveMlflowConfig(config) {
  try {
    fs.writeFileSync(CONFIG_FILE, JSON.stringify(config, null, 2));
    return true;
  } catch (error) {
    console.error('Error saving MLflow config:', error);
    return false;
  }
}

// è·å–MLflowé…ç½®
app.get('/api/mlflow-metric-config', (req, res) => {
  try {
    const config = readMlflowConfig();
    res.json({
      success: true,
      config: config
    });
  } catch (error) {
    console.error('Error fetching MLflow config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜MLflowé…ç½®
app.post('/api/mlflow-metric-config', (req, res) => {
  try {
    const { tracking_uri } = req.body;
    
    if (!tracking_uri) {
      return res.status(400).json({
        success: false,
        error: 'tracking_uri is required'
      });
    }

    const config = { tracking_uri };
    
    if (saveMlflowConfig(config)) {
      console.log('MLflow config saved:', config);
      res.json({
        success: true,
        config: config
      });
    } else {
      res.status(500).json({
        success: false,
        error: 'Failed to save configuration'
      });
    }
  } catch (error) {
    console.error('Error saving MLflow config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// æµ‹è¯•MLflowè¿æ¥
app.post('/api/mlflow-metric-config/test', async (req, res) => {
  try {
    const { tracking_uri } = req.body;
    
    if (!tracking_uri) {
      return res.status(400).json({
        success: false,
        error: 'tracking_uri is required'
      });
    }

    console.log(`Testing MLflow connection to: ${tracking_uri}`);
    
    const { spawn } = require('child_process');
    
    // åˆ›å»ºæµ‹è¯•è„šæœ¬
    const testScript = `#!/usr/bin/env python3
import mlflow
import sys
import json

try:
    tracking_uri = "${tracking_uri}"
    mlflow.set_tracking_uri(tracking_uri)
    
    # å°è¯•è·å–å®éªŒåˆ—è¡¨æ¥æµ‹è¯•è¿æ¥
    experiments = mlflow.search_experiments()
    
    result = {
        "success": True,
        "experiments_count": len(experiments),
        "message": f"Successfully connected to MLflow. Found {len(experiments)} experiments."
    }
    
    print(json.dumps(result))
    sys.exit(0)
    
except Exception as e:
    result = {
        "success": False,
        "error": str(e)
    }
    print(json.dumps(result))
    sys.exit(1)
`;
    
    const tempScriptPath = path.join(__dirname, '../temp/test_mlflow_connection.py');
    
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.dirname(tempScriptPath);
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    
    fs.writeFileSync(tempScriptPath, testScript);
    
    const pythonPath = 'python3'; // ä½¿ç”¨ç³»ç»ŸPython
    const pythonProcess = spawn(pythonPath, [tempScriptPath], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    pythonProcess.on('close', (code) => {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        fs.unlinkSync(tempScriptPath);
      } catch (e) {
        console.warn('Failed to cleanup temp file:', e);
      }
      
      if (stderr) {
        console.log('Python test script stderr:', stderr);
      }
      
      try {
        const result = JSON.parse(stdout);
        if (result.success) {
          res.json(result);
        } else {
          res.status(400).json(result);
        }
      } catch (parseError) {
        console.error('Failed to parse test result:', parseError);
        console.error('Raw output:', stdout);
        res.status(500).json({
          success: false,
          error: 'Failed to test MLflow connection',
          details: stdout || stderr
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      console.error('Failed to start Python test script:', error);
      res.status(500).json({
        success: false,
        error: `Failed to start test script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('MLflow connection test error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// MLflowè·¨è´¦æˆ·åŒæ­¥API
app.post('/api/mlflow-sync', async (req, res) => {
  try {
    const { sync_config, experiment_name, experiment_id } = req.body;
    
    // æ”¯æŒä¸¤ç§å‚æ•°æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
    const experimentIdentifier = experiment_name || experiment_id;
    
    // éªŒè¯å¿…éœ€å­—æ®µ
    if (!sync_config || !experimentIdentifier) {
      return res.status(400).json({
        success: false,
        error: 'sync_config and experiment_name (or experiment_id) are required'
      });
    }

    // éªŒè¯JSONé…ç½®
    let configObj;
    try {
      configObj = JSON.parse(sync_config);
    } catch (e) {
      return res.status(400).json({
        success: false,
        error: 'Invalid JSON format in sync_config'
      });
    }

    // éªŒè¯å¿…éœ€çš„é…ç½®å­—æ®µ
    const requiredFields = ['contributor_name', 'source_mlflow_arn', 'shared_account_id', 'shared_aws_region', 'cross_account_role_arn', 'shared_mlflow_arn'];
    const missingFields = requiredFields.filter(field => !configObj[field]);
    if (missingFields.length > 0) {
      return res.status(400).json({
        success: false,
        error: `Missing required fields in sync_config: ${missingFields.join(', ')}`
      });
    }

    // éªŒè¯sourceå’Œdestination ARNä¸èƒ½ç›¸åŒ
    if (configObj.source_mlflow_arn === configObj.shared_mlflow_arn) {
      return res.status(400).json({
        success: false,
        error: 'Source MLflow ARN and Shared MLflow ARN cannot be the same. Please ensure you are syncing to a different MLflow server.'
      });
    }

    // æ·»åŠ æ—¶é—´æˆ³
    configObj.setup_date = new Date().toISOString();

    console.log(`Starting MLflow sync for experiment ${experimentIdentifier}...`);
    
    // 1. ä¿å­˜é…ç½®åˆ°mlflow-metric-config.json
    const currentConfig = readMlflowConfig();
    const updatedConfig = {
      ...currentConfig,
      experiment_name: experimentIdentifier,  // æ”¹ä¸ºexperiment_name
      sync_configs: {
        ...configObj,
        last_sync: new Date().toISOString()
      }
    };
    
    if (!saveMlflowConfig(updatedConfig)) {
      return res.status(500).json({
        success: false,
        error: 'Failed to save sync configuration'
      });
    }

    // 2. åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ä¾›Pythonè„šæœ¬ä½¿ç”¨
    const tempConfigPath = path.join(__dirname, '../temp/sync-config-temp.json');
    
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.dirname(tempConfigPath);
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    
    fs.writeFileSync(tempConfigPath, JSON.stringify(configObj, null, 2));

    // 3. è°ƒç”¨PythonåŒæ­¥è„šæœ¬
    const { spawn } = require('child_process');
    const pythonPath = 'python3'; // ä½¿ç”¨ç³»ç»ŸPython
    const syncScriptPath = path.join(__dirname, '../mlflow/cross_account_sync.py');
    
    const pythonProcess = spawn(pythonPath, [
      syncScriptPath,
      '--config-file', tempConfigPath,
      '--experiment-name', experimentIdentifier
    ], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    pythonProcess.on('close', (code) => {
      // æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
      try {
        fs.unlinkSync(tempConfigPath);
      } catch (e) {
        console.warn('Failed to cleanup temp config file:', e);
      }
      
      if (code === 0) {
        console.log('MLflow sync completed successfully');
        console.log('Sync output:', stdout);
        
        res.json({
          success: true,
          message: 'Successfully synced experiment to shared MLflow server',
          output: stdout,
          experiment_id: experimentIdentifier,
          contributor: configObj.contributor_name
        });
      } else {
        console.error('MLflow sync failed with code:', code);
        console.error('Sync stderr:', stderr);
        console.error('Sync stdout:', stdout);
        
        res.status(500).json({
          success: false,
          error: 'MLflow sync failed',
          details: stderr || stdout,
          exit_code: code
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      console.error('Failed to start MLflow sync script:', error);
      
      // æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
      try {
        fs.unlinkSync(tempConfigPath);
      } catch (e) {
        console.warn('Failed to cleanup temp config file:', e);
      }
      
      res.status(500).json({
        success: false,
        error: `Failed to start sync script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('MLflow sync API error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åˆ›å»ºMLflow Tracking Server API
app.post('/api/create-mlflow-tracking-server', async (req, res) => {
  try {
    const { mlflowServerName, trackingServerSize = 'Small' } = req.body;
    
    const MLflowTrackingServerManager = require('./utils/mlflowTrackingServerManager');
    const mlflowManager = new MLflowTrackingServerManager();
    
    // éªŒè¯è¾“å…¥å‚æ•°
    mlflowManager.validateServerName(mlflowServerName);
    mlflowManager.validateServerSize(trackingServerSize);
    
    // åˆ›å»ºtracking server
    const result = await mlflowManager.createTrackingServer(mlflowServerName, trackingServerSize);
    
    // å¹¿æ’­åˆ›å»ºæˆåŠŸæ¶ˆæ¯
    broadcast({
      type: 'mlflow_tracking_server_created',
      status: 'success',
      message: result.message,
      serverName: mlflowServerName
    });
    
    res.json(result);
    
  } catch (error) {
    console.error('Error creating MLflow tracking server:', error);
    
    // å¹¿æ’­åˆ›å»ºå¤±è´¥æ¶ˆæ¯
    broadcast({
      type: 'mlflow_tracking_server_created',
      status: 'error',
      message: error.message
    });
    
    res.json({ 
      success: false, 
      error: error.message 
    });
  }
});

// è·å–è®­ç»ƒå†å²æ•°æ®ï¼ˆä»MLflowï¼‰
app.get('/api/training-history', async (req, res) => {
  try {
    console.log('Fetching training history from MLflow...');
    
    // è¯»å–å½“å‰MLflowé…ç½®
    const mlflowConfig = readMlflowConfig();
    console.log('Using MLflow URI:', mlflowConfig.tracking_uri);
    
    const { spawn } = require('child_process');
    const path = require('path');
    
    // ä½¿ç”¨ç³»ç»ŸPythonæ‰§è¡Œè„šæœ¬ï¼Œä¼ é€’é…ç½®å‚æ•°
    const pythonPath = 'python3'; // ä½¿ç”¨ç³»ç»ŸPython
    const scriptPath = path.join(__dirname, '../mlflow/get_training_history.py');
    
    const pythonProcess = spawn(pythonPath, [scriptPath, mlflowConfig.tracking_uri], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    let responseHandled = false;
    
    pythonProcess.on('close', (code) => {
      if (responseHandled) return;
      
      if (stderr) {
        console.log('Python script stderr:', stderr);
      }
      
      if (code !== 0) {
        console.error(`Python script exited with code ${code}`);
        responseHandled = true;
        return res.status(500).json({ 
          success: false, 
          error: `Failed to fetch training history: exit code ${code}`,
          stderr: stderr
        });
      }
      
      try {
        const result = JSON.parse(stdout);
        console.log(`Training history fetched: ${result.total} records`);
        responseHandled = true;
        res.json(result);
      } catch (parseError) {
        console.error('Failed to parse Python script output:', parseError);
        console.error('Raw output:', stdout);
        responseHandled = true;
        res.status(500).json({ 
          success: false, 
          error: 'Failed to parse training history data',
          raw_output: stdout
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      if (responseHandled) return;
      
      console.error('Failed to start Python script:', error);
      responseHandled = true;
      res.status(500).json({ 
        success: false, 
        error: `Failed to start Python script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('Training history fetch error:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// è·å–è®­ç»ƒä»»åŠ¡å…³è”çš„pods
app.get('/api/training-jobs/:jobName/pods', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Fetching pods for training job: ${jobName}`);
    
    // è·å–æ‰€æœ‰podsï¼Œç„¶åç­›é€‰å‡ºå±äºè¯¥è®­ç»ƒä»»åŠ¡çš„pods
    const output = await executeKubectl('get pods -o json');
    const result = JSON.parse(output);
    
    // ç­›é€‰å‡ºå±äºè¯¥è®­ç»ƒä»»åŠ¡çš„pods
    const trainingPods = result.items.filter(pod => {
      const labels = pod.metadata.labels || {};
      const ownerReferences = pod.metadata.ownerReferences || [];
      
      // æ£€æŸ¥æ˜¯å¦é€šè¿‡æ ‡ç­¾æˆ–ownerReferenceså…³è”åˆ°è®­ç»ƒä»»åŠ¡
      return labels['training-job-name'] === jobName || 
             labels['app'] === jobName ||
             ownerReferences.some(ref => ref.name === jobName) ||
             pod.metadata.name.includes(jobName);
    });
    
    const pods = trainingPods.map(pod => ({
      name: pod.metadata.name,
      namespace: pod.metadata.namespace || 'default',
      status: pod.status.phase,
      creationTimestamp: pod.metadata.creationTimestamp,
      nodeName: pod.spec.nodeName,
      containerStatuses: pod.status.containerStatuses || []
    }));
    
    console.log(`Found ${pods.length} pods for training job ${jobName}:`, pods.map(p => p.name));
    
    res.json({
      success: true,
      pods: pods
    });
  } catch (error) {
    console.error('Error fetching training job pods:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      pods: []
    });
  }
});

// è·å–å®Œæ•´æ—¥å¿—æ–‡ä»¶
app.get('/api/logs/:jobName/:podName', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      res.sendFile(path.resolve(logFilePath));
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found',
        path: logFilePath
      });
    }
  } catch (error) {
    console.error('Error serving log file:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// ä¸‹è½½å®Œæ•´æ—¥å¿—æ–‡ä»¶
app.get('/api/logs/:jobName/:podName/download', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      res.download(logFilePath, `${podName}.log`, (err) => {
        if (err) {
          console.error('Error downloading log file:', err);
          res.status(500).json({ 
            success: false, 
            error: 'Failed to download log file' 
          });
        }
      });
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found',
        path: logFilePath
      });
    }
  } catch (error) {
    console.error('Error downloading log file:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
app.get('/api/logs/:jobName/:podName/info', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      const stats = fs.statSync(logFilePath);
      res.json({
        success: true,
        info: {
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime,
          path: logFilePath
        }
      });
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found' 
      });
    }
  } catch (error) {
    console.error('Error getting log file info:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// åˆ é™¤éƒ¨ç½² - æ”¹è¿›ç‰ˆæœ¬
app.post('/api/undeploy', async (req, res) => {
  try {
    const { modelTag, deleteType } = req.body;
    
    if (!modelTag) {
      return res.status(400).json({
        success: false,
        error: 'Model tag is required'
      });
    }
    
    console.log(`Undeploying model: ${modelTag}, type: ${deleteType}`);
    
    // æ„å»ºå¯èƒ½çš„èµ„æºåç§°
    const possibleDeployments = [
      `vllm-${modelTag}-inference`,
      `sglang-${modelTag}-inference`,
      `olm-${modelTag}-inference`,
      `${modelTag}-inference`  // å¤‡ç”¨æ ¼å¼
    ];
    
    const possibleServices = [
      `vllm-${modelTag}-nlb`,
      `sglang-${modelTag}-nlb`,
      `olm-${modelTag}-nlb`,
      `${modelTag}-nlb`,
      `${modelTag}-service`  // å¤‡ç”¨æ ¼å¼
    ];
    
    let deleteCommands = [];
    let deletedResources = [];
    
    // æ ¹æ®åˆ é™¤ç±»å‹å†³å®šåˆ é™¤å“ªäº›èµ„æº
    if (deleteType === 'all' || deleteType === 'deployment') {
      possibleDeployments.forEach(deploymentName => {
        deleteCommands.push(`delete deployment ${deploymentName} --ignore-not-found=true`);
      });
      deletedResources.push('Deployment');
    }
    
    if (deleteType === 'all' || deleteType === 'service') {
      possibleServices.forEach(serviceName => {
        deleteCommands.push(`delete service ${serviceName} --ignore-not-found=true`);
      });
      deletedResources.push('Service');
    }
    
    // æ‰§è¡Œåˆ é™¤å‘½ä»¤
    const results = [];
    let actuallyDeleted = 0;
    
    for (const command of deleteCommands) {
      try {
        const output = await executeKubectl(command);
        const success = !output.includes('not found');
        results.push({
          command,
          success: true,
          output: output.trim(),
          actuallyDeleted: success
        });
        if (success) actuallyDeleted++;
      } catch (error) {
        results.push({
          command,
          success: false,
          error: error.error || error.message,
          actuallyDeleted: false
        });
      }
    }
    
    // ç­‰å¾…ä¸€ä¸‹è®©èµ„æºå®Œå…¨åˆ é™¤
    if (actuallyDeleted > 0) {
      console.log(`Waiting for resources to be fully deleted...`);
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
    
    // å¹¿æ’­åˆ é™¤çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'undeployment',
      status: 'success',
      message: `Successfully deleted resources for ${modelTag} (${actuallyDeleted} resources)`,
      results: results
    });
    
    res.json({
      success: true,
      message: actuallyDeleted > 0 
        ? `Successfully deleted ${actuallyDeleted} resource(s)` 
        : 'No resources found to delete (may already be deleted)',
      deletedResources: deletedResources,
      results: results,
      modelTag: modelTag,
      actuallyDeleted: actuallyDeleted
    });
    
  } catch (error) {
    console.error('Undeploy error:', error);
    
    broadcast({
      type: 'undeployment',
      status: 'error',
      message: `Failed to undeploy ${req.body.modelTag}: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// è·å–éƒ¨ç½²è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«æ¨¡å‹å…ƒæ•°æ®ï¼‰
app.get('/api/deployment-details', async (req, res) => {
  try {
    console.log('Fetching deployment details with metadata...');
    
    // è·å–æ‰€æœ‰deployment
    const deploymentsOutput = await executeKubectl('get deployments -o json');
    const deployments = JSON.parse(deploymentsOutput);
    
    // è·å–æ‰€æœ‰service
    const servicesOutput = await executeKubectl('get services -o json');
    const services = JSON.parse(servicesOutput);
    
    // è¿‡æ»¤å‡ºæ¨¡å‹ç›¸å…³çš„éƒ¨ç½²å¹¶æå–å…ƒæ•°æ®
    const modelDeployments = deployments.items
      .filter(deployment => 
        deployment.metadata.name.includes('vllm') || 
        deployment.metadata.name.includes('olm') ||
        deployment.metadata.name.includes('inference')
      )
      .map(deployment => {
        const labels = deployment.metadata.labels || {};
        const appLabel = labels.app;
        
        // æŸ¥æ‰¾å¯¹åº”çš„service
        const matchingService = services.items.find(service => 
          service.spec.selector?.app === appLabel
        );
        
        // ä»æ ‡ç­¾ä¸­æå–æ¨¡å‹ä¿¡æ¯
        const modelType = labels['model-type'] || 'unknown';
        const encodedModelId = labels['model-id'] || 'unknown';
        const modelTag = labels['model-tag'] || 'unknown';
        
        // ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹ID - ä¼˜å…ˆä»å®¹å™¨å‘½ä»¤ä¸­æå–åŸå§‹ID
        let modelId = 'unknown';
        
        // å¯¹äºVLLMéƒ¨ç½²ï¼Œä»å®¹å™¨å‘½ä»¤ä¸­æå–åŸå§‹æ¨¡å‹ID
        if (modelType === 'vllm') {
          try {
            const containers = deployment.spec?.template?.spec?.containers || [];
            const vllmContainer = containers.find(c => c.name === 'vllm-openai');
            if (vllmContainer && vllmContainer.command) {
              const command = vllmContainer.command;
              
              // 1. ä¼˜å…ˆæ£€æŸ¥æ–°çš„ vllm serve æ ¼å¼
              const serveIndex = command.findIndex(arg => arg === 'serve');
              if (serveIndex !== -1 && serveIndex + 1 < command.length) {
                // æ£€æŸ¥å‰ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯ vllm ç›¸å…³
                if (serveIndex > 0 && command[serveIndex - 1].includes('vllm')) {
                  const modelPath = command[serveIndex + 1];
                  // ç¡®ä¿ä¸æ˜¯ä»¥ -- å¼€å¤´çš„å‚æ•°
                  if (!modelPath.startsWith('--')) {
                    modelId = modelPath;
                  }
                }
              }
              
              // 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥ä¼ ç»Ÿçš„ --model å‚æ•°
              if (modelId === 'unknown') {
                const modelIndex = command.findIndex(arg => arg === '--model');
                if (modelIndex !== -1 && modelIndex + 1 < command.length) {
                  modelId = command[modelIndex + 1]; // è·å–--modelå‚æ•°åçš„å€¼
                }
              }
            }
          } catch (error) {
            console.log('Failed to extract model ID from VLLM command:', error.message);
          }
        }
        
        // å¯¹äºOllamaéƒ¨ç½²ï¼Œä»postStartç”Ÿå‘½å‘¨æœŸé’©å­ä¸­æå–æ¨¡å‹ID
        if (modelType === 'ollama' && modelId === 'unknown') {
          try {
            const containers = deployment.spec?.template?.spec?.containers || [];
            const ollamaContainer = containers.find(c => c.name === 'ollama');
            if (ollamaContainer && ollamaContainer.lifecycle?.postStart?.exec?.command) {
              const command = ollamaContainer.lifecycle.postStart.exec.command;
              // æŸ¥æ‰¾åŒ…å«"ollama pull"çš„å‘½ä»¤
              const commandStr = command.join(' ');
              const pullMatch = commandStr.match(/ollama pull ([^\s\\]+)/);
              if (pullMatch) {
                modelId = pullMatch[1]; // æå–æ¨¡å‹ID
                console.log('Extracted Ollama model ID from postStart:', modelId);
              }
            }
          } catch (error) {
            console.log('Failed to extract model ID from Ollama postStart command:', error.message);
          }
        }
        
        // å¯¹äºæ— æ³•æå–çš„æƒ…å†µï¼Œä½¿ç”¨è§£ç é€»è¾‘
        if (modelId === 'unknown' && encodedModelId !== 'unknown') {
          modelId = decodeModelIdFromLabel(encodedModelId);
        }
        
        // è·å–æœåŠ¡URL
        let serviceUrl = '';
        if (matchingService) {
          const ingress = matchingService.status?.loadBalancer?.ingress?.[0];
          if (ingress) {
            const host = ingress.hostname || ingress.ip;
            const port = matchingService.spec.ports?.[0]?.port || 8000;
            serviceUrl = `http://${host}:${port}`;
          }
        }
        
        return {
          deploymentName: deployment.metadata.name,
          serviceName: matchingService?.metadata.name || 'N/A',
          modelType: modelType,
          modelId: modelId,
          modelTag: modelTag,
          serviceUrl: serviceUrl,
          status: deployment.status.readyReplicas === deployment.spec.replicas ? 'Ready' : 'Pending',
          replicas: deployment.spec.replicas,
          readyReplicas: deployment.status.readyReplicas || 0,
          hasService: !!matchingService,
          isExternal: matchingService?.metadata?.annotations?.['service.beta.kubernetes.io/aws-load-balancer-scheme'] === 'internet-facing'
        };
      });
    
    console.log('Deployment details fetched:', modelDeployments.length, 'deployments');
    res.json(modelDeployments);
    
  } catch (error) {
    console.error('Deployment details fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–å·²éƒ¨ç½²çš„æ¨¡å‹åˆ—è¡¨
app.get('/api/deployments', async (req, res) => {
  try {
    console.log('Fetching deployments...');
    
    // è·å–æ‰€æœ‰deployment
    const deploymentsOutput = await executeKubectl('get deployments -o json');
    const deployments = JSON.parse(deploymentsOutput);
    
    // è·å–æ‰€æœ‰service
    const servicesOutput = await executeKubectl('get services -o json');
    const services = JSON.parse(servicesOutput);
    
    // è¿‡æ»¤å‡ºVLLMå’ŒOllamaç›¸å…³çš„éƒ¨ç½²
    const modelDeployments = deployments.items.filter(deployment => 
      deployment.metadata.name.includes('vllm') || 
      deployment.metadata.name.includes('olm') ||
      deployment.metadata.name.includes('inference')
    );
    
    // ä¸ºæ¯ä¸ªéƒ¨ç½²åŒ¹é…å¯¹åº”çš„service
    const deploymentList = modelDeployments.map(deployment => {
      const appLabel = deployment.metadata.labels?.app;
      const matchingService = services.items.find(service => 
        service.spec.selector?.app === appLabel
      );
      
      // ä»deploymentåç§°æå–model tagå’Œç±»å‹
      const deploymentName = deployment.metadata.name;
      let modelTag = 'unknown';
      let deploymentType = 'unknown';
      
      if (deploymentName.startsWith('vllm-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(5, -10); // ç§»é™¤ 'vllm-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'VLLM';
      } else if (deploymentName.startsWith('sglang-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(7, -10); // ç§»é™¤ 'sglang-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'SGLANG';
      } else if (deploymentName.startsWith('olm-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(4, -10); // ç§»é™¤ 'olm-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'Ollama';
      }
      
      // æ£€æŸ¥æ˜¯å¦ä¸ºexternalè®¿é—®
      const isExternal = matchingService?.metadata?.annotations?.['service.beta.kubernetes.io/aws-load-balancer-scheme'] === 'internet-facing';
      
      return {
        modelTag,
        deploymentType,
        deploymentName: deployment.metadata.name,
        serviceName: matchingService?.metadata.name || 'N/A',
        replicas: deployment.spec.replicas,
        readyReplicas: deployment.status.readyReplicas || 0,
        status: deployment.status.readyReplicas === deployment.spec.replicas ? 'Ready' : 'Pending',
        createdAt: deployment.metadata.creationTimestamp,
        hasService: !!matchingService,
        serviceType: matchingService?.spec.type || 'N/A',
        isExternal: isExternal,
        externalIP: matchingService?.status?.loadBalancer?.ingress?.[0]?.hostname || 
                   matchingService?.status?.loadBalancer?.ingress?.[0]?.ip || 'Pending'
      };
    });
    
    console.log('Deployments fetched:', deploymentList.length, 'model deployments');
    res.json(deploymentList);
    
  } catch (error) {
    console.error('Deployments fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// æµ‹è¯•æ¨¡å‹APIï¼ˆç”ŸæˆcURLå‘½ä»¤ï¼‰
app.post('/api/test-model', async (req, res) => {
  const { serviceUrl, payload } = req.body;
  
  try {
    let parsedPayload;
    
    try {
      parsedPayload = JSON.parse(payload);
    } catch (error) {
      return res.status(400).json({ error: 'Invalid JSON payload' });
    }
    
    const curlCommand = `curl -X POST "${serviceUrl}" \\
  -H "Content-Type: application/json" \\
  -d '${JSON.stringify(parsedPayload, null, 2)}'`;
    
    res.json({
      curlCommand,
      fullUrl: serviceUrl,
      message: 'Use the curl command to test your model'
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// WebSocketè¿æ¥å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ—¥å¿—æ±¡æŸ“
wss.on('connection', (ws) => {
  console.log('WebSocket client connected');
  
  // å‘é€çŠ¶æ€æ›´æ–°çš„å‡½æ•°
  const sendStatusUpdate = async () => {
    try {
      const [pods, services] = await Promise.all([
        executeKubectl('get pods -o json').then(output => JSON.parse(output).items),
        executeKubectl('get services -o json').then(output => JSON.parse(output).items)
      ]);
      
      const statusData = {
        type: 'status_update',
        pods,
        services,
        timestamp: new Date().toISOString()
      };
      
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify(statusData));
        console.log(`ğŸ“¡ Status update sent: ${pods.length} pods, ${services.length} services`);
      }
    } catch (error) {
      console.error('âŒ Error fetching status for WebSocket:', error);
    }
  };
  
  // ğŸš€ ä¼˜åŒ–ï¼šåªåœ¨è¿æ¥æ—¶å‘é€ä¸€æ¬¡åˆå§‹çŠ¶æ€ï¼Œä¸å†å®šæ—¶å‘é€
  sendStatusUpdate();
  
  // å­˜å‚¨WebSocketè¿æ¥ï¼Œç”¨äºæŒ‰éœ€å¹¿æ’­
  ws.isAlive = true;
  ws.lastActivity = Date.now();
  
  // å¤„ç†WebSocketæ¶ˆæ¯
  ws.on('message', (message) => {
    try {
      const data = JSON.parse(message);
      ws.lastActivity = Date.now();
      
      // ğŸ¯ æŒ‰éœ€å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
      switch (data.type) {
        case 'request_status_update':
          // å®¢æˆ·ç«¯ä¸»åŠ¨è¯·æ±‚çŠ¶æ€æ›´æ–°
          console.log('ğŸ“¡ Client requested status update');
          sendStatusUpdate();
          break;
          
        case 'start_log_stream':
          console.log(`ğŸ”„ Starting log stream for ${data.jobName}/${data.podName}`);
          startLogStream(ws, data.jobName, data.podName);
          break;
          
        case 'stop_log_stream':
          console.log(`â¹ï¸ Stopping log stream for ${data.jobName}/${data.podName}`);
          stopLogStream(ws, data.jobName, data.podName);
          break;
          
        case 'stop_all_log_streams':
          console.log('â¹ï¸ Stopping all log streams');
          stopAllLogStreams(ws);
          break;
          
        case 'ping':
          // å¿ƒè·³æ£€æµ‹
          if (ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify({ type: 'pong', timestamp: new Date().toISOString() }));
          }
          break;
          
        default:
          console.log('ğŸ“¨ Received WebSocket message:', data.type);
      }
    } catch (error) {
      console.error('âŒ Error parsing WebSocket message:', error);
    }
  });
  
  // å¿ƒè·³æ£€æµ‹
  ws.on('pong', () => {
    ws.isAlive = true;
    ws.lastActivity = Date.now();
  });
  
  ws.on('close', () => {
    console.log('ğŸ”Œ WebSocket client disconnected');
    // æ¸…ç†è¯¥è¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
    stopAllLogStreams(ws);
  });
  
  ws.on('error', (error) => {
    console.error('âŒ WebSocket error:', error);
    // æ¸…ç†è¯¥è¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
    stopAllLogStreams(ws);
  });
});

// ğŸš€ å¹¿æ’­å‡½æ•° - å‘æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯
function broadcast(message) {
  const messageStr = JSON.stringify({
    ...message,
    timestamp: new Date().toISOString()
  });
  
  let sentCount = 0;
  wss.clients.forEach((client) => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(messageStr);
      sentCount++;
    }
  });
  
  if (sentCount > 0) {
    console.log(`ğŸ“¡ Broadcast sent to ${sentCount} clients:`, message.type);
  }
}

// ğŸ”„ æŒ‰éœ€çŠ¶æ€æ›´æ–°å¹¿æ’­
function broadcastStatusUpdate() {
  const message = {
    type: 'request_status_update_broadcast',
    source: 'server'
  };
  broadcast(message);
}

// â¤ï¸ WebSocketå¿ƒè·³æ£€æµ‹ - æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡è¿æ¥çŠ¶æ€
const heartbeatInterval = setInterval(() => {
  const now = Date.now();
  let activeConnections = 0;
  
  wss.clients.forEach((ws) => {
    if (ws.readyState === WebSocket.OPEN) {
      // æ£€æŸ¥è¿æ¥æ˜¯å¦æ´»è·ƒï¼ˆ5åˆ†é’Ÿå†…æœ‰æ´»åŠ¨ï¼‰
      if (now - ws.lastActivity < 300000) {
        ws.ping();
        activeConnections++;
      } else {
        console.log('ğŸ”Œ Terminating inactive WebSocket connection');
        ws.terminate();
      }
    }
  });
  
  // åªåœ¨æœ‰è¿æ¥æ—¶è¾“å‡ºå¿ƒè·³æ—¥å¿—
  if (activeConnections > 0) {
    console.log(`â¤ï¸ WebSocket heartbeat: ${activeConnections} active connections`);
  }
}, 30000);

// ğŸ§¹ è¿›ç¨‹æ¸…ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬
process.on('SIGTERM', () => {
  console.log('ğŸ›‘ Received SIGTERM signal - Server shutting down gracefully...');
  gracefulShutdown('SIGTERM');
});

process.on('SIGINT', () => {
  console.log('ğŸ›‘ Received SIGINT signal (Ctrl+C) - Server shutting down gracefully...');
  gracefulShutdown('SIGINT');
});

process.on('uncaughtException', (error) => {
  console.error('âŒ Uncaught Exception:', error);
  console.error('Stack trace:', error.stack);
  gracefulShutdown('uncaughtException');
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('âŒ Unhandled Rejection at:', promise, 'reason:', reason);
  gracefulShutdown('unhandledRejection');
});

// ä¼˜é›…å…³é—­å‡½æ•°
function gracefulShutdown(signal) {
  console.log(`ğŸ”„ Starting graceful shutdown (signal: ${signal})...`);
  
  // æ¸…ç†WebSocketå¿ƒè·³æ£€æµ‹
  if (typeof heartbeatInterval !== 'undefined') {
    clearInterval(heartbeatInterval);
    console.log('âœ… WebSocket heartbeat interval cleared');
  }
  
  // å…³é—­WebSocketæœåŠ¡å™¨
  if (wss) {
    console.log(`ğŸ“¡ Closing WebSocket server (${wss.clients.size} active connections)...`);
    wss.close(() => {
      console.log('âœ… WebSocket server closed');
    });
  }
  
  // æ¸…ç†æ´»è·ƒçš„æ—¥å¿—æµ
  if (activeLogStreams && activeLogStreams.size > 0) {
    console.log(`ğŸ§¹ Cleaning up ${activeLogStreams.size} active log streams...`);
    activeLogStreams.clear();
    console.log('âœ… Log streams cleaned up');
  }
  
  console.log('âœ… Graceful shutdown completed');
  
  // ç»™ä¸€äº›æ—¶é—´è®©æ¸…ç†å®Œæˆï¼Œç„¶åé€€å‡º
  setTimeout(() => {
    process.exit(signal === 'uncaughtException' || signal === 'unhandledRejection' ? 1 : 0);
  }, 1000);
}

// å¯åŠ¨podæ—¥å¿—æµ
function startLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  
  // å¦‚æœå·²ç»æœ‰è¯¥podçš„æ—¥å¿—æµï¼Œå…ˆåœæ­¢å®ƒ
  if (activeLogStreams.has(streamKey)) {
    stopLogStream(ws, jobName, podName);
  }
  
  console.log(`Starting log stream for pod: ${podName} in job: ${jobName}`);
  
  // åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
  const logFilePath = ensureLogDirectory(jobName, podName);
  const logStream = fs.createWriteStream(logFilePath, { flags: 'a' });
  
  // å¯åŠ¨kubectl logså‘½ä»¤
  const logProcess = spawn('kubectl', ['logs', '-f', podName], {
    stdio: ['pipe', 'pipe', 'pipe']
  });
  
  // å­˜å‚¨è¿›ç¨‹å¼•ç”¨å’Œæ–‡ä»¶æµ
  activeLogStreams.set(streamKey, {
    process: logProcess,
    logStream: logStream,
    ws: ws,
    jobName: jobName,
    podName: podName
  });
  
  // å¤„ç†æ ‡å‡†è¾“å‡º
  logProcess.stdout.on('data', (data) => {
    const logLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    logStream.write(`[${timestamp}] ${logLine}`);
    
    // å‘é€åˆ°å‰ç«¯
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_data',
        jobName: jobName,
        podName: podName,
        data: logLine,
        timestamp: timestamp
      }));
    }
  });
  
  // å¤„ç†æ ‡å‡†é”™è¯¯
  logProcess.stderr.on('data', (data) => {
    const errorLine = data.toString();
    const timestamp = new Date().toISOString();
    
    console.error(`Log stream error for ${podName}:`, errorLine);
    
    // å†™å…¥é”™è¯¯åˆ°æ–‡ä»¶
    logStream.write(`[${timestamp}] ERROR: ${errorLine}`);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_error',
        jobName: jobName,
        podName: podName,
        error: errorLine,
        timestamp: timestamp
      }));
    }
  });
  
  // å¤„ç†è¿›ç¨‹é€€å‡º
  logProcess.on('close', (code) => {
    console.log(`Log stream for ${podName} closed with code: ${code}`);
    
    // å…³é—­æ–‡ä»¶æµ
    logStream.end();
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_closed',
        jobName: jobName,
        podName: podName,
        code: code,
        timestamp: new Date().toISOString()
      }));
    }
  });
  
  // å¤„ç†è¿›ç¨‹é”™è¯¯
  logProcess.on('error', (error) => {
    console.error(`Log stream process error for ${podName}:`, error);
    
    // å…³é—­æ–‡ä»¶æµ
    logStream.end();
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_error',
        jobName: jobName,
        podName: podName,
        error: error.message,
        timestamp: new Date().toISOString()
      }));
    }
  });
}

// åœæ­¢ç‰¹å®špodçš„æ—¥å¿—æµ
function stopLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  const streamInfo = activeLogStreams.get(streamKey);
  
  if (streamInfo) {
    console.log(`Stopping log stream for pod: ${podName}`);
    streamInfo.process.kill('SIGTERM');
    
    // å…³é—­æ–‡ä»¶æµ
    if (streamInfo.logStream) {
      streamInfo.logStream.end();
    }
    
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_stopped',
        jobName: jobName,
        podName: podName,
        timestamp: new Date().toISOString()
      }));
    }
  }
}

// åœæ­¢æŸä¸ªWebSocketè¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
function stopAllLogStreams(ws) {
  const streamsToStop = [];
  
  // ä»ç»Ÿä¸€æ—¥å¿—æµä¸­ç§»é™¤è¯¥WebSocketè¿æ¥
  unifiedLogStreams.forEach((stream, streamKey) => {
    if (stream.webSockets.has(ws)) {
      const [jobName, podName] = streamKey.split('-');
      streamsToStop.push({ jobName, podName });
    }
  });
  
  // ç§»é™¤WebSocketè¿æ¥
  streamsToStop.forEach(({ jobName, podName }) => {
    removeWebSocketFromLogStream(ws, jobName, podName);
  });
  
  if (streamsToStop.length > 0) {
    console.log(`ğŸ§¹ Cleaned up ${streamsToStop.length} log streams for disconnected WebSocket`);
  }
}

const S3StorageManager = require('./s3-storage-manager');
const s3StorageManager = new S3StorageManager();

// S3å­˜å‚¨ç®¡ç†API
app.get('/api/s3-storages', async (req, res) => {
  const result = await s3StorageManager.getStorages();
  res.json(result);
});

// è·å–S3å­˜å‚¨é»˜è®¤å€¼
app.get('/api/s3-storage-defaults', async (req, res) => {
  try {
    const fs = require('fs');
    const path = require('path');
    
    let defaultBucket = '';
    
    // ä»å®¹å™¨ä¸­çš„ /s3-workspace-metadata è·¯å¾„è¯»å–é»˜è®¤bucket
    try {
      const metadataPath = '/s3-workspace-metadata';
      if (fs.existsSync(metadataPath)) {
        const files = fs.readdirSync(metadataPath);
        const bucketFile = files.find(file => file.startsWith('CURRENT_BUCKET_'));
        if (bucketFile) {
          defaultBucket = bucketFile.replace('CURRENT_BUCKET_', '');
        }
      }
    } catch (error) {
      console.log('Could not read s3-workspace-metadata:', error.message);
    }
    
    res.json({
      success: true,
      defaults: {
        name: 's3-claim',
        bucketName: defaultBucket,
        region: 'us-west-2'
      }
    });
  } catch (error) {
    console.error('Error getting S3 storage defaults:', error);
    res.json({
      success: false,
      error: error.message,
      defaults: {
        name: 's3-claim',
        bucketName: '',
        region: 'us-west-2'
      }
    });
  }
});

app.post('/api/s3-storages', async (req, res) => {
  const result = await s3StorageManager.createStorage(req.body);
  if (result.success) {
    broadcast({
      type: 's3_storage_created',
      status: 'success',
      message: `S3 storage ${req.body.name} created successfully`
    });
  }
  res.json(result);
});

app.delete('/api/s3-storages/:name', async (req, res) => {
  const result = await s3StorageManager.deleteStorage(req.params.name);
  if (result.success) {
    broadcast({
      type: 's3_storage_deleted',
      status: 'success',
      message: `S3 storage ${req.params.name} deleted successfully`
    });
  }
  res.json(result);
});

// å¢å¼ºçš„æ¨¡å‹ä¸‹è½½API
app.post('/api/download-model-enhanced', async (req, res) => {
  try {
    const { modelId, hfToken, resources, s3Storage } = req.body;
    
    if (!modelId) {
      return res.json({ success: false, error: 'Model ID is required' });
    }

    console.log(`ğŸš€ Starting enhanced model download: ${modelId}`);
    console.log(`ğŸ“Š Resources: CPU=${resources.cpu}, Memory=${resources.memory}GB`);
    console.log(`ğŸ’¾ S3 Storage: ${s3Storage}`);

    // ç”Ÿæˆå¢å¼ºçš„ä¸‹è½½Job
    const jobResult = await s3StorageManager.generateEnhancedDownloadJob({
      modelId,
      hfToken,
      resources,
      s3Storage
    });

    if (!jobResult.success) {
      return res.json({ success: false, error: jobResult.error });
    }

    // ç¡®ä¿deploymentsç›®å½•å­˜åœ¨
    const deploymentsDir = path.join(__dirname, '..', 'deployments');
    if (!fs.existsSync(deploymentsDir)) {
      fs.mkdirSync(deploymentsDir, { recursive: true });
    }
    
    // ä¿å­˜ç”Ÿæˆçš„YAMLæ–‡ä»¶åˆ°deploymentsç›®å½•
    const modelTag = modelId.replace(/[^a-zA-Z0-9]/g, '-').toLowerCase();
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const deploymentFile = path.join(deploymentsDir, `enhanced-model-download-${modelTag}-${timestamp}.yaml`);
    await fs.writeFile(deploymentFile, jobResult.yamlContent);
    
    console.log(`ğŸ“ Saved deployment template: ${deploymentFile}`);

    // åº”ç”¨Jobåˆ°Kubernetes
    const tempFile = `/tmp/enhanced-download-job-${Date.now()}.yaml`;
    fs.writeFileSync(tempFile, jobResult.yamlContent);

    exec(`kubectl apply -f ${tempFile}`, (error, stdout, stderr) => {
      fs.removeSync(tempFile);
      
      if (error) {
        console.error('âŒ Failed to create enhanced download job:', stderr);
        broadcast({
          type: 'model_download',
          status: 'error',
          message: `Failed to start enhanced model download: ${stderr}`
        });
        return res.json({ success: false, error: stderr });
      }

      console.log('âœ… Enhanced model download job created successfully');
      broadcast({
        type: 'model_download',
        status: 'success',
        message: `Enhanced model download started: ${modelId}`,
        jobName: jobResult.jobName
      });

      res.json({ 
        success: true, 
        message: 'Enhanced model download job created successfully',
        jobName: jobResult.jobName,
        deploymentFile: path.basename(deploymentFile)
      });
    });

  } catch (error) {
    console.error('âŒ Error in enhanced model download:', error);
    res.json({ success: false, error: error.message });
  }
});

// æ¨¡å‹ä¸‹è½½API
app.post('/api/download-model', async (req, res) => {
  try {
    const { modelId, hfToken } = req.body;
    
    if (!modelId) {
      return res.json({ success: false, error: 'Model ID is required' });
    }
    
    console.log(`Starting model download for: ${modelId}`);
    
    // è¯»å–HFä¸‹è½½æ¨¡æ¿
    const templatePath = path.join(__dirname, '..', 'templates', 'hf-download-template.yaml');
    let template = await fs.readFile(templatePath, 'utf8');
    
    // ç”Ÿæˆæ¨¡å‹æ ‡ç­¾
    const modelTag = generateModelTag(modelId);
    
    // æ›¿æ¢åŸºæœ¬å˜é‡
    const replacements = {
      'HF_MODEL_ID': modelId,
      'MODEL_TAG': modelTag
    };
    
    // å¤„ç†HF Tokenç¯å¢ƒå˜é‡
    if (hfToken && hfToken.trim()) {
      const tokenEnv = `
        - name: HF_TOKEN
          value: "${hfToken.trim()}"`;
      template = template.replace('env:HF_TOKEN_ENV', `env:${tokenEnv}`);
      
      // åŒæ—¶åœ¨hf downloadå‘½ä»¤ä¸­å¯ç”¨token
      template = template.replace('#  --token=$HF_TOKEN', '          --token=$HF_TOKEN \\');
    } else {
      // ç§»é™¤HF_TOKEN_ENVå ä½ç¬¦ï¼Œä¿ç•™å…¶ä»–ç¯å¢ƒå˜é‡
      template = template.replace('      env:HF_TOKEN_ENV', '      env:');
    }
    
    // æ›¿æ¢å…¶ä»–å˜é‡
    Object.keys(replacements).forEach(key => {
      const regex = new RegExp(key, 'g');
      template = template.replace(regex, replacements[key]);
    });
    
    // ç¡®ä¿deploymentsç›®å½•å­˜åœ¨
    const deploymentsDir = path.join(__dirname, '..', 'deployments');
    if (!fs.existsSync(deploymentsDir)) {
      fs.mkdirSync(deploymentsDir, { recursive: true });
    }
    
    // ä¿å­˜ç”Ÿæˆçš„YAMLæ–‡ä»¶
    const deploymentFile = path.join(deploymentsDir, `model-download-${modelTag}.yaml`);
    await fs.writeFile(deploymentFile, template);
    
    console.log(`Generated deployment file: ${deploymentFile}`);
    
    // åº”ç”¨åˆ°Kubernetes
    try {
      const result = await executeKubectl(`apply -f "${deploymentFile}"`);
      console.log('kubectl apply result:', result);
      
      // å¹¿æ’­éƒ¨ç½²çŠ¶æ€æ›´æ–°
      broadcast({
        type: 'model_download',
        status: 'success',
        message: `Model download started for ${modelId}`,
        modelId: modelId,
        modelTag: modelTag
      });
      
      res.json({ 
        success: true, 
        message: `Model download initiated for ${modelId}`,
        modelTag: modelTag
      });
      
    } catch (kubectlError) {
      console.error('kubectl apply error:', kubectlError);
      res.json({ 
        success: false, 
        error: `Failed to apply deployment: ${kubectlError.error || kubectlError}` 
      });
    }
    
  } catch (error) {
    console.error('Model download error:', error);
    res.json({ success: false, error: error.message });
  }
});

// S3å­˜å‚¨ä¿¡æ¯API - ä»s3-pv PersistentVolumeè·å–æ¡¶ä¿¡æ¯
app.get('/api/s3-storage', async (req, res) => {
  try {
    const { storage } = req.query;
    console.log(`ğŸ“¦ Fetching S3 storage content for: ${storage || 'default'}`);
    
    // è·å–å­˜å‚¨é…ç½®
    const storageResult = await s3StorageManager.getStorages();
    if (!storageResult.success) {
      return res.json({ success: false, error: 'Failed to get storage configurations' });
    }
    
    // æ‰¾åˆ°å¯¹åº”çš„å­˜å‚¨é…ç½®
    const selectedStorage = storageResult.storages.find(s => s.pvcName === storage) || 
                           storageResult.storages.find(s => s.pvcName === 's3-claim') ||
                           storageResult.storages[0];
    
    if (!selectedStorage) {
      return res.json({ 
        success: true, 
        data: [], 
        bucketInfo: { bucket: 'No storage configured', region: 'Unknown' }
      });
    }
    
    console.log(`ğŸ“¦ Using storage: ${selectedStorage.name} -> ${selectedStorage.bucketName}`);
    
    // ä½¿ç”¨AWS CLIè·å–S3å†…å®¹
    let s3Data = [];
    const region = selectedStorage.region || 'us-west-2';
    const awsCommand = `aws s3 ls s3://${selectedStorage.bucketName}/ --region ${region}`;
    
    console.log(`ğŸ” Executing: ${awsCommand}`);
    
    try {
      const { exec } = require('child_process');
      const { promisify } = require('util');
      const execAsync = promisify(exec);
      
      const { stdout, stderr } = await execAsync(awsCommand);
      
      if (stderr) {
        console.warn('AWS CLI stderr:', stderr);
      }
      
      console.log('AWS CLI stdout:', stdout);
      
      if (stdout) {
        const lines = stdout.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          const trimmed = line.trim();
          if (!trimmed) continue;
          
          if (trimmed.startsWith('PRE ')) {
            // æ–‡ä»¶å¤¹æ ¼å¼: "PRE folder-name/"
            const folderName = trimmed.substring(4); // å»æ‰ "PRE "
            s3Data.push({
              key: folderName,
              type: 'folder',
              size: null,
              lastModified: new Date().toISOString()
            });
          } else {
            // æ–‡ä»¶æ ¼å¼: "2025-08-15 09:18:57 0 filename"
            const parts = trimmed.split(/\s+/);
            if (parts.length >= 4) {
              const date = parts[0];
              const time = parts[1];
              const size = parts[2];
              const name = parts.slice(3).join(' ');
              
              s3Data.push({
                key: name,
                type: 'file',
                size: parseInt(size) || 0,
                lastModified: new Date(`${date} ${time}`).toISOString(),
                storageClass: 'STANDARD'
              });
            }
          }
        }
      }
      
      console.log(`ğŸ“Š Found ${s3Data.length} items in S3`);
      
    } catch (awsError) {
      console.error('AWS CLI error:', awsError);
      throw new Error(`Failed to list S3 contents: ${awsError.message}`);
    }
    
    res.json({
      success: true,
      data: s3Data,
      bucketInfo: {
        bucket: selectedStorage.bucketName,
        region: selectedStorage.region || 'Unknown',
        pvcName: selectedStorage.pvcName
      }
    });
    
  } catch (error) {
    console.error('âŒ Error fetching S3 storage:', error);
    res.json({ success: false, error: error.message });
  }
});
app.get('/api/cluster/mlflow-info', (req, res) => {
  try {
    // ä½¿ç”¨å¤šé›†ç¾¤ç®¡ç†å™¨è·å–æ´»è·ƒé›†ç¾¤çš„MLflowä¿¡æ¯
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeCluster = clusterManager.getActiveCluster();
    
    if (!activeCluster) {
      return res.json({
        success: false,
        error: 'No active cluster found'
      });
    }

    // ä»æ´»è·ƒé›†ç¾¤çš„é…ç½®ç›®å½•è¯»å–MLflowä¿¡æ¯
    const configDir = clusterManager.getClusterConfigDir(activeCluster);
    const mlflowInfoPath = path.join(configDir, 'mlflow-server-info.json');
    
    if (fs.existsSync(mlflowInfoPath)) {
      const fileContent = fs.readFileSync(mlflowInfoPath, 'utf8').trim();
      
      // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
      if (!fileContent) {
        return res.json({
          success: true,
          data: {
            status: 'not_found',
            error: 'MLflow server info file is empty',
            clusterTag: activeCluster
          }
        });
      }
      
      let mlflowInfo;
      try {
        mlflowInfo = JSON.parse(fileContent);
      } catch (parseError) {
        return res.json({
          success: true,
          data: {
            status: 'error',
            error: 'Invalid JSON in MLflow server info file',
            clusterTag: activeCluster
          }
        });
      }
      
      // æ£€æŸ¥è§£æåçš„å¯¹è±¡æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
      if (!mlflowInfo || Object.keys(mlflowInfo).length === 0) {
        return res.json({
          success: true,
          data: {
            status: 'not_found',
            error: 'MLflow server info is empty',
            clusterTag: activeCluster
          }
        });
      }
      
      // è¿”å›å‰ç«¯æœŸæœ›çš„æ•°æ®ç»“æ„
      res.json({
        success: true,
        data: {
          status: 'found',
          trackingServerArn: mlflowInfo.TrackingServerArn,
          trackingServerName: mlflowInfo.TrackingServerName,
          trackingServerUrl: mlflowInfo.TrackingServerUrl,
          trackingServerStatus: mlflowInfo.TrackingServerStatus,
          isActive: mlflowInfo.IsActive,
          mlflowVersion: mlflowInfo.MlflowVersion,
          artifactStoreUri: mlflowInfo.ArtifactStoreUri,
          trackingServerSize: mlflowInfo.TrackingServerSize,
          roleArn: mlflowInfo.RoleArn,
          creationTime: mlflowInfo.CreationTime,
          clusterTag: activeCluster,
          rawData: mlflowInfo // ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡è°ƒè¯•
        }
      });
    } else {
      res.json({
        success: true,
        data: {
          status: 'not_found',
          error: `MLflow server info not found for cluster: ${activeCluster}`,
          clusterTag: activeCluster
        }
      });
    }
  } catch (error) {
    console.error('Error reading MLflow server info:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});

// æ¸…é™¤çŠ¶æ€ç¼“å­˜ API

// è·å– Step 1 çŠ¶æ€ API

// è·å–æ—¥å¿—å†…å®¹ API
// æ—§çš„æ—¥å¿—API - å·²è¢«å¤šé›†ç¾¤APIæ›¿ä»£
/*
app.get('/api/cluster/logs/:step', (req, res) => {
  try {
    const { step } = req.params;
    const { offset = 0 } = req.query;
    
    const logContent = logManager.getCurrentLogContent(step);
    const statusFile = path.join(logManager.metadataDir, `${step}_status.json`);
    
    let status = { status: 'idle' };
    if (fs.existsSync(statusFile)) {
      status = JSON.parse(fs.readFileSync(statusFile, 'utf8'));
    }

    // æ”¯æŒå¢é‡è¯»å–ï¼ˆä»æŒ‡å®šåç§»é‡å¼€å§‹ï¼‰
    const newContent = logContent.slice(parseInt(offset));
    
    res.json({
      success: true,
      data: {
        content: newContent,
        fullContent: logContent,
        totalLength: logContent.length,
        status: status.status,
        timestamp: status.timestamp,
        logId: status.logId
      }
    });

  } catch (error) {
    console.error('Error reading logs:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});
*/

// è·å–å†å²æ—¥å¿—åˆ—è¡¨
app.get('/api/cluster/logs-history', (req, res) => {
  try {
    const logFiles = fs.readdirSync(logManager.logsDir)
      .filter(file => file.endsWith('.log'))
      .map(file => {
        const filePath = path.join(logManager.logsDir, file);
        const stats = fs.statSync(filePath);
        const [timestamp, step] = file.replace('.log', '').split('_');
        
        return {
          filename: file,
          step: step,
          timestamp: timestamp.replace(/_/g, 'T').replace(/-/g, ':'),
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime
        };
      })
      .sort((a, b) => new Date(b.created) - new Date(a.created));

    res.json({
      success: true,
      data: logFiles
    });

  } catch (error) {
    console.error('Error getting log history:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});

// è·å– CloudFormation å †æ ˆçŠ¶æ€ - ä» init_envs è‡ªåŠ¨è¯»å–å †æ ˆåç§°
// ==================== å¤šé›†ç¾¤ç®¡ç† API ====================
// å¼•å…¥å¤šé›†ç¾¤ç®¡ç†æ¨¡å—
const MultiClusterAPIs = require('./multi-cluster-apis');
const MultiClusterStatus = require('./multi-cluster-status');

const multiClusterAPIs = new MultiClusterAPIs();
const multiClusterStatus = new MultiClusterStatus();

// å¤šé›†ç¾¤ç®¡ç†API
app.get('/api/multi-cluster/list', (req, res) => multiClusterAPIs.handleGetClusters(req, res));
app.post('/api/multi-cluster/switch', (req, res) => multiClusterAPIs.handleSwitchCluster(req, res));
app.post('/api/multi-cluster/switch-kubectl', (req, res) => multiClusterAPIs.handleSwitchKubectlConfig(req, res));

// é›†ç¾¤å¯¼å…¥API
app.post('/api/cluster/import', (req, res) => multiClusterAPIs.handleImportCluster(req, res));
app.post('/api/cluster/test-connection', (req, res) => multiClusterAPIs.handleTestConnection(req, res));

// é‡å†™ç°æœ‰çš„é›†ç¾¤APIä»¥æ”¯æŒå¤šé›†ç¾¤
app.post('/api/cluster/save-config', (req, res) => multiClusterAPIs.handleSaveConfig(req, res));
app.post('/api/cluster/launch', (req, res) => multiClusterAPIs.handleLaunch(req, res));
app.post('/api/cluster/configure', (req, res) => multiClusterAPIs.handleConfigure(req, res));
app.get('/api/cluster/logs/:step', (req, res) => multiClusterAPIs.handleGetLogs(req, res));
app.get('/api/cluster/logs-history', (req, res) => multiClusterAPIs.handleGetLogsHistory(req, res));
app.post('/api/cluster/clear-status-cache', (req, res) => multiClusterAPIs.handleClearStatusCache(req, res));

// é‡å†™çŠ¶æ€æ£€æŸ¥APIä»¥æ”¯æŒå¤šé›†ç¾¤
app.get('/api/cluster/step1-status', (req, res) => multiClusterStatus.handleStep1Status(req, res));
app.get('/api/cluster/step2-status', (req, res) => multiClusterStatus.handleStep2Status(req, res));
app.get('/api/cluster/cloudformation-status', (req, res) => multiClusterStatus.handleCloudFormationStatus(req, res));

// èŠ‚ç‚¹ç»„ç®¡ç†API
app.get('/api/cluster/nodegroups', async (req, res) => {
  try {
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(exec);
    const fs = require('fs');
    const path = require('path');
    
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeClusterName = clusterManager.getActiveCluster();
    
    if (!activeClusterName) {
      return res.status(400).json({ error: 'No active cluster found' });
    }

    // è¯»å–é›†ç¾¤é…ç½®æ–‡ä»¶
    const configDir = clusterManager.getClusterConfigDir(activeClusterName);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return res.status(400).json({ error: 'Cluster configuration file not found' });
    }

    // è§£æinit_envsæ–‡ä»¶ - ä½¿ç”¨shell sourceæ–¹å¼
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const clusterName = await getEnvVar('EKS_CLUSTER_NAME') || activeClusterName;
    const region = await getEnvVar('AWS_REGION') || 'us-west-2';
    
    // è·å–EKSèŠ‚ç‚¹ç»„
    const eksCmd = `aws eks list-nodegroups --cluster-name ${clusterName} --region ${region} --output json`;
    const eksResult = await execAsync(eksCmd);
    const eksData = JSON.parse(eksResult.stdout);
    
    const eksNodeGroups = [];
    for (const nodegroupName of eksData.nodegroups || []) {
      const detailCmd = `aws eks describe-nodegroup --cluster-name ${clusterName} --nodegroup-name ${nodegroupName} --region ${region} --output json`;
      const detailResult = await execAsync(detailCmd);
      const nodegroup = JSON.parse(detailResult.stdout).nodegroup;
      
      // è·å–å®ä¾‹ç±»å‹ï¼Œå¦‚æœä¸ºnullåˆ™ä»Launch Templateè·å–
      let instanceTypes = nodegroup.instanceTypes;
      if (!instanceTypes || instanceTypes.length === 0) {
        try {
          if (nodegroup.launchTemplate && nodegroup.launchTemplate.id) {
            const ltCmd = `aws ec2 describe-launch-template-versions --launch-template-id ${nodegroup.launchTemplate.id} --region ${region} --query 'LaunchTemplateVersions[0].LaunchTemplateData.InstanceType' --output json`;
            const ltResult = await execAsync(ltCmd);
            const instanceType = JSON.parse(ltResult.stdout);
            if (instanceType) {
              instanceTypes = [instanceType];
            }
          }
        } catch (ltError) {
          console.warn('Failed to get instance type from launch template:', ltError.message);
        }
      }
      
      eksNodeGroups.push({
        name: nodegroup.nodegroupName,
        status: nodegroup.status,
        instanceTypes: instanceTypes || [],
        capacityType: nodegroup.capacityType,
        scalingConfig: nodegroup.scalingConfig,
        amiType: nodegroup.amiType,
        subnets: nodegroup.subnets,
        nodeRole: nodegroup.nodeRole
      });
    }
    
    // è·å–HyperPodå®ä¾‹ç»„
    const hyperPodGroups = [];
    try {
      const hpClusterName = clusterName.replace('eks-cluster-', 'hp-cluster-');
      const hpCmd = `aws sagemaker describe-cluster --cluster-name ${hpClusterName} --region ${region} --output json`;
      const hpResult = await execAsync(hpCmd);
      const hpData = JSON.parse(hpResult.stdout);
      
      for (const instanceGroup of hpData.InstanceGroups || []) {
        hyperPodGroups.push({
          clusterName: hpClusterName,
          clusterArn: hpData.ClusterArn,
          name: instanceGroup.InstanceGroupName,
          status: instanceGroup.Status,
          instanceType: instanceGroup.InstanceType,
          currentCount: instanceGroup.CurrentCount,
          targetCount: instanceGroup.TargetCount,
          executionRole: instanceGroup.ExecutionRole
        });
      }
    } catch (hpError) {
      console.log('No HyperPod cluster found or error:', hpError.message);
    }
    
    res.json({
      eksNodeGroups,
      hyperPodInstanceGroups: hyperPodGroups
    });
  } catch (error) {
    console.error('Error fetching node groups:', error);
    res.status(500).json({ error: error.message });
  }
});

app.put('/api/cluster/nodegroups/:name/scale', async (req, res) => {
  try {
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(exec);
    const fs = require('fs');
    const path = require('path');
    
    const { name } = req.params;
    const { minSize, maxSize, desiredSize } = req.body;
    
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeClusterName = clusterManager.getActiveCluster();
    
    if (!activeClusterName) {
      return res.status(400).json({ error: 'No active cluster found' });
    }

    // è¯»å–é›†ç¾¤é…ç½®æ–‡ä»¶
    const configDir = clusterManager.getClusterConfigDir(activeClusterName);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return res.status(400).json({ error: 'Cluster configuration file not found' });
    }

    // è§£æinit_envsæ–‡ä»¶ - ä½¿ç”¨shell sourceæ–¹å¼
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const clusterName = await getEnvVar('EKS_CLUSTER_NAME') || activeClusterName;
    const region = await getEnvVar('AWS_REGION') || 'us-west-2';
    
    const cmd = `aws eks update-nodegroup-config --cluster-name ${clusterName} --nodegroup-name ${name} --scaling-config minSize=${minSize},maxSize=${maxSize},desiredSize=${desiredSize} --region ${region}`;
    
    await execAsync(cmd);
    
    // WebSocketé€šçŸ¥
    broadcast({
      type: 'nodegroup_updated',
      status: 'success',
      message: `EKS node group ${name} scaling updated successfully`
    });
    
    res.json({ success: true });
  } catch (error) {
    console.error('Error updating EKS node group:', error);
    broadcast({
      type: 'nodegroup_updated',
      status: 'error',
      message: `Failed to update EKS node group: ${error.message}`
    });
    res.status(500).json({ error: error.message });
  }
});

app.put('/api/cluster/hyperpod/instances/:name/scale', async (req, res) => {
  try {
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(exec);
    const fs = require('fs');
    const path = require('path');
    
    const { name } = req.params;
    const { targetCount } = req.body;
    
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeClusterName = clusterManager.getActiveCluster();
    
    if (!activeClusterName) {
      return res.status(400).json({ error: 'No active cluster found' });
    }

    // è¯»å–é›†ç¾¤é…ç½®æ–‡ä»¶
    const configDir = clusterManager.getClusterConfigDir(activeClusterName);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return res.status(400).json({ error: 'Cluster configuration file not found' });
    }

    // è§£æinit_envsæ–‡ä»¶ - ä½¿ç”¨shell sourceæ–¹å¼
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const clusterName = await getEnvVar('EKS_CLUSTER_NAME') || activeClusterName;
    const region = await getEnvVar('AWS_REGION') || 'us-west-2';
    const hpClusterName = clusterName.replace('eks-cluster-', 'hp-cluster-');
    
    // HyperPodéœ€è¦å®Œæ•´çš„å®ä¾‹ç»„é…ç½®ï¼Œä¸èƒ½åªæ›´æ–°InstanceCount
    // æˆ‘ä»¬éœ€è¦å…ˆè·å–å½“å‰é…ç½®ï¼Œç„¶åæ›´æ–°InstanceCount
    const getCmd = `aws sagemaker describe-cluster --cluster-name ${hpClusterName} --region ${region}`;
    const getResult = await execAsync(getCmd);
    const clusterData = JSON.parse(getResult.stdout);
    
    // æ‰¾åˆ°è¦æ›´æ–°çš„å®ä¾‹ç»„
    const instanceGroup = clusterData.InstanceGroups.find(ig => ig.InstanceGroupName === name);
    if (!instanceGroup) {
      throw new Error(`Instance group ${name} not found`);
    }
    
    // æ„å»ºæ›´æ–°å‘½ä»¤ï¼Œä½¿ç”¨å®Œæ•´çš„å®ä¾‹ç»„é…ç½®
    const updateInstanceGroup = {
      InstanceGroupName: instanceGroup.InstanceGroupName,
      InstanceType: instanceGroup.InstanceType,
      InstanceCount: targetCount,
      ExecutionRole: instanceGroup.ExecutionRole,
      LifeCycleConfig: instanceGroup.LifeCycleConfig
    };
    
    // æ·»åŠ å¯é€‰å‚æ•°
    if (instanceGroup.ThreadsPerCore) {
      updateInstanceGroup.ThreadsPerCore = instanceGroup.ThreadsPerCore;
    }
    if (instanceGroup.InstanceStorageConfigs) {
      updateInstanceGroup.InstanceStorageConfigs = instanceGroup.InstanceStorageConfigs;
    }
    
    const cmd = `aws sagemaker update-cluster --cluster-name ${hpClusterName} --instance-groups '${JSON.stringify(updateInstanceGroup)}' --region ${region}`;
    
    await execAsync(cmd);
    
    // WebSocketé€šçŸ¥
    broadcast({
      type: 'nodegroup_updated',
      status: 'success',
      message: `HyperPod instance group ${name} scaling updated successfully`
    });
    
    res.json({ success: true });
  } catch (error) {
    console.error('Error updating HyperPod instance group:', error);
    broadcast({
      type: 'nodegroup_updated',
      status: 'error',
      message: `Failed to update HyperPod instance group: ${error.message}`
    });
    res.status(500).json({ error: error.message });
  }
});

app.post('/api/cluster/hyperpod/update-software', async (req, res) => {
  try {
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(exec);
    const fs = require('fs');
    const path = require('path');
    
    const { clusterArn } = req.body;
    
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeClusterName = clusterManager.getActiveCluster();
    
    if (!activeClusterName) {
      return res.status(400).json({ error: 'No active cluster found' });
    }

    // è¯»å–é›†ç¾¤é…ç½®æ–‡ä»¶è·å–region
    const configDir = clusterManager.getClusterConfigDir(activeClusterName);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return res.status(400).json({ error: 'Cluster configuration file not found' });
    }

    // è§£æinit_envsæ–‡ä»¶ - ä½¿ç”¨shell sourceæ–¹å¼
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const region = await getEnvVar('AWS_REGION') || 'us-west-2';

    // æ‰§è¡Œupdate-cluster-softwareå‘½ä»¤
    const updateCmd = `aws sagemaker update-cluster-software --cluster-name ${clusterArn} --region ${region}`;
    
    await execAsync(updateCmd);
    
    broadcast({
      type: 'hyperpod_software_update',
      status: 'success',
      message: 'âœ… HyperPod cluster software update started successfully',
      clusterArn: clusterArn
    });

    res.json({ success: true, message: 'Cluster software update initiated successfully' });
  } catch (error) {
    console.error('Error updating HyperPod cluster software:', error);
    
    broadcast({
      type: 'hyperpod_software_update',
      status: 'error',
      message: `âŒ HyperPod software update failed: ${error.message}`
    });

    res.status(500).json({ error: error.message });
  }
});

console.log('Multi-cluster management APIs loaded');

// å¼•å…¥CIDRç”Ÿæˆå·¥å…·
const CidrGenerator = require('./utils/cidrGenerator');
const CloudFormationManager = require('./utils/cloudFormationManager');
const ClusterDependencyManager = require('./utils/clusterDependencyManager');
const ClusterManager = require('./cluster-manager');
const clusterManager = new ClusterManager();

// CIDRç”Ÿæˆç›¸å…³API
app.get('/api/cluster/generate-cidr', async (req, res) => {
  try {
    const { region, excludeCidr } = req.query;
    
    if (!region) {
      return res.status(400).json({ error: 'AWS region is required' });
    }
    
    const cidr = await CidrGenerator.generateUniqueCidr(region, excludeCidr);
    
    res.json({
      success: true,
      cidr,
      region,
      generatedAt: new Date().toISOString()
    });
  } catch (error) {
    console.error('Error generating CIDR:', error);
    res.status(500).json({ error: error.message });
  }
});

// ç”Ÿæˆå®Œæ•´CIDRé…ç½®
app.post('/api/cluster/generate-cidr-config', async (req, res) => {
  try {
    const { region, customVpcCidr } = req.body;
    
    if (!region) {
      return res.status(400).json({ error: 'AWS region is required' });
    }
    
    const cidrConfig = await CidrGenerator.generateFullCidrConfiguration(region, customVpcCidr);
    
    res.json({
      success: true,
      ...cidrConfig
    });
  } catch (error) {
    console.error('Error generating CIDR configuration:', error);
    res.status(500).json({ error: error.message });
  }
});

// éªŒè¯CIDRæ ¼å¼å’Œå†²çª
app.post('/api/cluster/validate-cidr', async (req, res) => {
  try {
    const { cidr, region } = req.body;
    
    if (!cidr || !region) {
      return res.status(400).json({ error: 'CIDR and region are required' });
    }
    
    // éªŒè¯æ ¼å¼
    const isValidFormat = CidrGenerator.validateCidrFormat(cidr);
    if (!isValidFormat) {
      return res.json({
        success: false,
        valid: false,
        error: 'Invalid CIDR format'
      });
    }
    
    // æ£€æŸ¥å†²çª
    const hasConflict = await CidrGenerator.checkCidrConflict(cidr, region);
    
    res.json({
      success: true,
      valid: !hasConflict,
      conflict: hasConflict,
      message: hasConflict ? 'CIDR conflicts with existing VPC' : 'CIDR is available'
    });
  } catch (error) {
    console.error('Error validating CIDR:', error);
    res.status(500).json({ error: error.message });
  }
});

console.log('CIDR generation APIs loaded');

// EKSé›†ç¾¤åˆ›å»ºç›¸å…³API
app.post('/api/cluster/create-eks', async (req, res) => {
  try {
    const { clusterTag, awsRegion, customVpcCidr } = req.body;
    
    // éªŒè¯å¿…å¡«å­—æ®µ
    if (!clusterTag || !awsRegion) {
      return res.status(400).json({ error: 'Missing required fields: clusterTag and awsRegion' });
    }
    
    // ç”ŸæˆCIDRé…ç½®
    const cidrConfig = await CidrGenerator.generateFullCidrConfiguration(awsRegion, customVpcCidr);
    
    // ç«‹å³åˆ›å»ºé›†ç¾¤ç›®å½•å’ŒçŠ¶æ€è®°å½•ï¼ˆåœ¨CloudFormationè°ƒç”¨å‰ï¼‰
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
    const stackName = `full-stack-${clusterTag}-${timestamp}`;
    
    const clusterConfig = {
      clusterTag,
      awsRegion,
      customVpcCidr: customVpcCidr || 'auto-generated'
    };
    
    // åˆ›å»ºé›†ç¾¤ç›®å½•ç»“æ„
    clusterManager.createClusterDirs(clusterTag);
    
    // ç«‹å³ä¿å­˜ç”¨æˆ·è¾“å…¥å’ŒCIDRé…ç½®
    const metadataDir = clusterManager.getClusterMetadataDir(clusterTag);
    const fs = require('fs');
    const path = require('path');
    
    // æ·»åŠ åˆ°creating-clustersè·Ÿè¸ªæ–‡ä»¶
    const creatingClustersPath = path.join(__dirname, '../managed_clusters_info/creating-clusters.json');
    let creatingClusters = {};
    if (fs.existsSync(creatingClustersPath)) {
      creatingClusters = JSON.parse(fs.readFileSync(creatingClustersPath, 'utf8'));
    }
    creatingClusters[clusterTag] = {
      type: 'eks',
      status: 'IN_PROGRESS',
      createdAt: new Date().toISOString(),
      stackName: stackName,
      region: awsRegion
    };
    fs.writeFileSync(creatingClustersPath, JSON.stringify(creatingClusters, null, 2));
    
    // ä¿å­˜ç”¨æˆ·è¾“å…¥ä¿¡æ¯
    fs.writeFileSync(
      path.join(metadataDir, 'user_input.json'),
      JSON.stringify({
        clusterTag,
        awsRegion,
        customVpcCidr: customVpcCidr || null,
        inputAt: new Date().toISOString()
      }, null, 2)
    );
    
    // ä¿å­˜CIDRé…ç½®
    fs.writeFileSync(
      path.join(metadataDir, 'cidr_configuration.json'),
      JSON.stringify(cidrConfig, null, 2)
    );
    
    // ä¿å­˜åˆ›å»ºçŠ¶æ€
    fs.writeFileSync(
      path.join(metadataDir, 'creation_status.json'),
      JSON.stringify({
        status: 'IN_PROGRESS',
        createdAt: new Date().toISOString(),
        lastUpdated: new Date().toISOString(),
        stackName: stackName,
        region: awsRegion,
        phase: 'CLOUDFORMATION_CREATING'
      }, null, 2)
    );
    
    // åˆ›å»ºCloudFormation Stack
    const stackResult = await CloudFormationManager.createEKSStack({
      clusterTag,
      awsRegion,
      stackName
    }, cidrConfig);
    
    // æ›´æ–°åˆ›å»ºçŠ¶æ€ï¼Œæ·»åŠ Stack ID
    fs.writeFileSync(
      path.join(metadataDir, 'creation_status.json'),
      JSON.stringify({
        status: 'IN_PROGRESS',
        createdAt: new Date().toISOString(),
        lastUpdated: new Date().toISOString(),
        stackName: stackName,
        stackId: stackResult.stackId,
        region: awsRegion,
        phase: 'CLOUDFORMATION_IN_PROGRESS'
      }, null, 2)
    );
    
    // æ›´æ–°creating-clustersè·Ÿè¸ªæ–‡ä»¶
    creatingClusters[clusterTag].stackId = stackResult.stackId;
    creatingClusters[clusterTag].phase = 'CLOUDFORMATION_IN_PROGRESS';
    creatingClusters[clusterTag].lastUpdated = new Date().toISOString();
    fs.writeFileSync(creatingClustersPath, JSON.stringify(creatingClusters, null, 2));
    
    await clusterManager.saveCreationConfig(clusterTag, clusterConfig, cidrConfig, stackResult);
    
    // å‘é€WebSocketé€šçŸ¥
    broadcast({
      type: 'cluster_creation_started',
      status: 'success',
      message: `EKS cluster creation started: ${clusterTag}`,
      clusterTag,
      stackName: stackResult.stackName
    });
    
    res.json({
      success: true,
      clusterTag,
      stackName: stackResult.stackName,
      stackId: stackResult.stackId,
      cidrConfig,
      message: 'EKS cluster creation started successfully'
    });
  } catch (error) {
    console.error('Error creating EKS cluster:', error);
    
    broadcast({
      type: 'cluster_creation_started',
      status: 'error',
      message: `Failed to create EKS cluster: ${error.message}`
    });
    
    res.status(500).json({ error: error.message });
  }
});

// HyperPodåˆ›å»ºçŠ¶æ€ç®¡ç†
const CREATING_HYPERPOD_CLUSTERS_FILE = path.join(__dirname, '../managed_clusters_info/creating-hyperpod-clusters.json');

function getCreatingHyperPodClusters() {
  try {
    if (fs.existsSync(CREATING_HYPERPOD_CLUSTERS_FILE)) {
      return JSON.parse(fs.readFileSync(CREATING_HYPERPOD_CLUSTERS_FILE, 'utf8'));
    }
    return {};
  } catch (error) {
    console.error('Error reading creating HyperPod clusters file:', error);
    return {};
  }
}

function updateCreatingHyperPodStatus(clusterTag, statusUpdate) {
  try {
    const creatingClusters = getCreatingHyperPodClusters();
    
    if (statusUpdate === 'COMPLETED') {
      // ç§»é™¤å·²å®Œæˆçš„é›†ç¾¤
      delete creatingClusters[clusterTag];
    } else {
      // æ›´æ–°æˆ–æ·»åŠ é›†ç¾¤çŠ¶æ€
      creatingClusters[clusterTag] = {
        ...creatingClusters[clusterTag],
        ...statusUpdate,
        lastUpdated: new Date().toISOString()
      };
    }
    
    fs.writeFileSync(CREATING_HYPERPOD_CLUSTERS_FILE, JSON.stringify(creatingClusters, null, 2));
  } catch (error) {
    console.error('Error updating creating HyperPod clusters status:', error);
  }
}

async function registerCompletedHyperPod(clusterTag) {
  try {
    console.log(`HyperPod cluster ${clusterTag} creation completed`);
    
    // é…ç½®HyperPodè‡ªå®šä¹‰ä¾èµ–
    await HyperPodDependencyManager.configureHyperPodDependencies(clusterTag, clusterManager);
    
  } catch (error) {
    console.error('Error registering completed HyperPod:', error);
    // ä¸æŠ›å‡ºé”™è¯¯ï¼Œé¿å…å½±å“ä¸»æµç¨‹
  }
}

// è¾…åŠ©å‡½æ•°ï¼šæ›´æ–°creating-clustersçŠ¶æ€
function updateCreatingClustersStatus(clusterTag, status, additionalData = {}) {
  const fs = require('fs');
  const path = require('path');
  const creatingClustersPath = path.join(__dirname, '../managed_clusters_info/creating-clusters.json');
  
  let creatingClusters = {};
  if (fs.existsSync(creatingClustersPath)) {
    creatingClusters = JSON.parse(fs.readFileSync(creatingClustersPath, 'utf8'));
  }
  
  if (creatingClusters[clusterTag]) {
    if (status === 'COMPLETED' || status === 'FAILED') {
      // åˆ›å»ºå®Œæˆæˆ–å¤±è´¥ï¼Œä»è·Ÿè¸ªæ–‡ä»¶ä¸­ç§»é™¤
      delete creatingClusters[clusterTag];
    } else {
      // æ›´æ–°çŠ¶æ€
      creatingClusters[clusterTag] = {
        ...creatingClusters[clusterTag],
        status: status,
        lastUpdated: new Date().toISOString(),
        ...additionalData
      };
    }
    fs.writeFileSync(creatingClustersPath, JSON.stringify(creatingClusters, null, 2));
  }
}

// è·å–æ­£åœ¨åˆ›å»ºçš„é›†ç¾¤åˆ—è¡¨
app.get('/api/cluster/creating-clusters', async (req, res) => {
  try {
    const fs = require('fs');
    const path = require('path');
    const creatingClustersPath = path.join(__dirname, '../managed_clusters_info/creating-clusters.json');
    
    if (!fs.existsSync(creatingClustersPath)) {
      return res.json({ success: true, clusters: {} });
    }
    
    const creatingClusters = JSON.parse(fs.readFileSync(creatingClustersPath, 'utf8'));
    
    // æ£€æŸ¥çŠ¶æ€å¹¶å¤„ç†å®Œæˆçš„é›†ç¾¤
    for (const [clusterTag, clusterInfo] of Object.entries(creatingClusters)) {
      if (clusterInfo.type === 'eks' && clusterInfo.stackName) {
        try {
          const stackStatus = await CloudFormationManager.getStackStatus(clusterInfo.stackName, clusterInfo.region);
          clusterInfo.currentStackStatus = stackStatus.stackStatus;
          
          // å¦‚æœEKSé›†ç¾¤åˆ›å»ºå®Œæˆï¼Œè§¦å‘ä¾èµ–é…ç½®
          if (stackStatus.stackStatus === 'CREATE_COMPLETE') {
            console.log(`EKS cluster ${clusterTag} creation completed, configuring dependencies...`);
            
            // å¼‚æ­¥é…ç½®ä¾èµ–ï¼Œä¸é˜»å¡APIå“åº”
            setImmediate(async () => {
              try {
                await configureClusterDependencies(clusterTag);
              } catch (error) {
                console.error(`Failed to configure dependencies for ${clusterTag}:`, error);
              }
            });
          } else if (stackStatus.stackStatus.includes('FAILED') || stackStatus.stackStatus.includes('ROLLBACK')) {
            // åˆ›å»ºå¤±è´¥ï¼Œæ¸…ç†çŠ¶æ€
            console.log(`EKS cluster ${clusterTag} creation failed, cleaning up...`);
            updateCreatingClustersStatus(clusterTag, 'COMPLETED');
          }
          
        } catch (error) {
          console.error(`Error checking status for cluster ${clusterTag}:`, error);
          clusterInfo.currentStackStatus = 'UNKNOWN';
          
          // å¦‚æœstackä¸å­˜åœ¨ï¼Œæ¸…ç†çŠ¶æ€
          if (error.message && error.message.includes('does not exist')) {
            console.log(`Stack for ${clusterTag} does not exist, cleaning up...`);
            updateCreatingClustersStatus(clusterTag, 'COMPLETED');
          }
        }
      }
    }
    
    res.json({ success: true, clusters: creatingClusters });
  } catch (error) {
    console.error('Error getting creating clusters:', error);
    res.status(500).json({ error: error.message });
  }
});

// é…ç½®é›†ç¾¤ä¾èµ–ï¼ˆhelmç­‰ï¼‰
async function configureClusterDependencies(clusterTag) {
  try {
    console.log(`Configuring dependencies for cluster: ${clusterTag}`);
    
    // ä½¿ç”¨ClusterDependencyManagerè¿›è¡Œé…ç½®
    await ClusterDependencyManager.configureClusterDependencies(clusterTag, clusterManager);
    
    console.log(`Successfully configured dependencies for cluster: ${clusterTag}`);
    
    // æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
    updateCreatingClustersStatus(clusterTag, 'COMPLETED');
    
    // æ³¨å†Œå®Œæˆçš„é›†ç¾¤
    await registerCompletedCluster(clusterTag);
    
  } catch (error) {
    console.error(`Error configuring dependencies for cluster ${clusterTag}:`, error);
    updateCreatingClustersStatus(clusterTag, 'DEPENDENCY_CONFIG_FAILED', { error: error.message });
    
    // å³ä½¿ä¾èµ–é…ç½®å¤±è´¥ï¼Œä¹Ÿè¦æ³¨å†Œé›†ç¾¤ï¼ˆè®©ç”¨æˆ·èƒ½çœ‹åˆ°é›†ç¾¤ï¼‰
    console.log(`Registering cluster ${clusterTag} despite dependency configuration failure`);
    try {
      await registerCompletedCluster(clusterTag, 'dependency-failed');
    } catch (registerError) {
      console.error(`Failed to register cluster ${clusterTag} after dependency failure:`, registerError);
    }
    
    throw error;
  }
}

// æ³¨å†Œå®Œæˆçš„é›†ç¾¤åˆ°å¯é€‰åˆ—è¡¨
async function registerCompletedCluster(clusterTag, status = 'active') {
  try {
    console.log(`Registering completed cluster: ${clusterTag}`);
    
    const fs = require('fs');
    const path = require('path');
    
    // è¯»å–åˆ›å»ºæ—¶çš„metadata
    const metadataDir = clusterManager.getClusterMetadataDir(clusterTag);
    const creationMetadataPath = path.join(metadataDir, 'creation_metadata.json');
    
    if (!fs.existsSync(creationMetadataPath)) {
      console.error(`Creation metadata not found for cluster: ${clusterTag}`);
      return;
    }
    
    const creationMetadata = JSON.parse(fs.readFileSync(creationMetadataPath, 'utf8'));
    
    // ç”Ÿæˆcluster_info.jsonï¼ˆå…¼å®¹ç°æœ‰æ ¼å¼ï¼‰
    const clusterInfo = {
      clusterTag: clusterTag,
      region: creationMetadata.userConfig.awsRegion,
      status: status, // ä½¿ç”¨ä¼ å…¥çš„çŠ¶æ€
      type: 'created',
      createdAt: creationMetadata.createdAt,
      lastModified: new Date().toISOString(),
      source: 'ui-panel-creation',
      cloudFormation: {
        stackName: creationMetadata.cloudFormation.stackName,
        stackId: creationMetadata.cloudFormation.stackId
      }
    };
    
    // ä¿å­˜cluster_info.json
    const clusterInfoPath = path.join(metadataDir, 'cluster_info.json');
    fs.writeFileSync(clusterInfoPath, JSON.stringify(clusterInfo, null, 2));
    
    console.log(`Successfully registered cluster: ${clusterTag}`);
    
    // è‡ªåŠ¨è®¾ç½®ä¸ºactive cluster
    try {
      await clusterManager.setActiveCluster(clusterTag);
      console.log(`Set ${clusterTag} as active cluster`);
    } catch (error) {
      console.error(`Failed to set ${clusterTag} as active cluster:`, error);
    }
    
    // å‘é€WebSocketé€šçŸ¥
    broadcast({
      type: 'cluster_creation_completed',
      status: 'success',
      message: `EKS cluster created and registered: ${clusterTag}`,
      clusterTag: clusterTag
    });
    
  } catch (error) {
    console.error(`Failed to register completed cluster ${clusterTag}:`, error);
  }
}

// æ¸…ç†creating metadataï¼ˆä¸è§¦ç¢°CloudFormationï¼‰
function cleanupCreatingMetadata(clusterTag) {
  const fs = require('fs');
  const path = require('path');
  
  try {
    console.log(`Cleaning up creating metadata for: ${clusterTag}`);
    
    // ä»creating-clusters.jsonä¸­ç§»é™¤
    const creatingClustersPath = path.join(__dirname, '../managed_clusters_info/creating-clusters.json');
    if (fs.existsSync(creatingClustersPath)) {
      const creatingClusters = JSON.parse(fs.readFileSync(creatingClustersPath, 'utf8'));
      delete creatingClusters[clusterTag];
      fs.writeFileSync(creatingClustersPath, JSON.stringify(creatingClusters, null, 2));
    }
    
    // åˆ é™¤é›†ç¾¤ç›®å½•ï¼ˆæ¢å¤åˆ°ç©ºç™½çŠ¶æ€ï¼‰
    const clusterDir = path.join(__dirname, '../managed_clusters_info', clusterTag);
    if (fs.existsSync(clusterDir)) {
      fs.rmSync(clusterDir, { recursive: true, force: true });
      console.log(`Removed cluster directory: ${clusterDir}`);
    }
    
    console.log(`Successfully cleaned up metadata for cluster: ${clusterTag}`);
    
  } catch (error) {
    console.error(`Error cleaning up metadata for cluster ${clusterTag}:`, error);
  }
}

// æ£€æŸ¥é›†ç¾¤ä¾èµ–é…ç½®çŠ¶æ€
app.get('/api/cluster/dependency-status/:clusterTag', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    
    const clusterDir = clusterManager.getClusterDir(clusterTag);
    const configDir = path.join(clusterDir, 'config');
    
    if (!fs.existsSync(configDir)) {
      return res.status(404).json({ error: 'Cluster not found' });
    }
    
    const status = await ClusterDependencyManager.checkDependencyStatus(configDir);
    
    res.json({
      success: true,
      clusterTag,
      dependencyStatus: status
    });
    
  } catch (error) {
    console.error('Error checking dependency status:', error);
    res.status(500).json({ error: error.message });
  }
});

// æ‰‹åŠ¨é‡æ–°é…ç½®é›†ç¾¤ä¾èµ–ï¼ˆç”¨äºè°ƒè¯•ï¼‰
app.post('/api/cluster/reconfigure-dependencies/:clusterTag', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    
    console.log(`Manual reconfiguration requested for cluster: ${clusterTag}`);
    
    // å…ˆæ¸…ç†ç°æœ‰é…ç½®
    const clusterDir = clusterManager.getClusterDir(clusterTag);
    const configDir = path.join(clusterDir, 'config');
    
    await ClusterDependencyManager.cleanupDependencies(configDir);
    
    // é‡æ–°é…ç½®
    await ClusterDependencyManager.configureClusterDependencies(clusterTag, clusterManager);
    
    res.json({
      success: true,
      message: `Successfully reconfigured dependencies for cluster: ${clusterTag}`
    });
    
  } catch (error) {
    console.error('Error reconfiguring dependencies:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–é›†ç¾¤åˆ›å»ºçŠ¶æ€
app.get('/api/cluster/creation-status/:clusterTag', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    const fs = require('fs');
    const path = require('path');
    
    // è¯»å–åˆ›å»ºmetadataè·å–regionå’Œstackä¿¡æ¯
    const metadataDir = clusterManager.getClusterMetadataDir(clusterTag);
    const creationStatusPath = path.join(metadataDir, 'creation_status.json');
    
    if (!fs.existsSync(creationStatusPath)) {
      return res.status(404).json({ error: 'Creation status not found' });
    }
    
    const creationStatus = JSON.parse(fs.readFileSync(creationStatusPath, 'utf8'));
    const stackName = creationStatus.stackName;
    const region = creationStatus.region;
    
    if (!stackName || !region) {
      return res.status(400).json({ error: 'Missing stack name or region in metadata' });
    }
    
    const stackStatus = await CloudFormationManager.getStackStatus(stackName, region);
    
    res.json({
      success: true,
      clusterTag,
      stackName,
      region,
      ...stackStatus
    });
  } catch (error) {
    console.error('Error getting cluster creation status:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–é›†ç¾¤åˆ›å»ºæ—¥å¿—
app.get('/api/cluster/creation-logs/:clusterTag', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    
    // è¯»å–é›†ç¾¤é…ç½®
    const clusterInfo = await clusterManager.getClusterInfo(clusterTag);
    if (!clusterInfo) {
      return res.status(404).json({ error: 'Cluster not found' });
    }
    
    const stackName = `full-stack-${clusterTag}`;
    const events = await CloudFormationManager.getStackEvents(stackName, clusterInfo.awsRegion);
    
    res.json({
      success: true,
      clusterTag,
      stackName,
      events
    });
  } catch (error) {
    console.error('Error getting cluster creation logs:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–é›†ç¾¤ä¿¡æ¯API
app.get('/api/cluster/info', async (req, res) => {
  try {
    const { promisify } = require('util');
    const execAsync = promisify(require('child_process').exec);
    
    const activeCluster = clusterManager.getActiveCluster();
    if (!activeCluster) {
      return res.status(400).json({ success: false, error: 'No active cluster selected' });
    }
    
    const initEnvsPath = path.join(__dirname, '../managed_clusters_info', activeCluster, 'config/init_envs');
    const stackEnvsPath = path.join(__dirname, '../managed_clusters_info', activeCluster, 'config/stack_envs');
    
    const getEnvVar = async (varName, filePath = initEnvsPath) => {
      try {
        const cmd = `source ${filePath} && echo $${varName}`;
        const result = await execAsync(cmd, { shell: '/bin/bash' });
        return result.stdout.trim();
      } catch (error) {
        return null;
      }
    };
    
    const eksClusterName = await getEnvVar('EKS_CLUSTER_NAME');
    const region = await getEnvVar('AWS_REGION');
    const vpcId = await getEnvVar('VPC_ID', stackEnvsPath);
    
    res.json({
      success: true,
      activeCluster,
      eksClusterName,
      region,
      vpcId
    });
  } catch (error) {
    console.error('Error getting cluster info:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// è·å–å¯ç”¨åŒºåˆ—è¡¨API
app.get('/api/cluster/availability-zones', async (req, res) => {
  try {
    const { execSync } = require('child_process');
    const { region } = req.query;
    if (!region) {
      return res.status(400).json({ success: false, error: 'Region parameter required' });
    }
    
    const command = `aws ec2 describe-availability-zones --region ${region} --query 'AvailabilityZones[*].{ZoneName:ZoneName,ZoneId:ZoneId}' --output json`;
    const result = execSync(command, { encoding: 'utf8' });
    const zones = JSON.parse(result);
    
    res.json({
      success: true,
      zones
    });
  } catch (error) {
    console.error('Error getting availability zones:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// HyperPodé›†ç¾¤åˆ›å»ºAPI
app.post('/api/cluster/create-hyperpod', async (req, res) => {
  try {
    const { execSync } = require('child_process');
    const { promisify } = require('util');
    const execAsync = promisify(require('child_process').exec);
    const path = require('path');
    
    const { userConfig } = req.body;
    
    // è·å–å½“å‰æ´»è·ƒé›†ç¾¤ä¿¡æ¯
    const activeCluster = clusterManager.getActiveCluster();
    console.log('Active cluster:', activeCluster);
    if (!activeCluster) {
      return res.status(400).json({ success: false, error: 'No active cluster selected' });
    }
    
    // æ£€æŸ¥kubectlè¿é€šæ€§
    try {
      console.log('Testing kubectl connectivity...');
      const kubectlResult = execSync('kubectl cluster-info', { encoding: 'utf8', timeout: 15000 });
      console.log('kubectl cluster-info result:', kubectlResult);
    } catch (error) {
      console.error('kubectl connectivity test failed:', error.message);
      console.error('kubectl error details:', error);
      return res.status(400).json({ 
        success: false, 
        error: `EKS cluster not accessible via kubectl: ${error.message}` 
      });
    }
    
    // è·å–EKSé›†ç¾¤ä¿¡æ¯
    const initEnvsPath = path.join(__dirname, '../managed_clusters_info', activeCluster, 'config/init_envs');
    console.log('Init envs path:', initEnvsPath);
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const eksStackName = await getEnvVar('CLOUD_FORMATION_FULL_STACK_NAME');
    const region = await getEnvVar('AWS_REGION');
    
    if (!eksStackName || !region) {
      return res.status(400).json({ success: false, error: 'Missing EKS cluster configuration' });
    }
    
    // ç”ŸæˆHyperPodé…ç½®
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').split('.')[0];
    const clusterTag = userConfig.clusterTag;
    const hyperPodStackName = `hyperpod-${clusterTag}-${timestamp}`;
    
    // ä½¿ç”¨EKSé›†ç¾¤æ ‡è¯†ä½œä¸ºHyperPodé›†ç¾¤åç§°ï¼Œä¿æŒå‘½åä¸€è‡´æ€§
    const eksClusterTag = activeCluster; // ä½¿ç”¨EKSé›†ç¾¤çš„æ ‡è¯†
    const hyperPodClusterName = `hp-cluster-${eksClusterTag}`;
    
    // è·å–å¯ç”¨åŒºID
    const azCommand = `aws ec2 describe-availability-zones --region ${region} --query "AvailabilityZones[?ZoneName=='${userConfig.availabilityZone}'].ZoneId" --output text`;
    const azResult = execSync(azCommand, { encoding: 'utf8' });
    const availabilityZoneId = azResult.trim();
    
    // è·å–EKSåŸºç¡€è®¾æ–½ä¿¡æ¯
    const stackInfo = await CloudFormationManager.fetchStackInfo(eksStackName, region);
    
    // ç”ŸæˆCIDRé…ç½®
    const cidrResponse = await fetch('http://localhost:3001/api/cluster/generate-cidr-config', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ 
        region: region
      })
    });
    const cidrConfig = await cidrResponse.json();
    
    console.log('Generated CIDR config:', cidrConfig);
    
    if (!cidrConfig.hyperPodPrivateSubnetCidr) {
      throw new Error('Failed to generate private subnet CIDR');
    }
    
    // è®°å½•åˆ›å»ºçŠ¶æ€
    updateCreatingHyperPodStatus(activeCluster, {
      type: 'hyperpod',
      status: 'IN_PROGRESS',
      phase: 'CREATING_STACK',
      stackName: hyperPodStackName,
      region: region,
      createdAt: new Date().toISOString()
    });
    
    // æ„å»ºHyperPodé…ç½®
    const hyperPodConfig = {
      ResourceNamePrefix: clusterTag,
      AvailabilityZoneId: availabilityZoneId,
      PrivateSubnet1CIDR: cidrConfig.hyperPodPrivateSubnetCidr,
      HyperPodClusterName: hyperPodClusterName,
      NodeRecovery: 'None',
      UseContinuousNodeProvisioningMode: 'false',
      CreateAcceleratedInstanceGroup: 'true',
      AcceleratedInstanceGroupName: `accelerated-${clusterTag}`,
      AcceleratedInstanceType: userConfig.AcceleratedInstanceType,
      AcceleratedInstanceCount: userConfig.AcceleratedInstanceCount,
      AcceleratedEBSVolumeSize: userConfig.AcceleratedEBSVolumeSize,
      AcceleratedTrainingPlanArn: userConfig.AcceleratedTrainingPlanArn || '',
      // è‡ªåŠ¨è®¾ç½®threads per coreï¼šæœ‰training planæ—¶ä¸º2ï¼Œå¦åˆ™ä¸º1
      AcceleratedThreadsPerCore: userConfig.AcceleratedTrainingPlanArn ? 2 : 1,
      EnableInstanceStressCheck: 'false',
      EnableInstanceConnectivityCheck: 'false'
    };
    
    const stackResult = await CloudFormationManager.createHyperPodStack(
      hyperPodStackName,
      region,
      stackInfo,
      hyperPodConfig
    );
    
    // ä¿å­˜é…ç½®åˆ°metadata
    await clusterManager.saveHyperPodConfig(activeCluster, {
      stackName: hyperPodStackName,
      stackId: stackResult.stackId,
      region: region,
      userConfig: userConfig,
      infrastructureInfo: stackInfo,
      createdAt: new Date().toISOString()
    });
    
    // å‘é€WebSocketé€šçŸ¥
    broadcast({
      type: 'hyperpod_creation_started',
      status: 'success',
      message: `HyperPod cluster creation started: ${hyperPodStackName}`,
      clusterTag: activeCluster,
      stackName: hyperPodStackName
    });
    
    res.json({
      success: true,
      stackName: hyperPodStackName,
      stackId: stackResult.stackId,
      message: 'HyperPod cluster creation started'
    });
    
  } catch (error) {
    console.error('Error creating HyperPod cluster:', error);
    
    // æ›´æ–°å¤±è´¥çŠ¶æ€
    const activeCluster = clusterManager.getActiveCluster();
    if (activeCluster) {
      updateCreatingHyperPodStatus(activeCluster, {
        status: 'FAILED',
        error: error.message,
        failedAt: new Date().toISOString()
      });
    }
    
    broadcast({
      type: 'hyperpod_creation_failed',
      status: 'error',
      message: `HyperPod creation failed: ${error.message}`
    });
    
    res.status(500).json({ success: false, error: error.message });
  }
});

// è·å–EKSèŠ‚ç‚¹ç»„åˆ›å»ºæ‰€éœ€çš„å­ç½‘ä¿¡æ¯
app.get('/api/cluster/subnets', async (req, res) => {
  try {
    const { promisify } = require('util');
    const execAsync = promisify(require('child_process').exec);
    const path = require('path');
    
    // è·å–å½“å‰æ´»è·ƒé›†ç¾¤ä¿¡æ¯
    const activeCluster = clusterManager.getActiveCluster();
    if (!activeCluster) {
      return res.status(400).json({ success: false, error: 'No active cluster selected' });
    }
    
    // è·å–é›†ç¾¤é…ç½®
    const initEnvsPath = path.join(__dirname, '../managed_clusters_info', activeCluster, 'config/init_envs');
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const eksStackName = await getEnvVar('CLOUD_FORMATION_FULL_STACK_NAME');
    const region = await getEnvVar('AWS_REGION');
    const eksClusterName = await getEnvVar('EKS_CLUSTER_NAME');
    
    if (!eksStackName || !region) {
      return res.status(400).json({ success: false, error: 'Missing EKS cluster configuration' });
    }
    
    // è·å–CloudFormationè¾“å‡ºä¿¡æ¯
    const stackInfo = await CloudFormationManager.fetchStackInfo(eksStackName, region);
    const vpcId = stackInfo.VPC_ID;
    
    if (!vpcId) {
      return res.status(400).json({ success: false, error: 'VPC ID not found in stack outputs' });
    }
    
    // è·å–å­ç½‘ä¿¡æ¯
    const subnetInfo = await CloudFormationManager.fetchSubnetInfo(vpcId, region);
    
    // è·å–HyperPodä½¿ç”¨çš„å­ç½‘
    let hyperPodSubnets = [];
    try {
      const hpClusterName = eksClusterName.replace('eks-cluster-', 'hp-cluster-');
      const hpCmd = `aws sagemaker describe-cluster --cluster-name ${hpClusterName} --region ${region} --output json`;
      const hpResult = await execAsync(hpCmd);
      const hpData = JSON.parse(hpResult.stdout);
      
      // ä»VpcConfigä¸­æå–HyperPodä½¿ç”¨çš„å­ç½‘ID
      if (hpData.VpcConfig && hpData.VpcConfig.Subnets) {
        hyperPodSubnets = hpData.VpcConfig.Subnets;
        console.log('Found HyperPod subnets:', hyperPodSubnets);
      }
    } catch (error) {
      console.log('No HyperPod cluster found or error fetching HyperPod subnets:', error.message);
    }
    
    // æ ‡è®°HyperPodä½¿ç”¨çš„å­ç½‘
    const markedSubnets = {
      publicSubnets: subnetInfo.publicSubnets,
      privateSubnets: subnetInfo.privateSubnets.map(subnet => ({
        ...subnet,
        isHyperPodSubnet: hyperPodSubnets.includes(subnet.subnetId)
      })),
      hyperPodSubnets: hyperPodSubnets
    };
    
    res.json({
      success: true,
      data: {
        eksClusterName,
        region,
        vpcId,
        ...markedSubnets
      }
    });
    
  } catch (error) {
    console.error('Error fetching subnets:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// åˆ›å»ºEKSèŠ‚ç‚¹ç»„
app.post('/api/cluster/create-nodegroup', async (req, res) => {
  try {
    const { promisify } = require('util');
    const execAsync = promisify(require('child_process').exec);
    const { spawn } = require('child_process');
    const path = require('path');
    const EksNodeGroupDependencyManager = require('./utils/eksNodeGroupDependencyManager');
    
    const { userConfig } = req.body;
    
    // è·å–å½“å‰æ´»è·ƒé›†ç¾¤ä¿¡æ¯
    const activeCluster = clusterManager.getActiveCluster();
    if (!activeCluster) {
      return res.status(400).json({ success: false, error: 'No active cluster selected' });
    }
    
    // è·å–é›†ç¾¤é…ç½®
    const initEnvsPath = path.join(__dirname, '../managed_clusters_info', activeCluster, 'config/init_envs');
    const getEnvVar = async (varName) => {
      const cmd = `source ${initEnvsPath} && echo $${varName}`;
      const result = await execAsync(cmd, { shell: '/bin/bash' });
      return result.stdout.trim();
    };
    
    const eksClusterName = await getEnvVar('EKS_CLUSTER_NAME');
    const region = await getEnvVar('AWS_REGION');
    const eksStackName = await getEnvVar('CLOUD_FORMATION_FULL_STACK_NAME');
    
    // è·å–CloudFormationè¾“å‡ºä¿¡æ¯
    const stackInfo = await CloudFormationManager.fetchStackInfo(eksStackName, region);
    
    // è·å–å­ç½‘ä¿¡æ¯
    const subnetInfo = await CloudFormationManager.fetchSubnetInfo(stackInfo.VPC_ID, region);
    
    // åˆ›å»ºèŠ‚ç‚¹ç»„é…ç½®
    const createResult = await CloudFormationManager.createEksNodeGroup(
      userConfig, 
      region, 
      eksClusterName,
      stackInfo.VPC_ID,
      stackInfo.SECURITY_GROUP_ID,
      subnetInfo
    );

    // å¼‚æ­¥æ‰§è¡Œeksctlå‘½ä»¤
    const childProcess = spawn('eksctl', ['create', 'nodegroup', '-f', createResult.configFile], {
      stdio: 'pipe'
    });
    
    let output = '';
    let errorOutput = '';
    
    childProcess.stdout.on('data', (data) => {
      output += data.toString();
      console.log('eksctl stdout:', data.toString());
    });
    
    childProcess.stderr.on('data', (data) => {
      errorOutput += data.toString();
      console.log('eksctl stderr:', data.toString());
    });
    
    childProcess.on('close', async (code) => {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      require('fs').unlinkSync(createResult.configFile);
      
      if (code === 0) {
        broadcast({
          type: 'nodegroup_creation_completed',
          status: 'success',
          message: `EKS node group ${userConfig.nodeGroupName} created successfully`
        });
        
        // è‡ªåŠ¨é…ç½®èŠ‚ç‚¹ç»„ä¾èµ–
        try {
          console.log(`Starting dependency configuration for node group: ${userConfig.nodeGroupName}`);
          
          broadcast({
            type: 'nodegroup_dependencies_started',
            status: 'info',
            message: `Configuring dependencies for node group: ${userConfig.nodeGroupName}`
          });
          
          await EksNodeGroupDependencyManager.configureNodeGroupDependencies(
            activeCluster, 
            userConfig.nodeGroupName, 
            clusterManager
          );
          
          broadcast({
            type: 'nodegroup_dependencies_completed',
            status: 'success',
            message: `Node group dependencies configured successfully: ${userConfig.nodeGroupName}`
          });
          
        } catch (error) {
          console.error('Error configuring node group dependencies:', error);
          broadcast({
            type: 'nodegroup_dependencies_failed',
            status: 'error',
            message: `Node group dependencies failed: ${error.message}`
          });
        }
      } else {
        broadcast({
          type: 'nodegroup_creation_failed',
          status: 'error',
          message: `EKS node group creation failed: ${errorOutput}`
        });
      }
    });
    
    // ç«‹å³è¿”å›æˆåŠŸå“åº”
    broadcast({
      type: 'nodegroup_creation_started',
      status: 'info',
      message: `EKS node group creation started: ${userConfig.nodeGroupName}`
    });
    
    res.json({
      success: true,
      message: 'EKS node group creation started',
      nodeGroupName: userConfig.nodeGroupName
    });
    
  } catch (error) {
    console.error('Error creating EKS node group:', error);
    
    broadcast({
      type: 'nodegroup_creation_failed',
      status: 'error',
      message: `EKS node group creation failed: ${error.message}`
    });
    
    res.status(500).json({ success: false, error: error.message });
  }
});

// æ£€æŸ¥EKSèŠ‚ç‚¹ç»„ä¾èµ–çŠ¶æ€
app.get('/api/cluster/:clusterTag/nodegroup/:nodeGroupName/dependencies/status', async (req, res) => {
  try {
    const { clusterTag, nodeGroupName } = req.params;
    const EksNodeGroupDependencyManager = require('./utils/eksNodeGroupDependencyManager');
    
    const clusterDir = clusterManager.getClusterDir(clusterTag);
    const configDir = path.join(clusterDir, 'config');
    
    const status = await EksNodeGroupDependencyManager.checkDependencyStatus(configDir, nodeGroupName);
    res.json(status);
    
  } catch (error) {
    console.error('Error checking node group dependency status:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// æ£€æŸ¥HyperPodä¾èµ–é…ç½®çŠ¶æ€
app.get('/api/cluster/:clusterTag/hyperpod/dependencies/status', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    
    const clusterDir = clusterManager.getClusterDir(clusterTag);
    const configDir = path.join(clusterDir, 'config');
    
    if (!fs.existsSync(configDir)) {
      return res.status(404).json({ error: 'Cluster not found' });
    }
    
    const status = await HyperPodDependencyManager.checkHyperPodDependencyStatus(configDir);
    
    res.json({
      success: true,
      clusterTag,
      hyperPodDependencyStatus: status
    });
    
  } catch (error) {
    console.error('Error checking HyperPod dependency status:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–HyperPodåˆ›å»ºçŠ¶æ€
app.get('/api/cluster/hyperpod-creation-status/:clusterTag', async (req, res) => {
  try {
    const { execSync } = require('child_process');
    const { clusterTag } = req.params;
    const creatingClusters = getCreatingHyperPodClusters();
    const status = creatingClusters[clusterTag];
    
    if (!status) {
      return res.json({ success: true, status: null });
    }
    
    // æ£€æŸ¥CloudFormationçŠ¶æ€
    if (status.stackName && status.region) {
      try {
        const checkCmd = `aws cloudformation describe-stacks --stack-name ${status.stackName} --region ${status.region} --query 'Stacks[0].StackStatus' --output text`;
        const stackStatus = execSync(checkCmd, { encoding: 'utf8', timeout: 10000 }).trim();
        
        if (stackStatus === 'CREATE_COMPLETE') {
          // åˆ›å»ºå®Œæˆï¼Œæ¸…ç†çŠ¶æ€å¹¶æ³¨å†Œ
          updateCreatingHyperPodStatus(clusterTag, 'COMPLETED');
          await registerCompletedHyperPod(clusterTag);
          
          broadcast({
            type: 'hyperpod_creation_completed',
            status: 'success',
            message: `HyperPod cluster created successfully: ${status.stackName}`,
            clusterTag: clusterTag
          });
          
          return res.json({ success: true, status: { ...status, cfStatus: 'CREATE_COMPLETE' } });
        } else if (stackStatus.includes('FAILED') || stackStatus.includes('ROLLBACK') || stackStatus.includes('DELETE')) {
          // åˆ›å»ºå¤±è´¥ï¼Œè‡ªåŠ¨æ¸…ç†è®°å½•
          console.log(`Auto-cleaning failed HyperPod creation: ${clusterTag}, status: ${stackStatus}`);
          updateCreatingHyperPodStatus(clusterTag, 'COMPLETED');
          
          broadcast({
            type: 'hyperpod_creation_failed',
            status: 'error',
            message: `HyperPod creation failed and cleaned up: ${stackStatus}`,
            clusterTag: clusterTag
          });
          
          return res.json({ success: true, status: null }); // è¿”å›nullè¡¨ç¤ºå·²æ¸…ç†
        }
        
        status.cfStatus = stackStatus;
      } catch (error) {
        // Stackä¸å­˜åœ¨ï¼Œæ¸…ç†è®°å½•
        if (error.message.includes('does not exist')) {
          console.log(`Auto-cleaning non-existent HyperPod stack: ${clusterTag}`);
          updateCreatingHyperPodStatus(clusterTag, 'COMPLETED');
          return res.json({ success: true, status: null });
        }
        console.error(`Error checking CloudFormation status: ${error.message}`);
      }
    }
    
    res.json({ success: true, status });
  } catch (error) {
    console.error('Error getting HyperPod creation status:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// è·å–æ­£åœ¨åˆ›å»ºçš„HyperPodé›†ç¾¤åˆ—è¡¨
app.get('/api/cluster/creating-hyperpod-clusters', async (req, res) => {
  try {
    const { execSync } = require('child_process');
    const creatingClusters = getCreatingHyperPodClusters();
    
    // æ£€æŸ¥æ‰€æœ‰åˆ›å»ºä¸­é›†ç¾¤çš„çŠ¶æ€ï¼Œè‡ªåŠ¨æ¸…ç†å¤±è´¥/ä¸å­˜åœ¨çš„è®°å½•
    for (const [clusterTag, clusterInfo] of Object.entries(creatingClusters)) {
      if (clusterInfo.stackName && clusterInfo.region) {
        try {
          // æ£€æŸ¥CloudFormation stackçŠ¶æ€
          const checkCmd = `aws cloudformation describe-stacks --stack-name ${clusterInfo.stackName} --region ${clusterInfo.region} --query 'Stacks[0].StackStatus' --output text`;
          const stackStatus = execSync(checkCmd, { encoding: 'utf8', timeout: 10000 }).trim();
          
          if (stackStatus === 'CREATE_COMPLETE') {
            updateCreatingHyperPodStatus(clusterTag, 'COMPLETED');
            await registerCompletedHyperPod(clusterTag);
          } else if (stackStatus.includes('FAILED') || stackStatus.includes('ROLLBACK') || stackStatus.includes('DELETE')) {
            // è‡ªåŠ¨æ¸…ç†å¤±è´¥çš„è®°å½•
            console.log(`Auto-cleaning failed HyperPod creation: ${clusterTag}, status: ${stackStatus}`);
            updateCreatingHyperPodStatus(clusterTag, 'COMPLETED'); // ç§»é™¤è®°å½•
          }
        } catch (error) {
          // Stackä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œæ¸…ç†è®°å½•
          if (error.message.includes('does not exist')) {
            console.log(`Auto-cleaning non-existent HyperPod stack: ${clusterTag}`);
            updateCreatingHyperPodStatus(clusterTag, 'COMPLETED'); // ç§»é™¤è®°å½•
          } else {
            console.error(`Error checking status for ${clusterTag}:`, error.message);
          }
        }
      }
    }
    
    // è¿”å›æ¸…ç†åçš„çŠ¶æ€
    const updatedClusters = getCreatingHyperPodClusters();
    res.json({ success: true, data: updatedClusters });
  } catch (error) {
    console.error('Error getting creating HyperPod clusters:', error);
    res.status(500).json({ success: false, error: error.message });
  }
});

// å–æ¶ˆé›†ç¾¤åˆ›å»º
app.post('/api/cluster/cancel-creation/:clusterTag', async (req, res) => {
  try {
    const { clusterTag } = req.params;
    
    // è¯»å–é›†ç¾¤é…ç½®
    const clusterInfo = await clusterManager.getClusterInfo(clusterTag);
    if (!clusterInfo) {
      return res.status(404).json({ error: 'Cluster not found' });
    }
    
    const stackName = `full-stack-${clusterTag}`;
    const result = await CloudFormationManager.cancelStackCreation(stackName, clusterInfo.awsRegion);
    
    // å‘é€WebSocketé€šçŸ¥
    broadcast({
      type: 'cluster_creation_cancelled',
      status: 'success',
      message: `Cluster creation cancelled: ${clusterTag}`,
      clusterTag
    });
    
    res.json({
      success: true,
      clusterTag,
      ...result
    });
  } catch (error) {
    console.error('Error cancelling cluster creation:', error);
    
    broadcast({
      type: 'cluster_creation_cancelled',
      status: 'error',
      message: `Failed to cancel cluster creation: ${error.message}`
    });
    
    res.status(500).json({ error: error.message });
  }
});

console.log('EKS cluster creation APIs loaded');

app.listen(PORT, () => {
  console.log('ğŸš€ ========================================');
  console.log('ğŸš€ HyperPod InstantStart Server Started');
  console.log('ğŸš€ ========================================');
  console.log(`ğŸ“¡ HTTP Server: http://localhost:${PORT}`);
  console.log(`ğŸ”Œ WebSocket Server: ws://localhost:${WS_PORT}`);
  console.log(`ğŸŒ Multi-cluster management: enabled`);
  console.log(`â° Server started at: ${new Date().toISOString()}`);
  console.log(`ğŸ–¥ï¸  Node.js version: ${process.version}`);
  console.log(`ğŸ’¾ Memory usage: ${Math.round(process.memoryUsage().heapUsed / 1024 / 1024)}MB`);
  console.log(`ğŸ”§ Environment: ${process.env.NODE_ENV || 'development'}`);
  console.log('ğŸš€ ========================================');
  console.log('âœ… Server is ready to accept connections');
});
