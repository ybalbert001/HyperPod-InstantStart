const express = require('express');
const cors = require('cors');
const WebSocket = require('ws');
const { exec, spawn } = require('child_process');
const fs = require('fs-extra');
const YAML = require('yaml');
const path = require('path');
const https = require('https');
const http = require('http');

// å¼•å…¥é›†ç¾¤çŠ¶æ€V2æ¨¡å—
const { 
  handleClusterStatusV2, 
  handleClearCache, 
  handleCacheStatus 
} = require('./clusterStatusV2');

// å¼•å…¥åº”ç”¨çŠ¶æ€V2æ¨¡å—
const {
  handlePodsV2,
  handleServicesV2,
  handleAppStatusV2,
  handleClearAppCache,
  handleAppCacheStatus
} = require('./appStatusV2');

const app = express();
const PORT = 3001;
const WS_PORT = 8081; // æ”¹ä¸º8081é¿å…ç«¯å£å†²çª

app.use(cors());
app.use(express.json());

// WebSocketæœåŠ¡å™¨ç”¨äºå®æ—¶æ›´æ–°
const wss = new WebSocket.Server({ port: WS_PORT });

// å­˜å‚¨æ´»è·ƒçš„æ—¥å¿—æµ
const activeLogStreams = new Map();

// æ—¥å¿—å­˜å‚¨é…ç½® - ç®€åŒ–è·¯å¾„ç»“æ„
const LOGS_BASE_DIR = path.join(__dirname, '..', 'logs');

// ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨ä»»åŠ¡å
function ensureLogDirectory(jobName, podName) {
  const jobLogDir = path.join(LOGS_BASE_DIR, jobName);
  if (!fs.existsSync(jobLogDir)) {
    fs.mkdirSync(jobLogDir, { recursive: true });
  }
  return path.join(jobLogDir, `${podName}.log`);
}

// å¹¿æ’­æ¶ˆæ¯ç»™æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
function broadcast(data) {
  wss.clients.forEach(client => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(JSON.stringify(data));
    }
  });
}

// æ‰§è¡Œkubectlå‘½ä»¤çš„è¾…åŠ©å‡½æ•° - ç®€åŒ–ç‰ˆé”™è¯¯ä¼˜åŒ–
function executeKubectl(command, timeout = 30000) { // é»˜è®¤30ç§’è¶…æ—¶
  return new Promise((resolve, reject) => {
    console.log(`Executing kubectl command: kubectl ${command}`);
    
    const child = exec(`kubectl ${command}`, { timeout }, (error, stdout, stderr) => {
      if (error) {
        console.error(`kubectl command failed: kubectl ${command}`);
        console.error(`Error details:`, error);
        console.error(`Stderr:`, stderr);
        
        if (error.code === 'ETIMEDOUT') {
          console.error(`kubectl command timed out after ${timeout}ms: ${command}`);
          reject(new Error(`Command timed out after ${timeout/1000} seconds. The cluster may be slow to respond.`));
        } else {
          const errorMessage = error.message || stderr || 'Unknown kubectl error';
          
          // é’ˆå¯¹ç‰¹å®šæƒ…å†µä¼˜åŒ–é”™è¯¯æ¶ˆæ¯
          let optimizedMessage = errorMessage;
          
          // å¦‚æœæ˜¯è·å–hyperpodpytorchjobä½†èµ„æºç±»å‹ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
          if (command.includes('get hyperpodpytorchjob') && 
              errorMessage.includes(`doesn't have a resource type "hyperpodpytorchjob"`)) {
            optimizedMessage = 'No HyperPod training jobs found (HyperPod operator may not be installed)';
          }
          // å¦‚æœæ˜¯èµ„æºä¸å­˜åœ¨ï¼Œä½¿ç”¨æ›´å‹å¥½çš„æ¶ˆæ¯
          else if (errorMessage.includes('not found') || errorMessage.includes('NotFound')) {
            optimizedMessage = 'Resource not found - this may be normal if no resources have been created yet';
          }
          // å¦‚æœæ˜¯è¿æ¥é—®é¢˜
          else if (errorMessage.includes('connection refused') || errorMessage.includes('unable to connect')) {
            optimizedMessage = 'Unable to connect to Kubernetes cluster. Please check if the cluster is accessible.';
          }
          
          console.error(`Optimized error message: ${optimizedMessage}`);
          reject(new Error(optimizedMessage));
        }
      } else {
        console.log(`kubectl command succeeded: kubectl ${command}`);
        console.log(`Output:`, stdout);
        resolve(stdout);
      }
    });
    
    // é¢å¤–çš„è¶…æ—¶ä¿æŠ¤
    const timeoutId = setTimeout(() => {
      child.kill('SIGTERM');
      console.error(`Force killing kubectl command after ${timeout}ms: ${command}`);
    }, timeout);
    
    child.on('exit', () => {
      clearTimeout(timeoutId);
    });
  });
}

// ç®€åŒ–çš„æ¨¡å‹æ ‡ç­¾ç”Ÿæˆå‡½æ•°ï¼ˆç”¨äºæ¨¡å‹ä¸‹è½½ï¼‰
function generateModelTag(modelId) {
  if (!modelId) return 'model';
  return modelId
    .toLowerCase()
    .replace(/[^a-z0-9-]/g, '-')
    .replace(/-+/g, '-')
    .replace(/^-|-$/g, '') || 'model';
}

// ç”ŸæˆNLBæ³¨è§£çš„å‡½æ•°
function generateNLBAnnotations(isExternal) {
  if (isExternal) {
    return `
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"`;
  } else {
    return `
    service.beta.kubernetes.io/aws-load-balancer-type: "external"
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"`;
  }
}

// è§£æå®Œæ•´çš„VLLM/SGLangå‘½ä»¤
function parseVllmCommand(vllmCommandString) {
  // ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œå¤„ç†åæ–œæ æ¢è¡Œ
  const cleanCommand = vllmCommandString
    .replace(/\\\s*\n/g, ' ')  // å¤„ç†åæ–œæ æ¢è¡Œ
    .replace(/\s+/g, ' ')      // åˆå¹¶å¤šä¸ªç©ºæ ¼
    .trim();
  
  // åˆ†å‰²å‘½ä»¤ä¸ºæ•°ç»„
  const parts = cleanCommand.split(' ').filter(part => part.trim());
  
  // æ£€æŸ¥å‘½ä»¤æ˜¯å¦ä¸ºç©º
  if (parts.length === 0) {
    throw new Error('Command cannot be empty');
  }
  
  // æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥çš„å‘½ä»¤æ ¼å¼ï¼ˆç”¨äºä¼˜åŒ–å¤„ç†ï¼Œä½†ä¸å¼ºåˆ¶è¦æ±‚ï¼‰
  const isVllmCommand = parts.includes('python3') && parts.includes('-m') && parts.includes('vllm.entrypoints.openai.api_server');
  const isVllmServeCommand = parts.includes('vllm') && parts.includes('serve');
  const isSglangCommand = parts.includes('python3') && parts.includes('-m') && parts.includes('sglang.launch_server');
  
  let entrypointIndex = -1, tensorParallelSize = 1;
  
  if (isVllmCommand) {
    // ä¼ ç»ŸVLLMå‘½ä»¤å¤„ç†: python3 -m vllm.entrypoints.openai.api_server
    entrypointIndex = parts.findIndex(part => part === 'vllm.entrypoints.openai.api_server');
    const args = parts.slice(entrypointIndex + 1);
    
    // è§£ætensor-parallel-sizeç”¨äºGPUé…ç½®
    const tensorParallelIndex = args.findIndex(arg => arg === '--tensor-parallel-size');
    if (tensorParallelIndex !== -1 && tensorParallelIndex + 1 < args.length) {
      tensorParallelSize = parseInt(args[tensorParallelIndex + 1]) || 1;
    }
  } else if (isVllmServeCommand) {
    // æ–°VLLM serveå‘½ä»¤å¤„ç†: vllm serve /path/to/model
    entrypointIndex = parts.findIndex(part => part === 'serve');
    const args = parts.slice(entrypointIndex + 1);
    
    // è§£ætensor-parallel-sizeç”¨äºGPUé…ç½®
    const tensorParallelIndex = args.findIndex(arg => arg === '--tensor-parallel-size');
    if (tensorParallelIndex !== -1 && tensorParallelIndex + 1 < args.length) {
      tensorParallelSize = parseInt(args[tensorParallelIndex + 1]) || 1;
    }
  } else if (isSglangCommand) {
    // SGLangå‘½ä»¤å¤„ç†
    entrypointIndex = parts.findIndex(part => part === 'sglang.launch_server');
    const args = parts.slice(entrypointIndex + 1);
    
    // è§£ætp-sizeç”¨äºGPUé…ç½® (SGLangä½¿ç”¨--tp-sizeè€Œä¸æ˜¯--tensor-parallel-size)
    const tpSizeIndex = args.findIndex(arg => arg === '--tp-size');
    if (tpSizeIndex !== -1 && tpSizeIndex + 1 < args.length) {
      tensorParallelSize = parseInt(args[tpSizeIndex + 1]) || 1;
    }
  } else {
    // å¯¹äºå…¶ä»–å‘½ä»¤æ ¼å¼ï¼Œå°è¯•é€šç”¨çš„GPUå‚æ•°è§£æ
    // æŸ¥æ‰¾å¸¸è§çš„GPUç›¸å…³å‚æ•°
    const gpuParams = ['--tensor-parallel-size', '--tp-size', '--gpus', '--gpu-count'];
    for (const param of gpuParams) {
      const paramIndex = parts.findIndex(arg => arg === param);
      if (paramIndex !== -1 && paramIndex + 1 < parts.length) {
        const value = parseInt(parts[paramIndex + 1]);
        if (!isNaN(value) && value > 0) {
          tensorParallelSize = value;
          break;
        }
      }
    }
  }
  
  const args = entrypointIndex >= 0 ? parts.slice(entrypointIndex + 1) : parts.slice(1);
  
  return {
    fullCommand: parts,
    args: args,
    tensorParallelSize: tensorParallelSize,
    commandType: (isVllmCommand || isVllmServeCommand) ? 'vllm' : (isSglangCommand ? 'sglang' : 'custom')
  };
}

// æ”¹è¿›çš„HTTPè¯·æ±‚ä»£ç†å‡½æ•°
function makeHttpRequest(url, payload, method = 'POST') {
  return new Promise((resolve, reject) => {
    try {
      const urlObj = new URL(url);
      const isHttps = urlObj.protocol === 'https:';
      const httpModule = isHttps ? https : http;
      
      const isGetRequest = method.toUpperCase() === 'GET';
      const postData = isGetRequest ? '' : JSON.stringify(payload);
      
      const options = {
        hostname: urlObj.hostname,
        port: urlObj.port || (isHttps ? 443 : 80),
        path: urlObj.pathname + urlObj.search,
        method: method.toUpperCase(),
        headers: {
          'User-Agent': 'Model-Deployment-UI/1.0'
        },
        timeout: 30000 // 30ç§’è¶…æ—¶
      };
      
      // åªæœ‰POSTè¯·æ±‚æ‰éœ€è¦Content-Typeå’ŒContent-Length
      if (!isGetRequest) {
        options.headers['Content-Type'] = 'application/json';
        options.headers['Content-Length'] = Buffer.byteLength(postData);
      }
      
      console.log(`Making HTTP ${method} request to: ${url}`);
      console.log(`Request options:`, JSON.stringify(options, null, 2));
      if (!isGetRequest) {
        console.log(`Payload:`, postData);
      }
      
      const req = httpModule.request(options, (res) => {
        let data = '';
        
        console.log(`Response status: ${res.statusCode}`);
        console.log(`Response headers:`, JSON.stringify(res.headers, null, 2));
        
        res.on('data', (chunk) => {
          data += chunk;
        });
        
        res.on('end', () => {
          console.log(`Response data:`, data);
          
          // å¤„ç†ä¸åŒçš„å“åº”çŠ¶æ€
          if (res.statusCode >= 200 && res.statusCode < 300) {
            // æˆåŠŸå“åº”
            try {
              const jsonData = JSON.parse(data);
              resolve({
                success: true,
                status: res.statusCode,
                data: jsonData
              });
            } catch (parseError) {
              // å¦‚æœä¸æ˜¯JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
              console.log('Response is not JSON, returning as text');
              resolve({
                success: true,
                status: res.statusCode,
                data: data,
                isText: true
              });
            }
          } else {
            // é”™è¯¯å“åº”
            try {
              const errorData = JSON.parse(data);
              resolve({
                success: false,
                status: res.statusCode,
                error: errorData.error || `HTTP ${res.statusCode}`,
                data: errorData
              });
            } catch (parseError) {
              resolve({
                success: false,
                status: res.statusCode,
                error: `HTTP ${res.statusCode}: ${data}`,
                data: data
              });
            }
          }
        });
      });
      
      req.on('error', (error) => {
        console.error('HTTP request error:', error);
        reject({
          success: false,
          error: `Network error: ${error.message}`
        });
      });
      
      req.on('timeout', () => {
        console.error('HTTP request timeout');
        req.destroy();
        reject({
          success: false,
          error: 'Request timeout (30s)'
        });
      });
      
      // åªæœ‰éGETè¯·æ±‚æ‰å†™å…¥payload
      if (!isGetRequest && postData) {
        req.write(postData);
      }
      req.end();
      
    } catch (error) {
      console.error('HTTP request setup error:', error);
      reject({
        success: false,
        error: `Request setup error: ${error.message}`
      });
    }
  });
}

// è·å–Pending GPUç»Ÿè®¡
app.get('/api/pending-gpus', async (req, res) => {
  try {
    const pendingPodsOutput = await executeKubectl('get pods --field-selector status.phase=Pending -o json');
    const pendingPodsData = JSON.parse(pendingPodsOutput);
    
    let pendingGPUs = 0;
    if (pendingPodsData.items && Array.isArray(pendingPodsData.items)) {
      pendingGPUs = pendingPodsData.items.reduce((sum, pod) => {
        if (pod.spec?.containers) {
          return sum + pod.spec.containers.reduce((containerSum, container) => {
            const gpuRequest = container.resources?.requests?.['nvidia.com/gpu'];
            return containerSum + (parseInt(gpuRequest) || 0);
          }, 0);
        }
        return sum;
      }, 0);
    }
    
    res.json({ pendingGPUs });
  } catch (error) {
    console.error('Error fetching pending GPUs:', error);
    res.status(500).json({ error: error.message, pendingGPUs: 0 });
  }
});

// è·å–é›†ç¾¤èŠ‚ç‚¹GPUä½¿ç”¨æƒ…å†µ - V2ä¼˜åŒ–ç‰ˆæœ¬
app.get('/api/cluster-status', handleClusterStatusV2);

// é›†ç¾¤çŠ¶æ€ç¼“å­˜ç®¡ç†API
app.post('/api/cluster-status/clear-cache', handleClearCache);
app.get('/api/cluster-status/cache-status', handleCacheStatus);

// ç»Ÿä¸€æ—¥å¿—æµç®¡ç† - é¿å…å†²çª
const unifiedLogStreams = new Map(); // ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ—¥å¿—æµ

// å¯åŠ¨ç»Ÿä¸€æ—¥å¿—æµï¼ˆæ”¯æŒè‡ªåŠ¨æ”¶é›†å’ŒWebSocketæµå¼ä¼ è¾“ï¼‰
function startUnifiedLogStream(jobName, podName, options = {}) {
  const streamKey = `${jobName}-${podName}`;
  const { ws = null, autoCollection = false } = options;
  
  // å¦‚æœå·²ç»æœ‰è¯¥podçš„æ—¥å¿—æµï¼Œæ·»åŠ WebSocketè¿æ¥ä½†ä¸é‡å¯è¿›ç¨‹
  if (unifiedLogStreams.has(streamKey)) {
    const existing = unifiedLogStreams.get(streamKey);
    if (ws && !existing.webSockets.has(ws)) {
      existing.webSockets.add(ws);
      console.log(`Added WebSocket to existing log stream for ${streamKey}`);
      
      // å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: 'log_stream_started',
          jobName: jobName,
          podName: podName,
          timestamp: new Date().toISOString()
        }));
      }
    }
    return;
  }
  
  console.log(`ğŸš€ Starting unified log stream for pod: ${podName} in job: ${jobName} (auto: ${autoCollection})`);
  
  // åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
  const logFilePath = ensureLogDirectory(jobName, podName);
  const logStream = fs.createWriteStream(logFilePath, { flags: 'a' });
  
  // å¯åŠ¨kubectl logså‘½ä»¤
  const logProcess = spawn('kubectl', ['logs', '-f', podName], {
    stdio: ['pipe', 'pipe', 'pipe']
  });
  
  // åˆ›å»ºWebSocketé›†åˆ
  const webSockets = new Set();
  if (ws) {
    webSockets.add(ws);
  }
  
  // å­˜å‚¨ç»Ÿä¸€çš„æ—¥å¿—æµä¿¡æ¯
  unifiedLogStreams.set(streamKey, {
    process: logProcess,
    logStream: logStream,
    webSockets: webSockets,
    jobName: jobName,
    podName: podName,
    autoCollection: autoCollection,
    startTime: new Date().toISOString()
  });
  
  // å¤„ç†æ ‡å‡†è¾“å‡º
  logProcess.stdout.on('data', (data) => {
    const logLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    logStream.write(`[${timestamp}] ${logLine}`);
    
    // å‘é€åˆ°æ‰€æœ‰è¿æ¥çš„WebSocket
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_data',
          jobName: jobName,
          podName: podName,
          data: logLine,
          timestamp: timestamp
        }));
      }
    });
  });
  
  // å¤„ç†é”™è¯¯è¾“å‡º
  logProcess.stderr.on('data', (data) => {
    const errorLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶
    logStream.write(`[${timestamp}] ERROR: ${errorLine}`);
    
    // å‘é€é”™è¯¯åˆ°WebSocket
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_error',
          jobName: jobName,
          podName: podName,
          error: errorLine,
          timestamp: timestamp
        }));
      }
    });
  });
  
  // å¤„ç†è¿›ç¨‹é€€å‡º
  logProcess.on('close', (code) => {
    console.log(`Unified log stream for ${podName} exited with code ${code}`);
    logStream.end();
    
    // é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_stream_closed',
          jobName: jobName,
          podName: podName,
          timestamp: new Date().toISOString()
        }));
      }
    });
    
    unifiedLogStreams.delete(streamKey);
  });
  
  // å¤„ç†è¿›ç¨‹é”™è¯¯
  logProcess.on('error', (error) => {
    console.error(`Unified log stream error for ${podName}:`, error);
    logStream.end();
    
    // é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
    webSockets.forEach(socket => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({
          type: 'log_stream_error',
          jobName: jobName,
          podName: podName,
          error: error.message,
          timestamp: new Date().toISOString()
        }));
      }
    });
    
    unifiedLogStreams.delete(streamKey);
  });
  
  // å‘é€å¯åŠ¨æˆåŠŸæ¶ˆæ¯
  if (ws && ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify({
      type: 'log_stream_started',
      jobName: jobName,
      podName: podName,
      timestamp: new Date().toISOString()
    }));
  }
}

// ä»ç»Ÿä¸€æ—¥å¿—æµä¸­ç§»é™¤WebSocketè¿æ¥
function removeWebSocketFromLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  const stream = unifiedLogStreams.get(streamKey);
  
  if (stream) {
    stream.webSockets.delete(ws);
    console.log(`Removed WebSocket from log stream ${streamKey}, remaining: ${stream.webSockets.size}`);
    
    // å¦‚æœæ²¡æœ‰WebSocketè¿æ¥ä¸”ä¸æ˜¯è‡ªåŠ¨æ”¶é›†ï¼Œåœæ­¢æ—¥å¿—æµ
    if (stream.webSockets.size === 0 && !stream.autoCollection) {
      console.log(`No more WebSocket connections for ${streamKey}, stopping log stream`);
      stream.process.kill();
      stream.logStream.end();
      unifiedLogStreams.delete(streamKey);
    }
  }
}

// ä¸ºè®­ç»ƒä»»åŠ¡è‡ªåŠ¨å¼€å§‹æ—¥å¿—æ”¶é›†
async function startAutoLogCollectionForJob(jobName) {
  try {
    console.log(`ğŸ” Starting auto log collection for training job: ${jobName}`);
    
    // è·å–è¯¥è®­ç»ƒä»»åŠ¡çš„æ‰€æœ‰pods
    const output = await executeKubectl('get pods -o json');
    const result = JSON.parse(output);
    
    const jobPods = result.items.filter(pod => {
      const labels = pod.metadata.labels || {};
      const ownerReferences = pod.metadata.ownerReferences || [];
      
      return labels['training-job-name'] === jobName || 
             labels['app'] === jobName ||
             ownerReferences.some(ref => ref.name === jobName) ||
             pod.metadata.name.includes(jobName);
    });
    
    // ä¸ºæ¯ä¸ªè¿è¡Œä¸­çš„podå¼€å§‹è‡ªåŠ¨æ—¥å¿—æ”¶é›†
    jobPods.forEach(pod => {
      if (pod.status.phase === 'Running' || pod.status.phase === 'Pending') {
        startUnifiedLogStream(jobName, pod.metadata.name, { autoCollection: true });
      }
    });
    
    console.log(`âœ… Started auto log collection for ${jobPods.length} pods in job ${jobName}`);
  } catch (error) {
    console.error(`âŒ Failed to start auto log collection for job ${jobName}:`, error);
  }
}

// ä¿®æ”¹åŸæœ‰çš„startLogStreamå‡½æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€ç®¡ç†
function startLogStream(ws, jobName, podName) {
  startUnifiedLogStream(jobName, podName, { ws: ws });
}

// ä¿®æ”¹åŸæœ‰çš„stopLogStreamå‡½æ•°
function stopLogStream(ws, jobName, podName) {
  removeWebSocketFromLogStream(ws, jobName, podName);
  
  if (ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify({
      type: 'log_stream_stopped',
      jobName: jobName,
      podName: podName,
      timestamp: new Date().toISOString()
    }));
  }
}

// åº”ç”¨çŠ¶æ€V2 API - ä¼˜åŒ–ç‰ˆæœ¬
app.get('/api/v2/pods', handlePodsV2);
app.get('/api/v2/services', handleServicesV2);
app.get('/api/v2/app-status', handleAppStatusV2);
app.post('/api/v2/app-status/clear-cache', handleClearAppCache);
app.get('/api/v2/app-status/cache-status', handleAppCacheStatus);

// è·å–PodçŠ¶æ€
app.get('/api/pods', async (req, res) => {
  try {
    console.log('Fetching pods...');
    const output = await executeKubectl('get pods -o json');
    const pods = JSON.parse(output);
    console.log('Pods fetched:', pods.items.length, 'pods');
    res.json(pods.items);
  } catch (error) {
    console.error('Pods fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–ServiceçŠ¶æ€
app.get('/api/services', async (req, res) => {
  try {
    console.log('Fetching services...');
    const output = await executeKubectl('get services -o json');
    const services = JSON.parse(output);
    console.log('Services fetched:', services.items.length, 'services');
    res.json(services.items);
  } catch (error) {
    console.error('Services fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// ä»£ç†HTTPè¯·æ±‚åˆ°æ¨¡å‹æœåŠ¡
app.post('/api/proxy-request', async (req, res) => {
  try {
    const { url, payload, method = 'POST' } = req.body;
    
    if (!url) {
      return res.status(400).json({
        success: false,
        error: 'Missing url'
      });
    }
    
    // GETè¯·æ±‚ä¸éœ€è¦payload
    if (method.toUpperCase() !== 'GET' && payload === undefined) {
      return res.status(400).json({
        success: false,
        error: 'Missing payload for non-GET request'
      });
    }
    
    console.log(`Proxying ${method} request to: ${url}`);
    if (method.toUpperCase() !== 'GET') {
      console.log(`Payload:`, JSON.stringify(payload, null, 2));
    }
    
    const result = await makeHttpRequest(url, payload, method);
    
    console.log('Proxy result:', JSON.stringify(result, null, 2));
    res.json(result);
    
  } catch (error) {
    console.error('Proxy request error:', error);
    res.json({
      success: false,
      error: error.error || error.message || 'Request failed'
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²YAMLé…ç½® - ä»…ç”¨äºæ¨ç†éƒ¨ç½²ï¼ˆVLLMå’ŒOllamaï¼‰
app.post('/api/deploy', async (req, res) => {
  try {
    const {
      replicas,
      huggingFaceToken,
      deploymentType,
      vllmCommand,
      ollamaModelId,
      gpuCount,
      isExternal = true,
      deploymentName,  // ç”¨æˆ·è¾“å…¥çš„éƒ¨ç½²åç§°
      dockerImage = 'vllm/vllm-openai:latest'
    } = req.body;

    console.log('Inference deployment request:', { 
      deploymentType, 
      deploymentName,
      ollamaModelId, 
      replicas, 
      isExternal,
      dockerImage
    });

    // ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€æ ‡ç­¾ï¼ˆç¬¦åˆKuberneteså‘½åè§„èŒƒï¼‰
    const timestamp = new Date().toISOString()
      .replace(/[:.]/g, '-')     // æ›¿æ¢å†’å·å’Œç‚¹å·ä¸ºè¿å­—ç¬¦
      .replace('T', '-')         // æ›¿æ¢Tä¸ºè¿å­—ç¬¦
      .slice(0, 19);             // æˆªå–åˆ°ç§’çº§
    const finalDeploymentTag = deploymentName ? `${deploymentName}-${timestamp}` : `model-${timestamp}`;
    
    console.log(`Generated deployment tag: "${finalDeploymentTag}"`);

    let templatePath, newYamlContent;

    // ç”ŸæˆNLBæ³¨è§£
    const nlbAnnotations = generateNLBAnnotations(isExternal);
    console.log(`Generated NLB annotations (external: ${isExternal}):`, nlbAnnotations);

    if (deploymentType === 'ollama') {
      // å¤„ç†Ollamaéƒ¨ç½²
      templatePath = path.join(__dirname, '../templates/ollama-template.yaml');
      const templateContent = await fs.readFile(templatePath, 'utf8');
      
      // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
      newYamlContent = templateContent
        .replace(/MODEL_TAG/g, finalDeploymentTag)
        .replace(/OLLAMA_MODEL_ID/g, ollamaModelId)
        .replace(/REPLICAS_COUNT/g, replicas.toString())
        .replace(/GPU_COUNT/g, gpuCount.toString())
        .replace(/NLB_ANNOTATIONS/g, nlbAnnotations);
      
    } else {
      // å¤„ç†VLLM/SGLang/Customéƒ¨ç½²
      const parsedCommand = parseVllmCommand(vllmCommand);
      console.log('Parsed command:', parsedCommand);
      
      // æ ¹æ®å‘½ä»¤ç±»å‹ç¡®å®šæœåŠ¡å¼•æ“å‰ç¼€
      let servEngine;
      if (parsedCommand.commandType === 'sglang') {
        servEngine = 'sglang';
      } else if (parsedCommand.commandType === 'vllm') {
        servEngine = 'vllm';
      } else {
        servEngine = 'custom';  // è‡ªå®šä¹‰å‘½ä»¤ä½¿ç”¨customå‰ç¼€
      }
      console.log(`Using service engine: ${servEngine} for command type: ${parsedCommand.commandType}`);

      templatePath = path.join(__dirname, '../templates/vllm-sglang-template.yaml');
      const templateContent = await fs.readFile(templatePath, 'utf8');
      
      // ç”ŸæˆHuggingFace tokenç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæä¾›äº†tokenï¼‰
      let hfTokenEnv = '';
      if (huggingFaceToken && huggingFaceToken.trim() !== '') {
        hfTokenEnv = `
            - name: HUGGING_FACE_HUB_TOKEN
              value: "${huggingFaceToken}"`;
      }
      
      // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
      newYamlContent = templateContent
        .replace(/SERVENGINE/g, servEngine)
        .replace(/MODEL_TAG/g, finalDeploymentTag)
        .replace(/REPLICAS_COUNT/g, replicas.toString())
        .replace(/GPU_COUNT/g, parsedCommand.tensorParallelSize.toString())
        .replace(/HF_TOKEN_ENV/g, hfTokenEnv)
        .replace(/VLLM_COMMAND/g, JSON.stringify(parsedCommand.fullCommand))
        .replace(/DOCKER_IMAGE/g, dockerImage)
        .replace(/NLB_ANNOTATIONS/g, nlbAnnotations);
    }
    
    // ä¿å­˜åˆ°é¡¹ç›®ç›®å½•ä¸­çš„deploymentsæ–‡ä»¶å¤¹
    const deploymentsDir = path.join(__dirname, '../deployments');
    await fs.ensureDir(deploymentsDir);
    
    const accessType = isExternal ? 'external' : 'internal';
    const tempYamlPath = path.join(deploymentsDir, `${finalDeploymentTag}-${deploymentType}-${accessType}.yaml`);
    await fs.writeFile(tempYamlPath, newYamlContent);
    
    console.log(`Generated YAML saved to: ${tempYamlPath}`);
    
    // æ‰§è¡Œkubectl apply
    const applyOutput = await executeKubectl(`apply -f ${tempYamlPath}`);
    
    // å¹¿æ’­éƒ¨ç½²çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'deployment',
      status: 'success',
      message: `Successfully deployed: ${finalDeploymentTag} (${accessType} access)`,
      output: applyOutput
    });
    
    res.json({
      success: true,
      message: 'Deployment successful',
      output: applyOutput,
      yamlPath: tempYamlPath,
      generatedYaml: newYamlContent,
      deploymentType,
      deploymentTag: finalDeploymentTag,
      accessType
    });
    
  } catch (error) {
    console.error('Deployment error:', error);
    
    broadcast({
      type: 'deployment',
      status: 'error',
      message: `Deployment failed: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç»Ÿä¸€çš„è®­ç»ƒYAMLéƒ¨ç½²å‡½æ•°
async function deployTrainingYaml(recipeType, jobName, yamlContent) {
  try {
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿deployments/trainingsç›®å½•å­˜åœ¨
    const trainingsDir = path.join(__dirname, '../deployments/trainings');
    if (!fs.existsSync(trainingsDir)) {
      fs.mkdirSync(trainingsDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `${recipeType}-${jobName}-${Date.now()}.yaml`;
    const tempFilePath = path.join(tempDir, tempFileName);
    await fs.writeFile(tempFilePath, yamlContent);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆç”¨äºè®°å½•ï¼‰
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const permanentFileName = `${recipeType}_${timestamp}.yaml`;
    const permanentFilePath = path.join(trainingsDir, permanentFileName);
    await fs.writeFile(permanentFilePath, yamlContent);

    console.log(`${recipeType} training YAML saved to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½®
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`);
    console.log(`${recipeType} training kubectl apply output:`, applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    fs.unlinkSync(tempFilePath);

    // å‘é€WebSocketå¹¿æ’­
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched ${recipeType} training job: ${jobName}`,
      output: applyOutput
    });

    return {
      success: true,
      permanentFileName,
      permanentFilePath,
      applyOutput
    };

  } catch (error) {
    // å‘é€é”™è¯¯å¹¿æ’­
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `${recipeType} training launch failed: ${error.message}`
    });

    throw error;
  }
}

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPod Torchè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºTorchè®­ç»ƒ
app.post('/api/launch-torch-training', async (req, res) => {
  try {
    console.log('Raw torch training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 16,
      entryPythonScriptPath,
      pythonScriptParameters,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Torch training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      entryPythonScriptPath,
      pythonScriptParameters,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!entryPythonScriptPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry Python Script Path is required'
      });
    }

    if (!pythonScriptParameters) {
      return res.status(400).json({
        success: false,
        error: 'Python Script Parameters are required'
      });
    }

    // è¯»å–Torchè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-torch-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // å¤„ç†Pythonè„šæœ¬å‚æ•° - ç¡®ä¿å¤šè¡Œå‚æ•°åœ¨YAMLä¸­æ­£ç¡®æ ¼å¼åŒ–
    let formattedPythonParams = pythonScriptParameters;
    if (pythonScriptParameters.includes('\\')) {
      // å¦‚æœåŒ…å«åæ–œæ æ¢è¡Œç¬¦ï¼Œå°†å…¶è½¬æ¢ä¸ºå•è¡Œæ ¼å¼
      formattedPythonParams = pythonScriptParameters
        .replace(/\\\s*\n\s*/g, ' ')  // å°†åæ–œæ æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
        .replace(/\s+/g, ' ')         // åˆå¹¶å¤šä¸ªç©ºæ ¼
        .trim();
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/TORCH_RECIPE_PYPATH_PH/g, entryPythonScriptPath)
      .replace(/TORCH_RECIPE_PYPARAMS_PH/g, formattedPythonParams)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated torch training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `torch-training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `torch_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Torch training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Torch training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Torch training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched torch training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Torch training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Torch training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Torch training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPodè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºLlamaFactoryè®­ç»ƒä»»åŠ¡
app.post('/api/launch-training', async (req, res) => {
  try {
    console.log('Raw request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = 'pytorch/pytorch:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 0,
      lmfRecipeRunPath,
      lmfRecipeYamlFile,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      lmfRecipeRunPath,
      lmfRecipeYamlFile,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!lmfRecipeRunPath) {
      return res.status(400).json({
        success: false,
        error: 'LlamaFactory Recipe Run Path is required'
      });
    }

    if (!lmfRecipeYamlFile) {
      return res.status(400).json({
        success: false,
        error: 'LlamaFactory Config YAML File Name is required'
      });
    }

    // è¯»å–è®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-lmf-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/LMF_RECIPE_RUNPATH_PH/g, lmfRecipeRunPath)
      .replace(/LMF_RECIPE_YAMLFILE_PH/g, lmfRecipeYamlFile)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `lma_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ä¿å­˜LlamaFactoryé…ç½®
app.post('/api/llamafactory-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/llamafactory-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('LlamaFactory config saved:', config);
    
    res.json({
      success: true,
      message: 'LlamaFactory configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving training config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½LlamaFactoryé…ç½®
app.get('/api/llamafactory-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/llamafactory-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'lmf-v1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op-v2:latest',
        instanceType: 'ml.g6.12xlarge',
        nprocPerNode: 4,
        replicas: 1,
        efaCount: 1,
        lmfRecipeRunPath: '/s3/train-recipes/llama-factory-project/',
        lmfRecipeYamlFile: 'qwen_full_dist_template.yaml',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('LlamaFactory config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading training config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜Scripté…ç½®
app.post('/api/script-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/script-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('Script config saved:', config);
    
    res.json({
      success: true,
      message: 'Script configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving script config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½Scripté…ç½®
app.get('/api/script-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/script-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'hypd-recipe-script-1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
        instanceType: 'ml.g5.12xlarge',
        nprocPerNode: 1,
        replicas: 1,
        efaCount: 16,
        projectPath: '/s3/training_code/my-training-project/',
        entryPath: 'train.py',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('Script config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading script config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²HyperPod Scriptè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºScriptè®­ç»ƒ
app.post('/api/launch-script-training', async (req, res) => {
  try {
    console.log('Raw script training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      trainingJobName,
      dockerImage = '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
      instanceType = 'ml.g5.12xlarge',
      nprocPerNode = 1,
      replicas = 1,
      efaCount = 16,
      projectPath,
      entryPath,
      mlflowTrackingUri = '',
      logMonitoringConfig
    } = req.body;

    console.log('Script training launch request parsed:', { 
      trainingJobName,
      dockerImage,
      instanceType,
      nprocPerNode,
      replicas,
      efaCount,
      projectPath,
      entryPath,
      mlflowTrackingUri,
      logMonitoringConfig: logMonitoringConfig ? 'present' : 'empty'
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!trainingJobName) {
      return res.status(400).json({
        success: false,
        error: 'Training job name is required'
      });
    }

    if (!projectPath) {
      return res.status(400).json({
        success: false,
        error: 'Project Path is required'
      });
    }

    if (!entryPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry Script Path is required'
      });
    }

    // è¯»å–Scriptè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/hyperpod-training-script-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // å¤„ç†æ—¥å¿—ç›‘æ§é…ç½®
    let logMonitoringConfigYaml = '';
    if (logMonitoringConfig && logMonitoringConfig.trim() !== '') {
      // æ·»åŠ é€‚å½“çš„ç¼©è¿›
      const indentedConfig = logMonitoringConfig
        .split('\n')
        .map(line => line.trim() ? `    ${line}` : line)
        .join('\n');
      logMonitoringConfigYaml = `
    logMonitoringConfiguration: 
${indentedConfig}`;
    }
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/TRAINING_JOB_NAME/g, trainingJobName)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/INSTANCE_TYPE/g, instanceType)
      .replace(/NPROC_PER_NODE/g, nprocPerNode.toString())
      .replace(/REPLICAS_COUNT/g, replicas.toString())
      .replace(/EFA_PER_NODE/g, efaCount.toString())
      .replace(/SCRIPT_RECIPE_PROJECTPATH_PH/g, projectPath)
      .replace(/SCRIPT_RECIPE_ENTRYPATH_PH/g, entryPath)
      .replace(/SM_MLFLOW_ARN/g, mlflowTrackingUri)
      .replace(/LOG_MONITORING_CONFIG/g, logMonitoringConfigYaml);

    console.log('Generated script training YAML content:', newYamlContent);

    // ç”Ÿæˆæ—¶é—´æˆ³
    const timestamp = Date.now();
    
    // ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆç”¨äºkubectl applyï¼‰
    const tempFileName = `script-training-${trainingJobName}-${timestamp}.yaml`;
    const tempFilePath = path.join(__dirname, '../temp', tempFileName);

    // ç”Ÿæˆæ°¸ä¹…ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    const permanentFileName = `script_${timestamp}.yaml`;
    const permanentFilePath = path.join(__dirname, '../templates/training', permanentFileName);

    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.join(__dirname, '../temp');
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // ç¡®ä¿templates/trainingç›®å½•å­˜åœ¨
    const trainingTemplateDir = path.join(__dirname, '../templates/training');
    if (!fs.existsSync(trainingTemplateDir)) {
      fs.mkdirSync(trainingTemplateDir, { recursive: true });
    }

    // å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºkubectl applyï¼‰
    await fs.writeFile(tempFilePath, newYamlContent);
    console.log(`Script training YAML written to temp file: ${tempFilePath}`);

    // å†™å…¥æ°¸ä¹…æ–‡ä»¶ï¼ˆä¿å­˜åˆ°templates/training/ç›®å½•ï¼‰
    await fs.writeFile(permanentFilePath, newYamlContent);
    console.log(`Script training YAML saved permanently to: ${permanentFilePath}`);

    // åº”ç”¨YAMLé…ç½® - è®­ç»ƒä»»åŠ¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    const applyOutput = await executeKubectl(`apply -f ${tempFilePath}`, 60000); // 60ç§’è¶…æ—¶
    console.log('Script training job apply output:', applyOutput);

    // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try {
      await fs.unlink(tempFilePath);
      console.log(`Temporary file deleted: ${tempFilePath}`);
    } catch (cleanupError) {
      console.warn(`Failed to delete temporary file: ${cleanupError.message}`);
    }

    // å¹¿æ’­è®­ç»ƒä»»åŠ¡å¯åŠ¨çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_launch',
      status: 'success',
      message: `Successfully launched script training job: ${trainingJobName}`,
      output: applyOutput
    });

    res.json({
      success: true,
      message: `Script training job "${trainingJobName}" launched successfully`,
      trainingJobName: trainingJobName,
      savedTemplate: permanentFileName,
      savedTemplatePath: permanentFilePath,
      output: applyOutput
    });

  } catch (error) {
    console.error('Script training launch error details:', {
      message: error.message,
      stack: error.stack,
      error: error
    });
    
    const errorMessage = error.message || error.toString() || 'Unknown error occurred';
    
    broadcast({
      type: 'training_launch',
      status: 'error',
      message: `Script training launch failed: ${errorMessage}`
    });
    
    res.status(500).json({
      success: false,
      error: errorMessage
    });
  }
});

// ä¿å­˜Torché…ç½®
app.post('/api/torch-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/torch-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('Torch config saved:', config);
    
    res.json({
      success: true,
      message: 'Torch configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving torch config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½Torché…ç½®
app.get('/api/torch-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/torch-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        trainingJobName: 'hypd-recipe-torch-1',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op:latest',
        instanceType: 'ml.g5.12xlarge',
        nprocPerNode: 1,
        replicas: 1,
        efaCount: 16,
        entryPythonScriptPath: '/s3/training_code/model-training-with-hyperpod-training-operator/torch-training.py',
        pythonScriptParameters: '--learning_rate 1e-5 \\\n--batch_size 1',
        mlflowTrackingUri: '',
        logMonitoringConfig: ''
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('Torch config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading torch config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜verlé…ç½®
app.post('/api/verl-config/save', async (req, res) => {
  try {
    const config = req.body;
    const configPath = path.join(__dirname, '../config/verl-config.json');
    
    // ç¡®ä¿configç›®å½•å­˜åœ¨
    const configDir = path.join(__dirname, '../config');
    if (!fs.existsSync(configDir)) {
      fs.mkdirSync(configDir, { recursive: true });
    }
    
    await fs.writeFile(configPath, JSON.stringify(config, null, 2));
    console.log('VERL config saved:', config);
    
    res.json({
      success: true,
      message: 'VERL configuration saved successfully'
    });
  } catch (error) {
    console.error('Error saving VERL config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// åŠ è½½verlé…ç½®
app.get('/api/verl-config/load', async (req, res) => {
  try {
    const configPath = path.join(__dirname, '../config/verl-config.json');
    
    if (!fs.existsSync(configPath)) {
      // è¿”å›é»˜è®¤é…ç½®
      const defaultConfig = {
        jobName: 'verl-training-a1',
        instanceType: 'ml.g5.12xlarge',
        entryPointPath: 'verl-project/src/qwen-3b-grpo-kuberay.sh',
        dockerImage: '633205212955.dkr.ecr.us-west-2.amazonaws.com/hypd-verl:latest',
        workerReplicas: 1,
        gpuPerNode: 4,
        efaPerNode: 1
      };
      
      return res.json({
        success: true,
        config: defaultConfig,
        isDefault: true
      });
    }
    
    const configContent = await fs.readFile(configPath, 'utf8');
    const config = JSON.parse(configContent);
    
    console.log('VERL config loaded:', config);
    
    res.json({
      success: true,
      config: config,
      isDefault: false
    });
  } catch (error) {
    console.error('Error loading VERL config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ç”Ÿæˆå¹¶éƒ¨ç½²VERLè®­ç»ƒä»»åŠ¡ - ä¸“é—¨ç”¨äºVERLè®­ç»ƒ
app.post('/api/launch-verl-training', async (req, res) => {
  try {
    console.log('Raw VERL training request body:', JSON.stringify(req.body, null, 2));
    
    const {
      jobName,
      instanceType = 'ml.g5.12xlarge',
      entryPointPath,
      dockerImage,
      workerReplicas = 1,
      gpuPerNode = 4,
      efaPerNode = 1,
      recipeType
    } = req.body;

    console.log('VERL training launch request parsed:', { 
      jobName,
      instanceType,
      entryPointPath,
      dockerImage,
      workerReplicas,
      gpuPerNode,
      efaPerNode,
      recipeType
    });

    // éªŒè¯å¿…éœ€å‚æ•°
    if (!jobName) {
      return res.status(400).json({
        success: false,
        error: 'Job name is required'
      });
    }

    if (!entryPointPath) {
      return res.status(400).json({
        success: false,
        error: 'Entry point path is required'
      });
    }

    if (!dockerImage) {
      return res.status(400).json({
        success: false,
        error: 'Docker image is required'
      });
    }

    // è¯»å–VERLè®­ç»ƒä»»åŠ¡æ¨¡æ¿
    const templatePath = path.join(__dirname, '../templates/verl-training-template.yaml');
    const templateContent = await fs.readFile(templatePath, 'utf8');
    
    // æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    const newYamlContent = templateContent
      .replace(/JOB_NAME/g, jobName)
      .replace(/ENTRY_POINT_PATH/g, entryPointPath)
      .replace(/DOCKER_IMAGE/g, dockerImage)
      .replace(/WORKER_REPLICAS/g, workerReplicas.toString())
      .replace(/MAX_REPLICAS/g, Math.max(3, workerReplicas + 2).toString())
      .replace(/GPU_PER_NODE/g, gpuPerNode.toString())
      .replace(/EFA_PER_NODE/g, efaPerNode.toString());

    console.log('Generated VERL YAML content preview:', newYamlContent.substring(0, 500) + '...');

    // ä½¿ç”¨ç»Ÿä¸€çš„éƒ¨ç½²å‡½æ•°
    const deployResult = await deployTrainingYaml('verl', jobName, newYamlContent);

    res.json({
      success: true,
      message: `VERL training job "${jobName}" launched successfully`,
      jobName: jobName,
      templateUsed: 'verl-training-template.yaml',
      savedTemplate: deployResult.permanentFileName,
      savedTemplatePath: deployResult.permanentFilePath,
      output: deployResult.applyOutput
    });

  } catch (error) {
    console.error('VERL training launch error:', error);
    
    res.status(500).json({
      success: false,
      error: error.message || 'Unknown error occurred'
    });
  }
});

// è·å–æ‰€æœ‰RayJob
app.get('/api/rayjobs', async (req, res) => {
  try {
    const output = await executeKubectl('get rayjobs -o json');
    const rayjobs = JSON.parse(output);
    res.json(rayjobs.items);
  } catch (error) {
    console.error('RayJobs fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// åˆ é™¤æŒ‡å®šçš„RayJob
app.delete('/api/rayjobs/:jobName', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Deleting RayJob: ${jobName}`);
    
    const output = await executeKubectl(`delete rayjob ${jobName}`);
    console.log('RayJob delete output:', output);
    
    // å‘é€WebSocketå¹¿æ’­
    broadcast({
      type: 'rayjob_deleted',
      status: 'success',
      message: `RayJob "${jobName}" deleted successfully`,
      jobName: jobName
    });
    
    res.json({
      success: true,
      message: `RayJob "${jobName}" deleted successfully`,
      output: output
    });
  } catch (error) {
    console.error('Error deleting RayJob:', error);
    
    broadcast({
      type: 'rayjob_deleted',
      status: 'error',
      message: `Failed to delete RayJob: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// è·å–æ‰€æœ‰HyperPodè®­ç»ƒä»»åŠ¡
app.get('/api/training-jobs', async (req, res) => {
  try {
    console.log('Fetching training jobs (HyperPod PytorchJob + RayJob)...');
    
    // è·å–HyperPod PytorchJob
    let hyperpodJobs = [];
    try {
      const hyperpodOutput = await executeKubectl('get hyperpodpytorchjob -o json');
      const hyperpodResult = JSON.parse(hyperpodOutput);
      hyperpodJobs = hyperpodResult.items.map(job => ({
        name: job.metadata.name,
        namespace: job.metadata.namespace || 'default',
        creationTimestamp: job.metadata.creationTimestamp,
        status: job.status || {},
        type: 'hyperpod',
        spec: {
          replicas: job.spec?.replicaSpecs?.[0]?.replicas || 0,
          nprocPerNode: job.spec?.nprocPerNode || 0
        }
      }));
    } catch (error) {
      console.log('No HyperPod PytorchJobs found or error:', error.message);
    }

    // è·å–RayJob
    let rayJobs = [];
    try {
      const rayOutput = await executeKubectl('get rayjob -o json');
      const rayResult = JSON.parse(rayOutput);
      rayJobs = rayResult.items.map(job => ({
        name: job.metadata.name,
        namespace: job.metadata.namespace || 'default',
        creationTimestamp: job.metadata.creationTimestamp,
        status: job.status || {},
        type: 'rayjob',
        spec: {
          replicas: 1, // RayJobé€šå¸¸æ˜¯å•ä¸ªä½œä¸š
          nprocPerNode: 1
        }
      }));
    } catch (error) {
      console.log('No RayJobs found or error:', error.message);
    }

    // åˆå¹¶ä¸¤ç§ç±»å‹çš„ä½œä¸š
    const trainingJobs = [...hyperpodJobs, ...rayJobs];
    
    console.log(`Found ${trainingJobs.length} training jobs (${hyperpodJobs.length} HyperPod + ${rayJobs.length} Ray):`, 
                trainingJobs.map(j => `${j.name}(${j.type})`));
    
    res.json({
      success: true,
      jobs: trainingJobs
    });
  } catch (error) {
    console.error('Error fetching training jobs:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      jobs: []
    });
  }
});

// åˆ é™¤æŒ‡å®šçš„HyperPodè®­ç»ƒä»»åŠ¡
app.delete('/api/training-jobs/:jobName', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Deleting training job: ${jobName}`);
    
    const output = await executeKubectl(`delete hyperpodpytorchjob ${jobName}`);
    console.log('Delete output:', output);
    
    // å¹¿æ’­åˆ é™¤çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'training_job_deleted',
      status: 'success',
      message: `Training job "${jobName}" deleted successfully`,
      jobName: jobName
    });
    
    res.json({
      success: true,
      message: `Training job "${jobName}" deleted successfully`,
      output: output
    });
  } catch (error) {
    console.error('Error deleting training job:', error);
    
    broadcast({
      type: 'training_job_deleted',
      status: 'error',
      message: `Failed to delete training job: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// MLflowé…ç½®ç®¡ç†

const CONFIG_FILE = path.join(__dirname, '../config/mlflow-metric-config.json');

// ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
const configDir = path.dirname(CONFIG_FILE);
if (!fs.existsSync(configDir)) {
  fs.mkdirSync(configDir, { recursive: true });
}

// é»˜è®¤MLflowé…ç½®
const DEFAULT_MLFLOW_CONFIG = {
  tracking_uri: '',
  experiment_id: '',
  sync_configs: {}
};

// è¯»å–MLflowé…ç½®
function readMlflowConfig() {
  try {
    if (fs.existsSync(CONFIG_FILE)) {
      const configData = fs.readFileSync(CONFIG_FILE, 'utf8');
      return JSON.parse(configData);
    }
  } catch (error) {
    console.error('Error reading MLflow config:', error);
  }
  return DEFAULT_MLFLOW_CONFIG;
}

// ä¿å­˜MLflowé…ç½®
function saveMlflowConfig(config) {
  try {
    fs.writeFileSync(CONFIG_FILE, JSON.stringify(config, null, 2));
    return true;
  } catch (error) {
    console.error('Error saving MLflow config:', error);
    return false;
  }
}

// è·å–MLflowé…ç½®
app.get('/api/mlflow-metric-config', (req, res) => {
  try {
    const config = readMlflowConfig();
    res.json({
      success: true,
      config: config
    });
  } catch (error) {
    console.error('Error fetching MLflow config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// ä¿å­˜MLflowé…ç½®
app.post('/api/mlflow-metric-config', (req, res) => {
  try {
    const { tracking_uri } = req.body;
    
    if (!tracking_uri) {
      return res.status(400).json({
        success: false,
        error: 'tracking_uri is required'
      });
    }

    const config = { tracking_uri };
    
    if (saveMlflowConfig(config)) {
      console.log('MLflow config saved:', config);
      res.json({
        success: true,
        config: config
      });
    } else {
      res.status(500).json({
        success: false,
        error: 'Failed to save configuration'
      });
    }
  } catch (error) {
    console.error('Error saving MLflow config:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// æµ‹è¯•MLflowè¿æ¥
app.post('/api/mlflow-metric-config/test', async (req, res) => {
  try {
    const { tracking_uri } = req.body;
    
    if (!tracking_uri) {
      return res.status(400).json({
        success: false,
        error: 'tracking_uri is required'
      });
    }

    console.log(`Testing MLflow connection to: ${tracking_uri}`);
    
    const { spawn } = require('child_process');
    
    // åˆ›å»ºæµ‹è¯•è„šæœ¬
    const testScript = `#!/usr/bin/env python3
import mlflow
import sys
import json

try:
    tracking_uri = "${tracking_uri}"
    mlflow.set_tracking_uri(tracking_uri)
    
    # å°è¯•è·å–å®éªŒåˆ—è¡¨æ¥æµ‹è¯•è¿æ¥
    experiments = mlflow.search_experiments()
    
    result = {
        "success": True,
        "experiments_count": len(experiments),
        "message": f"Successfully connected to MLflow. Found {len(experiments)} experiments."
    }
    
    print(json.dumps(result))
    sys.exit(0)
    
except Exception as e:
    result = {
        "success": False,
        "error": str(e)
    }
    print(json.dumps(result))
    sys.exit(1)
`;
    
    const tempScriptPath = path.join(__dirname, '../temp/test_mlflow_connection.py');
    
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.dirname(tempScriptPath);
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    
    fs.writeFileSync(tempScriptPath, testScript);
    
    const pythonPath = path.join(__dirname, '../.venv/bin/python');
    const pythonProcess = spawn(pythonPath, [tempScriptPath], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    pythonProcess.on('close', (code) => {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        fs.unlinkSync(tempScriptPath);
      } catch (e) {
        console.warn('Failed to cleanup temp file:', e);
      }
      
      if (stderr) {
        console.log('Python test script stderr:', stderr);
      }
      
      try {
        const result = JSON.parse(stdout);
        if (result.success) {
          res.json(result);
        } else {
          res.status(400).json(result);
        }
      } catch (parseError) {
        console.error('Failed to parse test result:', parseError);
        console.error('Raw output:', stdout);
        res.status(500).json({
          success: false,
          error: 'Failed to test MLflow connection',
          details: stdout || stderr
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      console.error('Failed to start Python test script:', error);
      res.status(500).json({
        success: false,
        error: `Failed to start test script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('MLflow connection test error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// MLflowè·¨è´¦æˆ·åŒæ­¥API
app.post('/api/mlflow-sync', async (req, res) => {
  try {
    const { sync_config, experiment_name, experiment_id } = req.body;
    
    // æ”¯æŒä¸¤ç§å‚æ•°æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
    const experimentIdentifier = experiment_name || experiment_id;
    
    // éªŒè¯å¿…éœ€å­—æ®µ
    if (!sync_config || !experimentIdentifier) {
      return res.status(400).json({
        success: false,
        error: 'sync_config and experiment_name (or experiment_id) are required'
      });
    }

    // éªŒè¯JSONé…ç½®
    let configObj;
    try {
      configObj = JSON.parse(sync_config);
    } catch (e) {
      return res.status(400).json({
        success: false,
        error: 'Invalid JSON format in sync_config'
      });
    }

    // éªŒè¯å¿…éœ€çš„é…ç½®å­—æ®µ
    const requiredFields = ['contributor_name', 'source_mlflow_arn', 'shared_account_id', 'shared_aws_region', 'cross_account_role_arn', 'shared_mlflow_arn'];
    const missingFields = requiredFields.filter(field => !configObj[field]);
    if (missingFields.length > 0) {
      return res.status(400).json({
        success: false,
        error: `Missing required fields in sync_config: ${missingFields.join(', ')}`
      });
    }

    // éªŒè¯sourceå’Œdestination ARNä¸èƒ½ç›¸åŒ
    if (configObj.source_mlflow_arn === configObj.shared_mlflow_arn) {
      return res.status(400).json({
        success: false,
        error: 'Source MLflow ARN and Shared MLflow ARN cannot be the same. Please ensure you are syncing to a different MLflow server.'
      });
    }

    // æ·»åŠ æ—¶é—´æˆ³
    configObj.setup_date = new Date().toISOString();

    console.log(`Starting MLflow sync for experiment ${experimentIdentifier}...`);
    
    // 1. ä¿å­˜é…ç½®åˆ°mlflow-metric-config.json
    const currentConfig = readMlflowConfig();
    const updatedConfig = {
      ...currentConfig,
      experiment_name: experimentIdentifier,  // æ”¹ä¸ºexperiment_name
      sync_configs: {
        ...configObj,
        last_sync: new Date().toISOString()
      }
    };
    
    if (!saveMlflowConfig(updatedConfig)) {
      return res.status(500).json({
        success: false,
        error: 'Failed to save sync configuration'
      });
    }

    // 2. åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ä¾›Pythonè„šæœ¬ä½¿ç”¨
    const tempConfigPath = path.join(__dirname, '../temp/sync-config-temp.json');
    
    // ç¡®ä¿tempç›®å½•å­˜åœ¨
    const tempDir = path.dirname(tempConfigPath);
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }
    
    fs.writeFileSync(tempConfigPath, JSON.stringify(configObj, null, 2));

    // 3. è°ƒç”¨PythonåŒæ­¥è„šæœ¬
    const { spawn } = require('child_process');
    const pythonPath = path.join(__dirname, '../.venv/bin/python');
    const syncScriptPath = path.join(__dirname, '../mlflow/cross_account_sync.py');
    
    const pythonProcess = spawn(pythonPath, [
      syncScriptPath,
      '--config-file', tempConfigPath,
      '--experiment-name', experimentIdentifier
    ], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    pythonProcess.on('close', (code) => {
      // æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
      try {
        fs.unlinkSync(tempConfigPath);
      } catch (e) {
        console.warn('Failed to cleanup temp config file:', e);
      }
      
      if (code === 0) {
        console.log('MLflow sync completed successfully');
        console.log('Sync output:', stdout);
        
        res.json({
          success: true,
          message: 'Successfully synced experiment to shared MLflow server',
          output: stdout,
          experiment_id: experimentIdentifier,
          contributor: configObj.contributor_name
        });
      } else {
        console.error('MLflow sync failed with code:', code);
        console.error('Sync stderr:', stderr);
        console.error('Sync stdout:', stdout);
        
        res.status(500).json({
          success: false,
          error: 'MLflow sync failed',
          details: stderr || stdout,
          exit_code: code
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      console.error('Failed to start MLflow sync script:', error);
      
      // æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
      try {
        fs.unlinkSync(tempConfigPath);
      } catch (e) {
        console.warn('Failed to cleanup temp config file:', e);
      }
      
      res.status(500).json({
        success: false,
        error: `Failed to start sync script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('MLflow sync API error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// è·å–è®­ç»ƒå†å²æ•°æ®ï¼ˆä»MLflowï¼‰
app.get('/api/training-history', async (req, res) => {
  try {
    console.log('Fetching training history from MLflow...');
    
    // è¯»å–å½“å‰MLflowé…ç½®
    const mlflowConfig = readMlflowConfig();
    console.log('Using MLflow URI:', mlflowConfig.tracking_uri);
    
    const { spawn } = require('child_process');
    const path = require('path');
    
    // ä½¿ç”¨é¡¹ç›®å†…è™šæ‹Ÿç¯å¢ƒçš„Pythonæ‰§è¡Œè„šæœ¬ï¼Œä¼ é€’é…ç½®å‚æ•°
    const pythonPath = path.join(__dirname, '../.venv/bin/python');
    const scriptPath = path.join(__dirname, '../mlflow/get_training_history.py');
    
    const pythonProcess = spawn(pythonPath, [scriptPath, mlflowConfig.tracking_uri], {
      cwd: __dirname,
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    pythonProcess.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    pythonProcess.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    pythonProcess.on('close', (code) => {
      if (stderr) {
        console.log('Python script stderr:', stderr);
      }
      
      if (code !== 0) {
        console.error(`Python script exited with code ${code}`);
        return res.status(500).json({ 
          success: false, 
          error: `Failed to fetch training history: exit code ${code}`,
          stderr: stderr
        });
      }
      
      try {
        const result = JSON.parse(stdout);
        console.log(`Training history fetched: ${result.total} records`);
        res.json(result);
      } catch (parseError) {
        console.error('Failed to parse Python script output:', parseError);
        console.error('Raw output:', stdout);
        res.status(500).json({ 
          success: false, 
          error: 'Failed to parse training history data',
          raw_output: stdout
        });
      }
    });
    
    pythonProcess.on('error', (error) => {
      console.error('Failed to start Python script:', error);
      res.status(500).json({ 
        success: false, 
        error: `Failed to start Python script: ${error.message}`
      });
    });
    
  } catch (error) {
    console.error('Training history fetch error:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// è·å–è®­ç»ƒä»»åŠ¡å…³è”çš„pods
app.get('/api/training-jobs/:jobName/pods', async (req, res) => {
  try {
    const { jobName } = req.params;
    console.log(`Fetching pods for training job: ${jobName}`);
    
    // è·å–æ‰€æœ‰podsï¼Œç„¶åç­›é€‰å‡ºå±äºè¯¥è®­ç»ƒä»»åŠ¡çš„pods
    const output = await executeKubectl('get pods -o json');
    const result = JSON.parse(output);
    
    // ç­›é€‰å‡ºå±äºè¯¥è®­ç»ƒä»»åŠ¡çš„pods
    const trainingPods = result.items.filter(pod => {
      const labels = pod.metadata.labels || {};
      const ownerReferences = pod.metadata.ownerReferences || [];
      
      // æ£€æŸ¥æ˜¯å¦é€šè¿‡æ ‡ç­¾æˆ–ownerReferenceså…³è”åˆ°è®­ç»ƒä»»åŠ¡
      return labels['training-job-name'] === jobName || 
             labels['app'] === jobName ||
             ownerReferences.some(ref => ref.name === jobName) ||
             pod.metadata.name.includes(jobName);
    });
    
    const pods = trainingPods.map(pod => ({
      name: pod.metadata.name,
      namespace: pod.metadata.namespace || 'default',
      status: pod.status.phase,
      creationTimestamp: pod.metadata.creationTimestamp,
      nodeName: pod.spec.nodeName,
      containerStatuses: pod.status.containerStatuses || []
    }));
    
    console.log(`Found ${pods.length} pods for training job ${jobName}:`, pods.map(p => p.name));
    
    res.json({
      success: true,
      pods: pods
    });
  } catch (error) {
    console.error('Error fetching training job pods:', error);
    res.status(500).json({
      success: false,
      error: error.message,
      pods: []
    });
  }
});

// è·å–å®Œæ•´æ—¥å¿—æ–‡ä»¶
app.get('/api/logs/:jobName/:podName', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      res.sendFile(path.resolve(logFilePath));
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found',
        path: logFilePath
      });
    }
  } catch (error) {
    console.error('Error serving log file:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// ä¸‹è½½å®Œæ•´æ—¥å¿—æ–‡ä»¶
app.get('/api/logs/:jobName/:podName/download', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      res.download(logFilePath, `${podName}.log`, (err) => {
        if (err) {
          console.error('Error downloading log file:', err);
          res.status(500).json({ 
            success: false, 
            error: 'Failed to download log file' 
          });
        }
      });
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found',
        path: logFilePath
      });
    }
  } catch (error) {
    console.error('Error downloading log file:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
app.get('/api/logs/:jobName/:podName/info', (req, res) => {
  try {
    const { jobName, podName } = req.params;
    const logFilePath = path.join(LOGS_BASE_DIR, jobName, `${podName}.log`);
    
    if (fs.existsSync(logFilePath)) {
      const stats = fs.statSync(logFilePath);
      res.json({
        success: true,
        info: {
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime,
          path: logFilePath
        }
      });
    } else {
      res.status(404).json({ 
        success: false, 
        error: 'Log file not found' 
      });
    }
  } catch (error) {
    console.error('Error getting log file info:', error);
    res.status(500).json({ 
      success: false, 
      error: error.message 
    });
  }
});

// åˆ é™¤éƒ¨ç½² - æ”¹è¿›ç‰ˆæœ¬
app.post('/api/undeploy', async (req, res) => {
  try {
    const { modelTag, deleteType } = req.body;
    
    if (!modelTag) {
      return res.status(400).json({
        success: false,
        error: 'Model tag is required'
      });
    }
    
    console.log(`Undeploying model: ${modelTag}, type: ${deleteType}`);
    
    // æ„å»ºå¯èƒ½çš„èµ„æºåç§°
    const possibleDeployments = [
      `vllm-${modelTag}-inference`,
      `sglang-${modelTag}-inference`,
      `olm-${modelTag}-inference`,
      `${modelTag}-inference`  // å¤‡ç”¨æ ¼å¼
    ];
    
    const possibleServices = [
      `vllm-${modelTag}-nlb`,
      `sglang-${modelTag}-nlb`,
      `olm-${modelTag}-nlb`,
      `${modelTag}-nlb`,
      `${modelTag}-service`  // å¤‡ç”¨æ ¼å¼
    ];
    
    let deleteCommands = [];
    let deletedResources = [];
    
    // æ ¹æ®åˆ é™¤ç±»å‹å†³å®šåˆ é™¤å“ªäº›èµ„æº
    if (deleteType === 'all' || deleteType === 'deployment') {
      possibleDeployments.forEach(deploymentName => {
        deleteCommands.push(`delete deployment ${deploymentName} --ignore-not-found=true`);
      });
      deletedResources.push('Deployment');
    }
    
    if (deleteType === 'all' || deleteType === 'service') {
      possibleServices.forEach(serviceName => {
        deleteCommands.push(`delete service ${serviceName} --ignore-not-found=true`);
      });
      deletedResources.push('Service');
    }
    
    // æ‰§è¡Œåˆ é™¤å‘½ä»¤
    const results = [];
    let actuallyDeleted = 0;
    
    for (const command of deleteCommands) {
      try {
        const output = await executeKubectl(command);
        const success = !output.includes('not found');
        results.push({
          command,
          success: true,
          output: output.trim(),
          actuallyDeleted: success
        });
        if (success) actuallyDeleted++;
      } catch (error) {
        results.push({
          command,
          success: false,
          error: error.error || error.message,
          actuallyDeleted: false
        });
      }
    }
    
    // ç­‰å¾…ä¸€ä¸‹è®©èµ„æºå®Œå…¨åˆ é™¤
    if (actuallyDeleted > 0) {
      console.log(`Waiting for resources to be fully deleted...`);
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
    
    // å¹¿æ’­åˆ é™¤çŠ¶æ€æ›´æ–°
    broadcast({
      type: 'undeployment',
      status: 'success',
      message: `Successfully deleted resources for ${modelTag} (${actuallyDeleted} resources)`,
      results: results
    });
    
    res.json({
      success: true,
      message: actuallyDeleted > 0 
        ? `Successfully deleted ${actuallyDeleted} resource(s)` 
        : 'No resources found to delete (may already be deleted)',
      deletedResources: deletedResources,
      results: results,
      modelTag: modelTag,
      actuallyDeleted: actuallyDeleted
    });
    
  } catch (error) {
    console.error('Undeploy error:', error);
    
    broadcast({
      type: 'undeployment',
      status: 'error',
      message: `Failed to undeploy ${req.body.modelTag}: ${error.message}`
    });
    
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// è·å–éƒ¨ç½²è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«æ¨¡å‹å…ƒæ•°æ®ï¼‰
app.get('/api/deployment-details', async (req, res) => {
  try {
    console.log('Fetching deployment details with metadata...');
    
    // è·å–æ‰€æœ‰deployment
    const deploymentsOutput = await executeKubectl('get deployments -o json');
    const deployments = JSON.parse(deploymentsOutput);
    
    // è·å–æ‰€æœ‰service
    const servicesOutput = await executeKubectl('get services -o json');
    const services = JSON.parse(servicesOutput);
    
    // è¿‡æ»¤å‡ºæ¨¡å‹ç›¸å…³çš„éƒ¨ç½²å¹¶æå–å…ƒæ•°æ®
    const modelDeployments = deployments.items
      .filter(deployment => 
        deployment.metadata.name.includes('vllm') || 
        deployment.metadata.name.includes('olm') ||
        deployment.metadata.name.includes('inference')
      )
      .map(deployment => {
        const labels = deployment.metadata.labels || {};
        const appLabel = labels.app;
        
        // æŸ¥æ‰¾å¯¹åº”çš„service
        const matchingService = services.items.find(service => 
          service.spec.selector?.app === appLabel
        );
        
        // ä»æ ‡ç­¾ä¸­æå–æ¨¡å‹ä¿¡æ¯
        const modelType = labels['model-type'] || 'unknown';
        const encodedModelId = labels['model-id'] || 'unknown';
        const modelTag = labels['model-tag'] || 'unknown';
        
        // ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹ID - ä¼˜å…ˆä»å®¹å™¨å‘½ä»¤ä¸­æå–åŸå§‹ID
        let modelId = 'unknown';
        
        // å¯¹äºVLLMéƒ¨ç½²ï¼Œä»å®¹å™¨å‘½ä»¤ä¸­æå–åŸå§‹æ¨¡å‹ID
        if (modelType === 'vllm') {
          try {
            const containers = deployment.spec?.template?.spec?.containers || [];
            const vllmContainer = containers.find(c => c.name === 'vllm-openai');
            if (vllmContainer && vllmContainer.command) {
              const command = vllmContainer.command;
              
              // 1. ä¼˜å…ˆæ£€æŸ¥æ–°çš„ vllm serve æ ¼å¼
              const serveIndex = command.findIndex(arg => arg === 'serve');
              if (serveIndex !== -1 && serveIndex + 1 < command.length) {
                // æ£€æŸ¥å‰ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯ vllm ç›¸å…³
                if (serveIndex > 0 && command[serveIndex - 1].includes('vllm')) {
                  const modelPath = command[serveIndex + 1];
                  // ç¡®ä¿ä¸æ˜¯ä»¥ -- å¼€å¤´çš„å‚æ•°
                  if (!modelPath.startsWith('--')) {
                    modelId = modelPath;
                  }
                }
              }
              
              // 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥ä¼ ç»Ÿçš„ --model å‚æ•°
              if (modelId === 'unknown') {
                const modelIndex = command.findIndex(arg => arg === '--model');
                if (modelIndex !== -1 && modelIndex + 1 < command.length) {
                  modelId = command[modelIndex + 1]; // è·å–--modelå‚æ•°åçš„å€¼
                }
              }
            }
          } catch (error) {
            console.log('Failed to extract model ID from VLLM command:', error.message);
          }
        }
        
        // å¯¹äºOllamaéƒ¨ç½²ï¼Œä»postStartç”Ÿå‘½å‘¨æœŸé’©å­ä¸­æå–æ¨¡å‹ID
        if (modelType === 'ollama' && modelId === 'unknown') {
          try {
            const containers = deployment.spec?.template?.spec?.containers || [];
            const ollamaContainer = containers.find(c => c.name === 'ollama');
            if (ollamaContainer && ollamaContainer.lifecycle?.postStart?.exec?.command) {
              const command = ollamaContainer.lifecycle.postStart.exec.command;
              // æŸ¥æ‰¾åŒ…å«"ollama pull"çš„å‘½ä»¤
              const commandStr = command.join(' ');
              const pullMatch = commandStr.match(/ollama pull ([^\s\\]+)/);
              if (pullMatch) {
                modelId = pullMatch[1]; // æå–æ¨¡å‹ID
                console.log('Extracted Ollama model ID from postStart:', modelId);
              }
            }
          } catch (error) {
            console.log('Failed to extract model ID from Ollama postStart command:', error.message);
          }
        }
        
        // å¯¹äºæ— æ³•æå–çš„æƒ…å†µï¼Œä½¿ç”¨è§£ç é€»è¾‘
        if (modelId === 'unknown' && encodedModelId !== 'unknown') {
          modelId = decodeModelIdFromLabel(encodedModelId);
        }
        
        // è·å–æœåŠ¡URL
        let serviceUrl = '';
        if (matchingService) {
          const ingress = matchingService.status?.loadBalancer?.ingress?.[0];
          if (ingress) {
            const host = ingress.hostname || ingress.ip;
            const port = matchingService.spec.ports?.[0]?.port || 8000;
            serviceUrl = `http://${host}:${port}`;
          }
        }
        
        return {
          deploymentName: deployment.metadata.name,
          serviceName: matchingService?.metadata.name || 'N/A',
          modelType: modelType,
          modelId: modelId,
          modelTag: modelTag,
          serviceUrl: serviceUrl,
          status: deployment.status.readyReplicas === deployment.spec.replicas ? 'Ready' : 'Pending',
          replicas: deployment.spec.replicas,
          readyReplicas: deployment.status.readyReplicas || 0,
          hasService: !!matchingService,
          isExternal: matchingService?.metadata?.annotations?.['service.beta.kubernetes.io/aws-load-balancer-scheme'] === 'internet-facing'
        };
      });
    
    console.log('Deployment details fetched:', modelDeployments.length, 'deployments');
    res.json(modelDeployments);
    
  } catch (error) {
    console.error('Deployment details fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// è·å–å·²éƒ¨ç½²çš„æ¨¡å‹åˆ—è¡¨
app.get('/api/deployments', async (req, res) => {
  try {
    console.log('Fetching deployments...');
    
    // è·å–æ‰€æœ‰deployment
    const deploymentsOutput = await executeKubectl('get deployments -o json');
    const deployments = JSON.parse(deploymentsOutput);
    
    // è·å–æ‰€æœ‰service
    const servicesOutput = await executeKubectl('get services -o json');
    const services = JSON.parse(servicesOutput);
    
    // è¿‡æ»¤å‡ºVLLMå’ŒOllamaç›¸å…³çš„éƒ¨ç½²
    const modelDeployments = deployments.items.filter(deployment => 
      deployment.metadata.name.includes('vllm') || 
      deployment.metadata.name.includes('olm') ||
      deployment.metadata.name.includes('inference')
    );
    
    // ä¸ºæ¯ä¸ªéƒ¨ç½²åŒ¹é…å¯¹åº”çš„service
    const deploymentList = modelDeployments.map(deployment => {
      const appLabel = deployment.metadata.labels?.app;
      const matchingService = services.items.find(service => 
        service.spec.selector?.app === appLabel
      );
      
      // ä»deploymentåç§°æå–model tagå’Œç±»å‹
      const deploymentName = deployment.metadata.name;
      let modelTag = 'unknown';
      let deploymentType = 'unknown';
      
      if (deploymentName.startsWith('vllm-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(5, -10); // ç§»é™¤ 'vllm-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'VLLM';
      } else if (deploymentName.startsWith('sglang-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(7, -10); // ç§»é™¤ 'sglang-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'SGLANG';
      } else if (deploymentName.startsWith('olm-') && deploymentName.endsWith('-inference')) {
        modelTag = deploymentName.slice(4, -10); // ç§»é™¤ 'olm-' å‰ç¼€å’Œ '-inference' åç¼€
        deploymentType = 'Ollama';
      }
      
      // æ£€æŸ¥æ˜¯å¦ä¸ºexternalè®¿é—®
      const isExternal = matchingService?.metadata?.annotations?.['service.beta.kubernetes.io/aws-load-balancer-scheme'] === 'internet-facing';
      
      return {
        modelTag,
        deploymentType,
        deploymentName: deployment.metadata.name,
        serviceName: matchingService?.metadata.name || 'N/A',
        replicas: deployment.spec.replicas,
        readyReplicas: deployment.status.readyReplicas || 0,
        status: deployment.status.readyReplicas === deployment.spec.replicas ? 'Ready' : 'Pending',
        createdAt: deployment.metadata.creationTimestamp,
        hasService: !!matchingService,
        serviceType: matchingService?.spec.type || 'N/A',
        isExternal: isExternal,
        externalIP: matchingService?.status?.loadBalancer?.ingress?.[0]?.hostname || 
                   matchingService?.status?.loadBalancer?.ingress?.[0]?.ip || 'Pending'
      };
    });
    
    console.log('Deployments fetched:', deploymentList.length, 'model deployments');
    res.json(deploymentList);
    
  } catch (error) {
    console.error('Deployments fetch error:', error);
    res.status(500).json({ error: error.message });
  }
});

// æµ‹è¯•æ¨¡å‹APIï¼ˆç”ŸæˆcURLå‘½ä»¤ï¼‰
app.post('/api/test-model', async (req, res) => {
  const { serviceUrl, payload } = req.body;
  
  try {
    let parsedPayload;
    
    try {
      parsedPayload = JSON.parse(payload);
    } catch (error) {
      return res.status(400).json({ error: 'Invalid JSON payload' });
    }
    
    const curlCommand = `curl -X POST "${serviceUrl}" \\
  -H "Content-Type: application/json" \\
  -d '${JSON.stringify(parsedPayload, null, 2)}'`;
    
    res.json({
      curlCommand,
      fullUrl: serviceUrl,
      message: 'Use the curl command to test your model'
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// WebSocketè¿æ¥å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ—¥å¿—æ±¡æŸ“
wss.on('connection', (ws) => {
  console.log('WebSocket client connected');
  
  // å‘é€çŠ¶æ€æ›´æ–°çš„å‡½æ•°
  const sendStatusUpdate = async () => {
    try {
      const [pods, services] = await Promise.all([
        executeKubectl('get pods -o json').then(output => JSON.parse(output).items),
        executeKubectl('get services -o json').then(output => JSON.parse(output).items)
      ]);
      
      const statusData = {
        type: 'status_update',
        pods,
        services,
        timestamp: new Date().toISOString()
      };
      
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify(statusData));
        console.log(`ğŸ“¡ Status update sent: ${pods.length} pods, ${services.length} services`);
      }
    } catch (error) {
      console.error('âŒ Error fetching status for WebSocket:', error);
    }
  };
  
  // ğŸš€ ä¼˜åŒ–ï¼šåªåœ¨è¿æ¥æ—¶å‘é€ä¸€æ¬¡åˆå§‹çŠ¶æ€ï¼Œä¸å†å®šæ—¶å‘é€
  sendStatusUpdate();
  
  // å­˜å‚¨WebSocketè¿æ¥ï¼Œç”¨äºæŒ‰éœ€å¹¿æ’­
  ws.isAlive = true;
  ws.lastActivity = Date.now();
  
  // å¤„ç†WebSocketæ¶ˆæ¯
  ws.on('message', (message) => {
    try {
      const data = JSON.parse(message);
      ws.lastActivity = Date.now();
      
      // ğŸ¯ æŒ‰éœ€å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
      switch (data.type) {
        case 'request_status_update':
          // å®¢æˆ·ç«¯ä¸»åŠ¨è¯·æ±‚çŠ¶æ€æ›´æ–°
          console.log('ğŸ“¡ Client requested status update');
          sendStatusUpdate();
          break;
          
        case 'start_log_stream':
          console.log(`ğŸ”„ Starting log stream for ${data.jobName}/${data.podName}`);
          startLogStream(ws, data.jobName, data.podName);
          break;
          
        case 'stop_log_stream':
          console.log(`â¹ï¸ Stopping log stream for ${data.jobName}/${data.podName}`);
          stopLogStream(ws, data.jobName, data.podName);
          break;
          
        case 'stop_all_log_streams':
          console.log('â¹ï¸ Stopping all log streams');
          stopAllLogStreams(ws);
          break;
          
        case 'ping':
          // å¿ƒè·³æ£€æµ‹
          if (ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify({ type: 'pong', timestamp: new Date().toISOString() }));
          }
          break;
          
        default:
          console.log('ğŸ“¨ Received WebSocket message:', data.type);
      }
    } catch (error) {
      console.error('âŒ Error parsing WebSocket message:', error);
    }
  });
  
  // å¿ƒè·³æ£€æµ‹
  ws.on('pong', () => {
    ws.isAlive = true;
    ws.lastActivity = Date.now();
  });
  
  ws.on('close', () => {
    console.log('ğŸ”Œ WebSocket client disconnected');
    // æ¸…ç†è¯¥è¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
    stopAllLogStreams(ws);
  });
  
  ws.on('error', (error) => {
    console.error('âŒ WebSocket error:', error);
    // æ¸…ç†è¯¥è¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
    stopAllLogStreams(ws);
  });
});

// ğŸš€ å¹¿æ’­å‡½æ•° - å‘æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯
function broadcast(message) {
  const messageStr = JSON.stringify({
    ...message,
    timestamp: new Date().toISOString()
  });
  
  let sentCount = 0;
  wss.clients.forEach((client) => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(messageStr);
      sentCount++;
    }
  });
  
  if (sentCount > 0) {
    console.log(`ğŸ“¡ Broadcast sent to ${sentCount} clients:`, message.type);
  }
}

// ğŸ”„ æŒ‰éœ€çŠ¶æ€æ›´æ–°å¹¿æ’­
function broadcastStatusUpdate() {
  const message = {
    type: 'request_status_update_broadcast',
    source: 'server'
  };
  broadcast(message);
}

// â¤ï¸ WebSocketå¿ƒè·³æ£€æµ‹ - æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡è¿æ¥çŠ¶æ€
const heartbeatInterval = setInterval(() => {
  const now = Date.now();
  let activeConnections = 0;
  
  wss.clients.forEach((ws) => {
    if (ws.readyState === WebSocket.OPEN) {
      // æ£€æŸ¥è¿æ¥æ˜¯å¦æ´»è·ƒï¼ˆ5åˆ†é’Ÿå†…æœ‰æ´»åŠ¨ï¼‰
      if (now - ws.lastActivity < 300000) {
        ws.ping();
        activeConnections++;
      } else {
        console.log('ğŸ”Œ Terminating inactive WebSocket connection');
        ws.terminate();
      }
    }
  });
  
  // åªåœ¨æœ‰è¿æ¥æ—¶è¾“å‡ºå¿ƒè·³æ—¥å¿—
  if (activeConnections > 0) {
    console.log(`â¤ï¸ WebSocket heartbeat: ${activeConnections} active connections`);
  }
}, 30000);

// ğŸ§¹ è¿›ç¨‹æ¸…ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬
process.on('SIGTERM', () => {
  console.log('ğŸ›‘ Received SIGTERM signal - Server shutting down gracefully...');
  gracefulShutdown('SIGTERM');
});

process.on('SIGINT', () => {
  console.log('ğŸ›‘ Received SIGINT signal (Ctrl+C) - Server shutting down gracefully...');
  gracefulShutdown('SIGINT');
});

process.on('uncaughtException', (error) => {
  console.error('âŒ Uncaught Exception:', error);
  console.error('Stack trace:', error.stack);
  gracefulShutdown('uncaughtException');
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('âŒ Unhandled Rejection at:', promise, 'reason:', reason);
  gracefulShutdown('unhandledRejection');
});

// ä¼˜é›…å…³é—­å‡½æ•°
function gracefulShutdown(signal) {
  console.log(`ğŸ”„ Starting graceful shutdown (signal: ${signal})...`);
  
  // æ¸…ç†WebSocketå¿ƒè·³æ£€æµ‹
  if (typeof heartbeatInterval !== 'undefined') {
    clearInterval(heartbeatInterval);
    console.log('âœ… WebSocket heartbeat interval cleared');
  }
  
  // å…³é—­WebSocketæœåŠ¡å™¨
  if (wss) {
    console.log(`ğŸ“¡ Closing WebSocket server (${wss.clients.size} active connections)...`);
    wss.close(() => {
      console.log('âœ… WebSocket server closed');
    });
  }
  
  // æ¸…ç†æ´»è·ƒçš„æ—¥å¿—æµ
  if (activeLogStreams && activeLogStreams.size > 0) {
    console.log(`ğŸ§¹ Cleaning up ${activeLogStreams.size} active log streams...`);
    activeLogStreams.clear();
    console.log('âœ… Log streams cleaned up');
  }
  
  console.log('âœ… Graceful shutdown completed');
  
  // ç»™ä¸€äº›æ—¶é—´è®©æ¸…ç†å®Œæˆï¼Œç„¶åé€€å‡º
  setTimeout(() => {
    process.exit(signal === 'uncaughtException' || signal === 'unhandledRejection' ? 1 : 0);
  }, 1000);
}

// å¯åŠ¨podæ—¥å¿—æµ
function startLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  
  // å¦‚æœå·²ç»æœ‰è¯¥podçš„æ—¥å¿—æµï¼Œå…ˆåœæ­¢å®ƒ
  if (activeLogStreams.has(streamKey)) {
    stopLogStream(ws, jobName, podName);
  }
  
  console.log(`Starting log stream for pod: ${podName} in job: ${jobName}`);
  
  // åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
  const logFilePath = ensureLogDirectory(jobName, podName);
  const logStream = fs.createWriteStream(logFilePath, { flags: 'a' });
  
  // å¯åŠ¨kubectl logså‘½ä»¤
  const logProcess = spawn('kubectl', ['logs', '-f', podName], {
    stdio: ['pipe', 'pipe', 'pipe']
  });
  
  // å­˜å‚¨è¿›ç¨‹å¼•ç”¨å’Œæ–‡ä»¶æµ
  activeLogStreams.set(streamKey, {
    process: logProcess,
    logStream: logStream,
    ws: ws,
    jobName: jobName,
    podName: podName
  });
  
  // å¤„ç†æ ‡å‡†è¾“å‡º
  logProcess.stdout.on('data', (data) => {
    const logLine = data.toString();
    const timestamp = new Date().toISOString();
    
    // å†™å…¥æ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    logStream.write(`[${timestamp}] ${logLine}`);
    
    // å‘é€åˆ°å‰ç«¯
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_data',
        jobName: jobName,
        podName: podName,
        data: logLine,
        timestamp: timestamp
      }));
    }
  });
  
  // å¤„ç†æ ‡å‡†é”™è¯¯
  logProcess.stderr.on('data', (data) => {
    const errorLine = data.toString();
    const timestamp = new Date().toISOString();
    
    console.error(`Log stream error for ${podName}:`, errorLine);
    
    // å†™å…¥é”™è¯¯åˆ°æ–‡ä»¶
    logStream.write(`[${timestamp}] ERROR: ${errorLine}`);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_error',
        jobName: jobName,
        podName: podName,
        error: errorLine,
        timestamp: timestamp
      }));
    }
  });
  
  // å¤„ç†è¿›ç¨‹é€€å‡º
  logProcess.on('close', (code) => {
    console.log(`Log stream for ${podName} closed with code: ${code}`);
    
    // å…³é—­æ–‡ä»¶æµ
    logStream.end();
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_closed',
        jobName: jobName,
        podName: podName,
        code: code,
        timestamp: new Date().toISOString()
      }));
    }
  });
  
  // å¤„ç†è¿›ç¨‹é”™è¯¯
  logProcess.on('error', (error) => {
    console.error(`Log stream process error for ${podName}:`, error);
    
    // å…³é—­æ–‡ä»¶æµ
    logStream.end();
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_error',
        jobName: jobName,
        podName: podName,
        error: error.message,
        timestamp: new Date().toISOString()
      }));
    }
  });
}

// åœæ­¢ç‰¹å®špodçš„æ—¥å¿—æµ
function stopLogStream(ws, jobName, podName) {
  const streamKey = `${jobName}-${podName}`;
  const streamInfo = activeLogStreams.get(streamKey);
  
  if (streamInfo) {
    console.log(`Stopping log stream for pod: ${podName}`);
    streamInfo.process.kill('SIGTERM');
    
    // å…³é—­æ–‡ä»¶æµ
    if (streamInfo.logStream) {
      streamInfo.logStream.end();
    }
    
    activeLogStreams.delete(streamKey);
    
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({
        type: 'log_stream_stopped',
        jobName: jobName,
        podName: podName,
        timestamp: new Date().toISOString()
      }));
    }
  }
}

// åœæ­¢æŸä¸ªWebSocketè¿æ¥çš„æ‰€æœ‰æ—¥å¿—æµ
function stopAllLogStreams(ws) {
  const streamsToStop = [];
  
  // ä»ç»Ÿä¸€æ—¥å¿—æµä¸­ç§»é™¤è¯¥WebSocketè¿æ¥
  unifiedLogStreams.forEach((stream, streamKey) => {
    if (stream.webSockets.has(ws)) {
      const [jobName, podName] = streamKey.split('-');
      streamsToStop.push({ jobName, podName });
    }
  });
  
  // ç§»é™¤WebSocketè¿æ¥
  streamsToStop.forEach(({ jobName, podName }) => {
    removeWebSocketFromLogStream(ws, jobName, podName);
  });
  
  if (streamsToStop.length > 0) {
    console.log(`ğŸ§¹ Cleaned up ${streamsToStop.length} log streams for disconnected WebSocket`);
  }
}

// æ¨¡å‹ä¸‹è½½API
app.post('/api/download-model', async (req, res) => {
  try {
    const { modelId, hfToken } = req.body;
    
    if (!modelId) {
      return res.json({ success: false, error: 'Model ID is required' });
    }
    
    console.log(`Starting model download for: ${modelId}`);
    
    // è¯»å–HFä¸‹è½½æ¨¡æ¿
    const templatePath = path.join(__dirname, '..', 'templates', 'hf-download-template.yaml');
    let template = await fs.readFile(templatePath, 'utf8');
    
    // ç”Ÿæˆæ¨¡å‹æ ‡ç­¾
    const modelTag = generateModelTag(modelId);
    
    // æ›¿æ¢åŸºæœ¬å˜é‡
    const replacements = {
      'HF_MODEL_ID': modelId,
      'MODEL_TAG': modelTag
    };
    
    // å¤„ç†HF Tokenç¯å¢ƒå˜é‡
    if (hfToken && hfToken.trim()) {
      const tokenEnv = `
        - name: HF_TOKEN
          value: "${hfToken.trim()}"`;
      template = template.replace('env:HF_TOKEN_ENV', `env:${tokenEnv}`);
      
      // åŒæ—¶åœ¨hf downloadå‘½ä»¤ä¸­å¯ç”¨token
      template = template.replace('#  --token=$HF_TOKEN', '          --token=$HF_TOKEN \\');
    } else {
      // ç§»é™¤HF_TOKEN_ENVå ä½ç¬¦ï¼Œä¿ç•™å…¶ä»–ç¯å¢ƒå˜é‡
      template = template.replace('      env:HF_TOKEN_ENV', '      env:');
    }
    
    // æ›¿æ¢å…¶ä»–å˜é‡
    Object.keys(replacements).forEach(key => {
      const regex = new RegExp(key, 'g');
      template = template.replace(regex, replacements[key]);
    });
    
    // ç¡®ä¿deploymentsç›®å½•å­˜åœ¨
    const deploymentsDir = path.join(__dirname, '..', 'deployments');
    await fs.ensureDir(deploymentsDir);
    
    // ä¿å­˜ç”Ÿæˆçš„YAMLæ–‡ä»¶
    const deploymentFile = path.join(deploymentsDir, `model-download-${modelTag}.yaml`);
    await fs.writeFile(deploymentFile, template);
    
    console.log(`Generated deployment file: ${deploymentFile}`);
    
    // åº”ç”¨åˆ°Kubernetes
    try {
      const result = await executeKubectl(`apply -f "${deploymentFile}"`);
      console.log('kubectl apply result:', result);
      
      // å¹¿æ’­éƒ¨ç½²çŠ¶æ€æ›´æ–°
      broadcast({
        type: 'model_download',
        status: 'success',
        message: `Model download started for ${modelId}`,
        modelId: modelId,
        modelTag: modelTag
      });
      
      res.json({ 
        success: true, 
        message: `Model download initiated for ${modelId}`,
        modelTag: modelTag
      });
      
    } catch (kubectlError) {
      console.error('kubectl apply error:', kubectlError);
      res.json({ 
        success: false, 
        error: `Failed to apply deployment: ${kubectlError.error || kubectlError}` 
      });
    }
    
  } catch (error) {
    console.error('Model download error:', error);
    res.json({ success: false, error: error.message });
  }
});

// S3å­˜å‚¨ä¿¡æ¯API - ä»s3-pv PersistentVolumeè·å–æ¡¶ä¿¡æ¯
app.get('/api/s3-storage', async (req, res) => {
  try {
    console.log('Fetching S3 storage information from s3-pv...');
    
    let bucketName = null;
    let bucketInfo = null;
    let region = null;
    
    try {
      // ç›´æ¥ä»s3-pv PersistentVolumeè·å–S3æ¡¶ä¿¡æ¯
      const pvResult = await executeKubectl('get pv s3-pv -o json');
      const pvData = JSON.parse(pvResult);
      
      console.log('PV data retrieved:', JSON.stringify(pvData, null, 2));
      
      // ä»PVçš„spec.csi.volumeAttributesä¸­æå–S3æ¡¶ä¿¡æ¯
      if (pvData.spec && pvData.spec.csi && pvData.spec.csi.volumeAttributes) {
        const volumeAttributes = pvData.spec.csi.volumeAttributes;
        
        // å¸¸è§çš„S3 CSIé©±åŠ¨å™¨å±æ€§åç§°
        bucketName = volumeAttributes.bucketName || 
                    volumeAttributes.bucket || 
                    volumeAttributes['s3.bucket'] ||
                    volumeAttributes['csi.storage.k8s.io/bucket'];
        
        region = volumeAttributes.region || 
                volumeAttributes['s3.region'] ||
                volumeAttributes['csi.storage.k8s.io/region'];
        
        bucketInfo = {
          storageClass: pvData.spec.storageClassName,
          capacity: pvData.spec.capacity?.storage,
          accessModes: pvData.spec.accessModes,
          csiDriver: pvData.spec.csi?.driver,
          volumeHandle: pvData.spec.csi?.volumeHandle,
          region: region
        };
        
        console.log(`Extracted bucket info: bucket=${bucketName}, region=${region}`);
      }
      
      // å¦‚æœä»volumeAttributesä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»volumeHandleä¸­è§£æ
      if (!bucketName && pvData.spec && pvData.spec.csi && pvData.spec.csi.volumeHandle) {
        const volumeHandle = pvData.spec.csi.volumeHandle;
        console.log('Trying to extract bucket from volumeHandle:', volumeHandle);
        
        // volumeHandleé€šå¸¸åŒ…å«æ¡¶åï¼Œæ ¼å¼å¯èƒ½æ˜¯: s3://bucket-name æˆ– bucket-name
        if (volumeHandle.startsWith('s3://')) {
          bucketName = volumeHandle.replace('s3://', '').split('/')[0];
        } else if (volumeHandle.includes('::')) {
          // æŸäº›CSIé©±åŠ¨ä½¿ç”¨ region::bucket-name æ ¼å¼
          const parts = volumeHandle.split('::');
          if (parts.length >= 2) {
            region = parts[0];
            bucketName = parts[1];
          }
        } else {
          // ç›´æ¥ä½¿ç”¨volumeHandleä½œä¸ºæ¡¶å
          bucketName = volumeHandle;
        }
        
        console.log(`Extracted from volumeHandle: bucket=${bucketName}, region=${region}`);
      }
      
      // æ£€æŸ¥ mountOptions ä¸­çš„ region ä¿¡æ¯
      if (!region && pvData.spec && pvData.spec.mountOptions) {
        const mountOptions = pvData.spec.mountOptions;
        console.log('Checking mountOptions for region:', mountOptions);
        
        for (const option of mountOptions) {
          if (typeof option === 'string' && option.startsWith('region ')) {
            region = option.replace('region ', '').trim();
            console.log(`Found region in mountOptions: ${region}`);
            break;
          }
        }
      }
      
      // æ›´æ–° bucketInfo ä¸­çš„ region ä¿¡æ¯
      if (bucketInfo && region) {
        bucketInfo.region = region;
        console.log(`Updated bucketInfo with region: ${region}`);
      }
      
      // å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»annotationsä¸­è·å–
      if (!bucketName && pvData.metadata && pvData.metadata.annotations) {
        const annotations = pvData.metadata.annotations;
        bucketName = annotations['s3.bucket'] || 
                    annotations['csi.storage.k8s.io/bucket'] ||
                    annotations['volume.beta.kubernetes.io/bucket'];
        
        if (!region) {
          region = annotations['s3.region'] || 
                  annotations['csi.storage.k8s.io/region'] ||
                  annotations['volume.beta.kubernetes.io/region'];
        }
        
        console.log(`Extracted from annotations: bucket=${bucketName}, region=${region}`);
      }
      
    } catch (pvError) {
      console.error('Could not get s3-pv PV info:', pvError.error);
      return res.json({
        success: false,
        error: `Failed to get s3-pv PersistentVolume: ${pvError.error}`,
        bucketInfo: null
      });
    }
    
    // éªŒè¯æ˜¯å¦æˆåŠŸè·å–åˆ°æ¡¶å
    if (!bucketName) {
      return res.json({
        success: false,
        error: 'Could not extract S3 bucket name from s3-pv PersistentVolume',
        bucketInfo: bucketInfo,
        message: 'S3 bucket information not found in PV configuration'
      });
    }
    
    console.log(`Using S3 bucket: ${bucketName} in region: ${region || 'default'}`);
    
    // å°è¯•åˆ—å‡ºS3å†…å®¹ - åªè·å–ä¸€çº§ç›®å½•å’Œæ–‡ä»¶
    try {
      const s3ListResult = await new Promise((resolve, reject) => {
        const s3Command = region ? 
          `aws s3 ls s3://${bucketName}/ --region ${region}` :
          `aws s3 ls s3://${bucketName}/`;
          
        console.log('Executing S3 command:', s3Command);
        
        exec(s3Command, (error, stdout, stderr) => {
          if (error) {
            console.error('S3 command error:', error.message);
            console.error('S3 command stderr:', stderr);
            reject({ error: error.message, stderr });
          } else {
            resolve(stdout);
          }
        });
      });
      
      // è§£æS3 lsè¾“å‡º
      const s3Items = [];
      if (s3ListResult.trim()) {
        const lines = s3ListResult.trim().split('\n');
        console.log(`Processing ${lines.length} S3 items...`);
        
        lines.forEach(line => {
          const trimmedLine = line.trim();
          if (!trimmedLine) return;
          
          // S3 ls è¾“å‡ºæ ¼å¼:
          // å¯¹äºç›®å½•: "                           PRE dirname/"
          // å¯¹äºæ–‡ä»¶: "2023-08-06 05:48:52       1234 filename.txt"
          
          if (trimmedLine.includes('PRE ')) {
            // è¿™æ˜¯ä¸€ä¸ªç›®å½•
            const dirName = trimmedLine.split('PRE ')[1];
            s3Items.push({
              key: dirName,
              name: dirName.replace('/', ''),
              size: null,
              lastModified: null,
              type: 'folder',
              storageClass: null
            });
          } else {
            // è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶
            const parts = trimmedLine.split(/\s+/);
            if (parts.length >= 3) {
              const date = parts[0];
              const time = parts[1];
              const size = parseInt(parts[2]);
              const fileName = parts.slice(3).join(' ');
              
              s3Items.push({
                key: fileName,
                name: fileName,
                size: size,
                lastModified: `${date} ${time}`,
                type: 'file',
                storageClass: 'STANDARD'
              });
            }
          }
        });
      }
      
      console.log(`Successfully retrieved ${s3Items.length} items from S3`);
      
      res.json({
        success: true,
        data: s3Items,
        bucketInfo: {
          bucket: bucketName,
          region: region || 'us-east-1',
          ...bucketInfo
        }
      });
      
    } catch (s3Error) {
      console.error('S3 list error:', s3Error);
      res.json({
        success: false,
        error: `Failed to list S3 contents: ${s3Error.error || s3Error}`,
        bucketInfo: {
          bucket: bucketName,
          region: region || 'us-east-1',
          ...bucketInfo
        }
      });
    }
    
  } catch (error) {
    console.error('S3 storage API error:', error);
    res.json({ 
      success: false, 
      error: error.message,
      bucketInfo: null
    });
  }
});

// ==================== é›†ç¾¤ç®¡ç† API ====================

// ä¿å­˜é›†ç¾¤é…ç½®åˆ° init_envs æ–‡ä»¶

// æ‰§è¡Œé›†ç¾¤é…ç½®è„šæœ¬ (Step 2)
// ==================== é›†ç¾¤ç®¡ç† API ====================

// æ—¥å¿—ç®¡ç†ç±»
class ClusterLogManager {
  constructor() {
    this.baseDir = path.join(__dirname, '../tmp/cluster-management');
    this.logsDir = path.join(this.baseDir, 'logs');
    this.currentDir = path.join(this.baseDir, 'current');
    this.metadataDir = path.join(this.baseDir, 'metadata');
    
    // ç¡®ä¿ç›®å½•å­˜åœ¨
    [this.baseDir, this.logsDir, this.currentDir, this.metadataDir].forEach(dir => {
      fs.ensureDirSync(dir);
    });
  }

  createLogFile(step) {
    const timestamp = new Date().toISOString()
      .replace(/:/g, '-')
      .replace(/\..+/, '')
      .replace('T', '_');
    
    const logFileName = `${timestamp}_${step}.log`;
    const logFilePath = path.join(this.logsDir, logFileName);
    const currentLinkPath = path.join(this.currentDir, `${step}.log`);
    
    // åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    fs.writeFileSync(logFilePath, '');
    
    // åˆ›å»º/æ›´æ–°è½¯é“¾æ¥
    if (fs.existsSync(currentLinkPath)) {
      fs.unlinkSync(currentLinkPath);
    }
    fs.symlinkSync(path.relative(this.currentDir, logFilePath), currentLinkPath);
    
    return {
      logFilePath,
      logId: path.basename(logFileName, '.log')
    };
  }

  getCurrentLogContent(step) {
    const currentLogPath = path.join(this.currentDir, `${step}.log`);
    if (fs.existsSync(currentLogPath)) {
      return fs.readFileSync(currentLogPath, 'utf8');
    }
    return '';
  }

  updateStatus(step, status) {
    const statusFile = path.join(this.metadataDir, `${step}_status.json`);
    const statusData = {
      step,
      status,
      timestamp: new Date().toISOString(),
      logId: this.getCurrentLogId(step)
    };
    fs.writeFileSync(statusFile, JSON.stringify(statusData, null, 2));
  }

  getCurrentLogId(step) {
    const currentLogPath = path.join(this.currentDir, `${step}.log`);
    if (fs.existsSync(currentLogPath)) {
      const realPath = fs.readlinkSync(currentLogPath);
      return path.basename(realPath, '.log');
    }
    return null;
  }
}

const logManager = new ClusterLogManager();

// æ£€æŸ¥ Step 1 çŠ¶æ€çš„å‡½æ•° - åŸºäº CloudFormation
async function checkStep1Status() {
  try {
    // ä½¿ç”¨å¤šé›†ç¾¤ç®¡ç†å™¨è·å–æ´»è·ƒé›†ç¾¤é…ç½®
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeCluster = clusterManager.getActiveCluster();
    
    if (!activeCluster) {
      return { status: 'unknown', error: 'No active cluster found' };
    }

    // ä»æ´»è·ƒé›†ç¾¤çš„é…ç½®ç›®å½•è¯»å–
    const configDir = clusterManager.getClusterConfigDir(activeCluster);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return { status: 'unknown', error: `init_envs not found for cluster: ${activeCluster}` };
    }
    
    const envContent = fs.readFileSync(initEnvsPath, 'utf8');
    
    // é¦–å…ˆè§£æ CLUSTER_TAG
    const clusterTagMatch = envContent.match(/export CLUSTER_TAG=(.+)/);
    if (!clusterTagMatch) {
      return { status: 'unknown', error: 'CLUSTER_TAG not found in init_envs' };
    }
    
    const clusterTag = clusterTagMatch[1].trim();
    const stackName = `full-stack-${clusterTag}`;
    
    // æ£€æŸ¥çŠ¶æ€ç¼“å­˜æ–‡ä»¶
    const statusCacheFile = path.join(logManager.metadataDir, 'step1_status_cache.json');
    let cachedStatus = null;
    
    if (fs.existsSync(statusCacheFile)) {
      try {
        cachedStatus = JSON.parse(fs.readFileSync(statusCacheFile, 'utf8'));
        // å¦‚æœå †æ ˆåç§°æ²¡æœ‰å˜åŒ–ä¸”çŠ¶æ€æ˜¯å®Œæˆï¼Œç›´æ¥è¿”å›ç¼“å­˜
        if (cachedStatus.stackName === stackName && cachedStatus.status === 'completed') {
          console.log(`Using cached status for stack: ${stackName}`);
          return cachedStatus;
        }
      } catch (error) {
        console.warn('Failed to read status cache:', error);
      }
    }
    
    console.log(`Checking CloudFormation status for stack: ${stackName}`);
    
    // æŸ¥è¯¢ CloudFormation çŠ¶æ€
    const command = `aws cloudformation describe-stacks --stack-name "${stackName}" --output json`;
    
    return new Promise((resolve) => {
      exec(command, { timeout: 30000 }, (error, stdout, stderr) => {
        let result;
        
        if (error) {
          console.error('CloudFormation query error:', error.message);
          if (stderr.includes('does not exist')) {
            result = { status: 'not_started', stackName };
          } else {
            result = { status: 'error', error: error.message, stackName };
          }
        } else {
          try {
            const cfResult = JSON.parse(stdout);
            if (cfResult.Stacks && cfResult.Stacks.length > 0) {
              const stack = cfResult.Stacks[0];
              const cfStatus = stack.StackStatus;
              
              console.log(`CloudFormation status: ${cfStatus}`);
              
              let status;
              if (cfStatus === 'CREATE_COMPLETE' || cfStatus === 'UPDATE_COMPLETE') {
                status = 'completed';
              } else if (cfStatus.includes('IN_PROGRESS')) {
                status = 'running';
              } else if (cfStatus.includes('FAILED')) {
                status = 'failed';
              } else {
                status = 'unknown';
              }
              
              result = {
                status,
                stackName,
                cloudFormationStatus: cfStatus,
                lastUpdated: stack.LastUpdatedTime || stack.CreationTime,
                clusterTag: clusterTag
              };
            } else {
              result = { status: 'not_found', stackName };
            }
          } catch (parseError) {
            console.error('Error parsing CloudFormation response:', parseError);
            result = { status: 'error', error: 'Failed to parse CloudFormation response', stackName };
          }
        }
        
        // ç¼“å­˜çŠ¶æ€åˆ°æ–‡ä»¶
        try {
          fs.writeFileSync(statusCacheFile, JSON.stringify(result, null, 2));
        } catch (cacheError) {
          console.warn('Failed to cache status:', cacheError);
        }
        
        resolve(result);
      });
    });

  } catch (error) {
    console.error('Error in checkStep1Status:', error);
    return { status: 'error', error: error.message };
  }
}

// æ£€æŸ¥ Step 2 çŠ¶æ€çš„å‡½æ•° - åŸºäº Kubernetes èµ„æº
async function checkStep2Status() {
  try {
    // ä½¿ç”¨å¤šé›†ç¾¤ç®¡ç†å™¨è·å–æ´»è·ƒé›†ç¾¤é…ç½®
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeCluster = clusterManager.getActiveCluster();
    
    if (!activeCluster) {
      return { status: 'unknown', error: 'No active cluster found' };
    }

    // ä»æ´»è·ƒé›†ç¾¤çš„é…ç½®ç›®å½•è¯»å–
    const configDir = clusterManager.getClusterConfigDir(activeCluster);
    const initEnvsPath = path.join(configDir, 'init_envs');
    
    if (!fs.existsSync(initEnvsPath)) {
      return { status: 'unknown', error: `init_envs not found for cluster: ${activeCluster}` };
    }
    
    const envContent = fs.readFileSync(initEnvsPath, 'utf8');
    const clusterTagMatch = envContent.match(/export CLUSTER_TAG=(.+)/);
    const clusterTag = clusterTagMatch ? clusterTagMatch[1].trim() : activeCluster;
    
    // æ£€æŸ¥æ´»è·ƒé›†ç¾¤çš„çŠ¶æ€ç¼“å­˜æ–‡ä»¶
    const metadataDir = clusterManager.getClusterMetadataDir(activeCluster);
    const statusCacheFile = path.join(metadataDir, 'step2_status_cache.json');
    let cachedStatus = null;
    
    if (fs.existsSync(statusCacheFile)) {
      try {
        cachedStatus = JSON.parse(fs.readFileSync(statusCacheFile, 'utf8'));
        // å¦‚æœé›†ç¾¤æ ‡ç­¾æ²¡æœ‰å˜åŒ–ä¸”çŠ¶æ€æ˜¯å®Œæˆï¼Œç›´æ¥è¿”å›ç¼“å­˜
        if (cachedStatus.clusterTag === clusterTag && cachedStatus.status === 'completed') {
          console.log(`Using cached Step 2 status for cluster: ${clusterTag}`);
          return cachedStatus;
        }
      } catch (error) {
        console.warn('Failed to read Step 2 status cache:', error);
      }
    }
    
    console.log(`Checking Step 2 (Kubernetes) status for cluster: ${clusterTag}`);
    
    const checks = [];
    
    // æ£€æŸ¥ 1: S3 CSI Node Pods (åœ¨ kube-system å‘½åç©ºé—´)
    const checkS3CSINodes = new Promise((resolve) => {
      exec('kubectl get pods -n kube-system -l app=s3-csi-node -o json', { timeout: 10000 }, (error, stdout) => {
        if (error) {
          resolve({ name: 's3-mount', status: 'missing', error: error.message });
        } else {
          try {
            const result = JSON.parse(stdout);
            const pods = result.items || [];
            
            if (pods.length === 0) {
              resolve({ name: 's3-mount', status: 'missing', message: 'No s3-csi-node pods found' });
            } else {
              const runningPods = pods.filter(pod => pod.status?.phase === 'Running');
              const readyPods = pods.filter(pod => {
                const conditions = pod.status?.conditions || [];
                return conditions.some(condition => 
                  condition.type === 'Ready' && condition.status === 'True'
                );
              });
              
              resolve({
                name: 's3-mount',
                status: readyPods.length === pods.length ? 'ready' : 'not_ready'
              });
            }
          } catch (parseError) {
            resolve({ name: 's3-mount', status: 'error', error: 'Failed to parse s3-csi-node pods data' });
          }
        }
      });
    });

    // æ£€æŸ¥ 2: HyperPod Training Operator Pod
    const checkHPOperator = new Promise((resolve) => {
      exec('kubectl get pods -A -l app.kubernetes.io/name=hp-training-operator -o json', { timeout: 10000 }, (error, stdout) => {
        if (error) {
          resolve({ name: 'training-op', status: 'missing', error: error.message });
        } else {
          try {
            const result = JSON.parse(stdout);
            const pods = result.items || [];
            
            if (pods.length === 0) {
              resolve({ name: 'training-op', status: 'missing' });
            } else {
              const runningPods = pods.filter(pod => pod.status?.phase === 'Running');
              resolve({
                name: 'training-op',
                status: runningPods.length > 0 ? 'ready' : 'not_ready'
              });
            }
          } catch (parseError) {
            resolve({ name: 'training-op', status: 'error', error: 'Failed to parse pods data' });
          }
        }
      });
    });

    // æ£€æŸ¥ 3: ç‰¹å®šçš„ controller manager pod
    const checkControllerManager = new Promise((resolve) => {
      exec('kubectl get pods -A -o name | grep -E "hp-training-controller-manager|training-operator"', { timeout: 10000 }, (error, stdout) => {
        if (error || !stdout.trim()) {
          resolve({ name: 'controller-manager', status: 'missing' });
        } else {
          const pods = stdout.trim().split('\n').filter(line => line.trim());
          resolve({ 
            name: 'controller-manager', 
            status: pods.length > 0 ? 'ready' : 'missing',
            pods: pods
          });
        }
      });
    });

    // ç­‰å¾…æ‰€æœ‰æ£€æŸ¥å®Œæˆ
    const results = await Promise.all([checkS3CSINodes, checkHPOperator, checkControllerManager]);
    
    // åˆ¤æ–­æ•´ä½“çŠ¶æ€
    const readyCount = results.filter(result => result.status === 'ready').length;
    const missingCount = results.filter(result => result.status === 'missing').length;
    const errorCount = results.filter(result => result.status === 'error').length;

    let overallStatus;
    if (readyCount === results.length) {
      overallStatus = 'completed';
    } else if (errorCount > 0) {
      overallStatus = 'error';
    } else if (missingCount === results.length) {
      overallStatus = 'not_started';
    } else {
      overallStatus = 'partial'; // éƒ¨åˆ†ç»„ä»¶å°±ç»ª
    }

    const result = {
      status: overallStatus,
      checks: results,
      summary: {
        total: results.length,
        ready: readyCount,
        missing: missingCount,
        error: errorCount
      },
      clusterTag: clusterTag,
      lastChecked: new Date().toISOString()
    };
    
    // ç¼“å­˜çŠ¶æ€åˆ°æ–‡ä»¶
    try {
      fs.writeFileSync(statusCacheFile, JSON.stringify(result, null, 2));
    } catch (cacheError) {
      console.warn('Failed to cache Step 2 status:', cacheError);
    }
    
    return result;

  } catch (error) {
    console.error('Error in checkStep2Status:', error);
    return { status: 'error', error: error.message };
  }
}

// æ‰§è¡Œé›†ç¾¤é…ç½®è„šæœ¬ (Step 2) - ä½¿ç”¨ nohup åå°æ‰§è¡Œ

// è·å– MLFlow æœåŠ¡å™¨ä¿¡æ¯ API - æ”¯æŒå¤šé›†ç¾¤
app.get('/api/cluster/mlflow-info', (req, res) => {
  try {
    // ä½¿ç”¨å¤šé›†ç¾¤ç®¡ç†å™¨è·å–æ´»è·ƒé›†ç¾¤çš„MLflowä¿¡æ¯
    const ClusterManager = require('./cluster-manager');
    const clusterManager = new ClusterManager();
    const activeCluster = clusterManager.getActiveCluster();
    
    if (!activeCluster) {
      return res.json({
        success: false,
        error: 'No active cluster found'
      });
    }

    // ä»æ´»è·ƒé›†ç¾¤çš„é…ç½®ç›®å½•è¯»å–MLflowä¿¡æ¯
    const configDir = clusterManager.getClusterConfigDir(activeCluster);
    const mlflowInfoPath = path.join(configDir, 'mlflow-server-info.json');
    
    if (fs.existsSync(mlflowInfoPath)) {
      const fileContent = fs.readFileSync(mlflowInfoPath, 'utf8').trim();
      
      // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
      if (!fileContent) {
        return res.json({
          success: true,
          data: {
            status: 'not_found',
            error: 'MLflow server info file is empty',
            clusterTag: activeCluster
          }
        });
      }
      
      let mlflowInfo;
      try {
        mlflowInfo = JSON.parse(fileContent);
      } catch (parseError) {
        return res.json({
          success: true,
          data: {
            status: 'error',
            error: 'Invalid JSON in MLflow server info file',
            clusterTag: activeCluster
          }
        });
      }
      
      // æ£€æŸ¥è§£æåçš„å¯¹è±¡æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
      if (!mlflowInfo || Object.keys(mlflowInfo).length === 0) {
        return res.json({
          success: true,
          data: {
            status: 'not_found',
            error: 'MLflow server info is empty',
            clusterTag: activeCluster
          }
        });
      }
      
      // è¿”å›å‰ç«¯æœŸæœ›çš„æ•°æ®ç»“æ„
      res.json({
        success: true,
        data: {
          status: 'found',
          trackingServerArn: mlflowInfo.TrackingServerArn,
          trackingServerName: mlflowInfo.TrackingServerName,
          trackingServerUrl: mlflowInfo.TrackingServerUrl,
          trackingServerStatus: mlflowInfo.TrackingServerStatus,
          isActive: mlflowInfo.IsActive,
          mlflowVersion: mlflowInfo.MlflowVersion,
          artifactStoreUri: mlflowInfo.ArtifactStoreUri,
          trackingServerSize: mlflowInfo.TrackingServerSize,
          roleArn: mlflowInfo.RoleArn,
          creationTime: mlflowInfo.CreationTime,
          clusterTag: activeCluster,
          rawData: mlflowInfo // ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡è°ƒè¯•
        }
      });
    } else {
      res.json({
        success: true,
        data: {
          status: 'not_found',
          error: `MLflow server info not found for cluster: ${activeCluster}`,
          clusterTag: activeCluster
        }
      });
    }
  } catch (error) {
    console.error('Error reading MLflow server info:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});

// æ¸…é™¤çŠ¶æ€ç¼“å­˜ API

// è·å– Step 1 çŠ¶æ€ API

// è·å–æ—¥å¿—å†…å®¹ API
// æ—§çš„æ—¥å¿—API - å·²è¢«å¤šé›†ç¾¤APIæ›¿ä»£
/*
app.get('/api/cluster/logs/:step', (req, res) => {
  try {
    const { step } = req.params;
    const { offset = 0 } = req.query;
    
    const logContent = logManager.getCurrentLogContent(step);
    const statusFile = path.join(logManager.metadataDir, `${step}_status.json`);
    
    let status = { status: 'idle' };
    if (fs.existsSync(statusFile)) {
      status = JSON.parse(fs.readFileSync(statusFile, 'utf8'));
    }

    // æ”¯æŒå¢é‡è¯»å–ï¼ˆä»æŒ‡å®šåç§»é‡å¼€å§‹ï¼‰
    const newContent = logContent.slice(parseInt(offset));
    
    res.json({
      success: true,
      data: {
        content: newContent,
        fullContent: logContent,
        totalLength: logContent.length,
        status: status.status,
        timestamp: status.timestamp,
        logId: status.logId
      }
    });

  } catch (error) {
    console.error('Error reading logs:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});
*/

// è·å–å†å²æ—¥å¿—åˆ—è¡¨
app.get('/api/cluster/logs-history', (req, res) => {
  try {
    const logFiles = fs.readdirSync(logManager.logsDir)
      .filter(file => file.endsWith('.log'))
      .map(file => {
        const filePath = path.join(logManager.logsDir, file);
        const stats = fs.statSync(filePath);
        const [timestamp, step] = file.replace('.log', '').split('_');
        
        return {
          filename: file,
          step: step,
          timestamp: timestamp.replace(/_/g, 'T').replace(/-/g, ':'),
          size: stats.size,
          created: stats.birthtime,
          modified: stats.mtime
        };
      })
      .sort((a, b) => new Date(b.created) - new Date(a.created));

    res.json({
      success: true,
      data: logFiles
    });

  } catch (error) {
    console.error('Error getting log history:', error);
    res.json({
      success: false,
      error: error.message
    });
  }
});

// è·å– CloudFormation å †æ ˆçŠ¶æ€ - ä» init_envs è‡ªåŠ¨è¯»å–å †æ ˆåç§°
// ==================== å¤šé›†ç¾¤ç®¡ç† API ====================
// å¼•å…¥å¤šé›†ç¾¤ç®¡ç†æ¨¡å—
const MultiClusterAPIs = require('./multi-cluster-apis');
const MultiClusterStatus = require('./multi-cluster-status');

const multiClusterAPIs = new MultiClusterAPIs();
const multiClusterStatus = new MultiClusterStatus();

// å¤šé›†ç¾¤ç®¡ç†API
app.get('/api/multi-cluster/list', (req, res) => multiClusterAPIs.handleGetClusters(req, res));
app.post('/api/multi-cluster/switch', (req, res) => multiClusterAPIs.handleSwitchCluster(req, res));
app.post('/api/multi-cluster/switch-kubectl', (req, res) => multiClusterAPIs.handleSwitchKubectlConfig(req, res));

// é‡å†™ç°æœ‰çš„é›†ç¾¤APIä»¥æ”¯æŒå¤šé›†ç¾¤
app.post('/api/cluster/save-config', (req, res) => multiClusterAPIs.handleSaveConfig(req, res));
app.post('/api/cluster/launch', (req, res) => multiClusterAPIs.handleLaunch(req, res));
app.post('/api/cluster/configure', (req, res) => multiClusterAPIs.handleConfigure(req, res));
app.get('/api/cluster/logs/:step', (req, res) => multiClusterAPIs.handleGetLogs(req, res));
app.get('/api/cluster/logs-history', (req, res) => multiClusterAPIs.handleGetLogsHistory(req, res));
app.post('/api/cluster/clear-status-cache', (req, res) => multiClusterAPIs.handleClearStatusCache(req, res));

// é‡å†™çŠ¶æ€æ£€æŸ¥APIä»¥æ”¯æŒå¤šé›†ç¾¤
app.get('/api/cluster/step1-status', (req, res) => multiClusterStatus.handleStep1Status(req, res));
app.get('/api/cluster/step2-status', (req, res) => multiClusterStatus.handleStep2Status(req, res));
app.get('/api/cluster/cloudformation-status', (req, res) => multiClusterStatus.handleCloudFormationStatus(req, res));

console.log('Multi-cluster management APIs loaded');

app.listen(PORT, () => {
  console.log('ğŸš€ ========================================');
  console.log('ğŸš€ HyperPod InstantStart Server Started');
  console.log('ğŸš€ ========================================');
  console.log(`ğŸ“¡ HTTP Server: http://localhost:${PORT}`);
  console.log(`ğŸ”Œ WebSocket Server: ws://localhost:${WS_PORT}`);
  console.log(`ğŸŒ Multi-cluster management: enabled`);
  console.log(`â° Server started at: ${new Date().toISOString()}`);
  console.log(`ğŸ–¥ï¸  Node.js version: ${process.version}`);
  console.log(`ğŸ’¾ Memory usage: ${Math.round(process.memoryUsage().heapUsed / 1024 / 1024)}MB`);
  console.log(`ğŸ”§ Environment: ${process.env.NODE_ENV || 'development'}`);
  console.log('ğŸš€ ========================================');
  console.log('âœ… Server is ready to accept connections');
});
