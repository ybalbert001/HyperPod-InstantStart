ttag=$(date +%Y%m%d_%H%M%S)
milstone=T-torch-recipe

tarname=${ttag}_model-deployment-ui-${milstone}.tar.gz
tar -czf $tarname model-deployment-ui/
aws s3 cp $tarname s3://pdx-cluster-mount/UI_Panel/
mv $tarname _Backups/

traintar=${ttag}_model-training-with-hyperpod-training-operator-${milstone}.tar.gz
tar -czf $traintar model-training-with-hyperpod-training-operator/
aws s3 cp $traintar s3://pdx-cluster-mount/UI_Panel/
mv $traintar _Backups/

tar -xvf "$(date +%Y%m%d_%H%M%S)_model-deployment-ui.tar.gz"


source ~/uvenv/py312/bin/activate &&  uv pip install hyperpod-elastic-agent

æˆ‘åœ¨æœ¬åœ°æœ‰ä¸€ä¸ªæ¨¡å‹éƒ¨ç½²çš„å·¥ç¨‹ï¼š/home/ubuntu/workspace/250805-instant-start-model-hosting-on-hyperpod/deploy-modelã€‚ç°åœ¨ï¼Œæˆ‘å¸Œæœ›æ„å»ºä¸€ä¸ªå‰ç«¯é¡µé¢ï¼Œä¸éœ€è¦å¤ªå¤æ‚ï¼Œä½†æ˜¯éœ€è¦æœ‰ä¸€äº›åŸºæœ¬åŠŸèƒ½ã€‚
1/ é¡µé¢ä¸Šéœ€è¦æœ‰ä¸€ä¸ªéƒ¨åˆ†ï¼Œè®©ç”¨æˆ·å¡«å†™ä¸€äº›å­—æ®µï¼Œä¸”è¿™äº›å­—æ®µä¼šé…ç½®åˆ°yamlä¸­ï¼Œç”¨æˆ·åœ¨eksä¸Šéƒ¨ç½²ï¼Œå­—æ®µåŒ…å«ï¼ˆæ¯”å¦‚qwen-vllm.yamlï¼‰ï¼š
    - replicasä¸ªæ•°
    - model-tagï¼Œç”¨æˆ·æ›¿æ¢è·Ÿqwenç›¸å…³çš„æ‰€æœ‰å‘½å
    - HUGGING_FACE_HUB_TOKENï¼Œå¯ä»¥ä¸å†™
    - model_id_or_pathï¼Œç”¨äºæ›¿æ¢ "Qwen/Qwen3-0.6B" # æˆ–ä½¿ç”¨S3æŒ‚è½½è·¯å¾„ /models/Qwen3-0.6B
    - gpu per modelï¼Œç”¨äºå¡«å…… nvidia.com/gpu: 2

2/ ç®€å•åœ°å±•ç¤ºå½“å‰eksé›†ç¾¤çš„æœºå™¨ï¼ŒåŠæ¯ä¸ªæœºå™¨ç©ºä½™çš„å¡æ•°ã€‚æ¯”å¦‚ inst-1: 1/4; inst-2: 0/4ã€‚
3/ å½“ç”¨æˆ·å®Œæˆè¾“å…¥åï¼Œç‚¹å‡»æäº¤ï¼Œåˆ™æ‰§è¡Œkubectl apply -f xxx.yaml
4/ é¡µé¢çš„æŸä¸ªåŒºåŸŸæŒç»­åˆ·æ–°å±•ç¤ºkubectl get podsçš„ç»“æœ
5/ é¡µé¢çš„æŸä¸ªåŒºåŸŸï¼Œå±•ç¤ºkubectl get serviceçš„ç»“æœ
6/ 
è¯·åšä¸€ä¸ªè§„åˆ’ï¼ŒåŒ…æ‹¬é¡µé¢å¦‚ä½•è®¾è®¡ï¼Œæ¯”å¦‚å·¦ä¸Šéƒ¨åˆ†å±•ç¤ºé…ç½®åŠæäº¤æŒ‰é’®ï¼›å·¦ä¸‹ä¸ºé›†ç¾¤çŠ¶æ€ï¼Œpod/serviceï¼›å³ä¸Šæ˜¯ç”¨æˆ·è¾“å…¥çš„curlè¯·æ±‚ï¼Œå³ä¸‹æ˜¯æ¨¡å‹urlè¿”å›çš„ç»“æœï¼ŒåŒ…æ‹¬å±•ç¤ºæµå¼ã€‚
ä»¥åŠå¦‚ä½•æ­å»ºå‰ç«¯é¡µé¢ï¼Ÿæœ‰çœ‹ä¸Šå»æ¯”è¾ƒä¸“ä¸šï¼Œæˆç†Ÿçš„å·¥å…·å—ï¼Ÿ



1. Cluster Statusæ²¡æœ‰è‡ªåŠ¨åˆ·æ–°ï¼Œæˆ–è€…åˆ·æ–°æ—¶é—´æ…¢ï¼Œè¯·ç¡®è®¤ä¸‹
2. Model Configurationçš„ollamaæ¨¡æ¿ä¸­ï¼Œ
    - Ollama Model IDä¸è¦ç”¨å›ºå®šçš„é€‰é¡¹ï¼Œè®©ç”¨æˆ·è‡ªç”±å¡«å†™
    - Ollama Model IDçš„å¸®åŠ©æŒ‰é’®ä¸­çš„è¯´æ˜æ–‡å­—ï¼Œå¢åŠ link - https://ollama.com/search
    - modeltagå»æ‰å§ï¼Œç›´æ¥ç”¨model Idï¼Œä½†æ˜¯æ›¿æ¢æ‰æ¯”å¦‚:ç­‰å­—ç¬¦å³å¯
3. åˆšæ‰æˆ‘é€šè¿‡UIåˆ é™¤äº†ä½ éƒ¨ç½²çš„test deploymentï¼Œä½†æ˜¯podä»ç„¶å­˜åœ¨ï¼Œè¯·ç¡®è®¤ä¸‹



1. Model Configurationä¸­ï¼Œvllmçš„éƒ¨ç½²éƒ¨åˆ†ï¼Œå¦‚æœhuggingfaceä¸ºç©ºï¼Œé‚£ä¹ˆtemplateä¸­ä¸è¦æ·»åŠ ï¼š
- name: HUGGING_FACE_HUB_TOKEN
    value: "TOKEN"  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…token
2. Model Testing éƒ¨åˆ†ï¼Œå½“åˆ‡æ¢ollamaæ¨¡å‹æ—¶ï¼Œ
    - apiç”¨/api/generateæ²¡é—®é¢˜
    - paylaodéœ€è¦ç”¨å¯¹åº”çš„æ¨¡å‹æ¥å¡«å……ï¼Œæ¯”å¦‚"model": "gpt-oss:20b"ï¼Œ æˆ–è€…"model": "gpt-oss:120b", æˆ–è€…å…¶ä»–modelid
3. Model Testing éƒ¨åˆ†ï¼Œå½“åˆ‡æ¢vllmæ¨¡å‹æ—¶ï¼Œ
    - apiéœ€è¦ç”¨/v1/chat/completions
    - åŒæ—¶payloadéœ€è¦ç”¨/v1/chat/completionså¯¹åº”çš„æ ¼å¼ç»„ç»‡
å¯ä»¥å—ï¼Ÿ


è¯·å‚è€ƒï¼š/home/ubuntu/workspace/model-deployment-ui/PROJECT_INTRO.mdï¼Œäº†è§£ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®ã€‚
ç°åœ¨æœ‰ä¸€äº›å°é—®é¢˜ï¼Œè¯·å¸®æˆ‘åšä¸€äº›è°ƒæ•´è·Ÿä¿®æ”¹ï¼š
1. å½“ç‚¹å‡»æ¨¡å‹éƒ¨ç½²ä¹‹åï¼Œä¼šç”Ÿæˆä¸€ä¸ªyamlæ–‡ä»¶ï¼Œè¯·å¸®æˆ‘å°†è¿™ä¸ªæ–‡ä»¶çš„ä¿å­˜è·¯å¾„æ”¾åœ¨é¡¹ç›®ä¸­ï¼ˆåŒæ—¶ä¹Ÿéœ€è¦åœ¨gitignoreä¸­æ·»åŠ ï¼‰ï¼Œæ–¹ä¾¿è°ƒè¯•ï¼›
2. gitignoreæ•´ä½“éœ€è¦åšä¸ªç®€åŒ–ï¼Œå»æ‰è·Ÿæœ¬é¡¹ç›®æ— å…³çš„å†…å®¹ï¼›
3. Model Testing éƒ¨åˆ†ï¼Œä¹Ÿéœ€è¦å¢åŠ ä¸€ä¸ªåˆ·æ–°æŒ‰é’®ï¼›è·Ÿå…¶ä»–çš„è‡ªåŠ¨åˆ·æ–°æœºåˆ¶ä¸€æ ·ï¼Œå…¨å±€é…ç½®ä¸€ä¸ª30secçš„è‡ªåŠ¨åˆ·æ–°æ—¶é—´ï¼›
å¯ä»¥å—ï¼Ÿ



ç°åœ¨æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå½“æˆ‘éƒ¨ç½²æ¨¡å‹ä¹‹åï¼ˆæ— è®ºæ˜¯ollamaè¿˜æ˜¯vllmï¼‰ï¼Œåœ¨payloadçš„éƒ¨åˆ†ï¼Œå…¶ä¸­çš„"model"å­—æ®µï¼Œéœ€è¦è·Ÿéƒ¨ç½²æ—¶çš„modelidå®Œå…¨ä¸€è‡´ã€‚ç°åœ¨vllmæ˜¯è¿™æ ·ï¼Œä½†æ˜¯ollamaéƒ¨ç½²åï¼Œpayloadä¸­çš„modelå­—æ®µå¦‚ä¸‹ï¼š{
  "model": "gpt-oss-20b",
  "prompt": "Hello, how are you today?",
  "stream": false
}ã€‚è¯·å¸®æˆ‘æŸ¥çœ‹æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Œå¹¶åšä¸ªä¿®æ”¹ã€‚



è¯·å‚è€ƒï¼š/home/ubuntu/workspace/model-deployment-ui/PROJECT_INTRO.mdï¼Œäº†è§£ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®ã€‚


è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¹¶å°†ä½ è®¤ä¸ºé‡è¦çš„ä¿¡æ¯æ•´ç†åˆ°PROJECT_INTRO.mdï¼Œæ–¹ä¾¿ä½ åç»­ç†è§£ã€‚



### æ•´ä½“å¸ƒå±€ç»“æ„
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¸»æ ‡ç­¾åˆ‡æ¢åŒºåŸŸ                            â”‚
â”‚  [Inference] [Training]                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€ Inference Panel (å½“å‰æ¿€æ´») â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Model Configuration â”‚     Model Testing          â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Replicas      â”‚ â”‚  â”‚ â€¢ Service Selection     â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Model Tag     â”‚ â”‚  â”‚ â€¢ JSON Payload         â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ HF Token      â”‚ â”‚  â”‚ â€¢ cURL Generation      â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ VLLM Args     â”‚ â”‚  â”‚ â€¢ Test Results         â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Deploy Button â”‚ â”‚  â”‚                         â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€ Training Panel (éšè—çŠ¶æ€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Training Config    â”‚     Training Monitor        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ å…ˆç•™ç©º         â”‚ â”‚  â”‚ â€¢ å…ˆç•™ç©º                     â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    åº•éƒ¨ç›‘æ§åŒºåŸŸ                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Cluster Status    â”‚      Status Monitor             â”‚  â”‚
â”‚  â”‚   â€¢ GPU Usage       â”‚      â€¢ Pods Status              â”‚  â”‚
â”‚  â”‚   â€¢ Node Info       â”‚      â€¢ Services Status          â”‚  â”‚
â”‚  â”‚   â€¢ Resources       â”‚      â€¢ Real-time Updates        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚


è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¹¶å°†ä½ è®¤ä¸ºé‡è¦çš„ä¿¡æ¯æ•´ç†åˆ°PROJECT_INTRO.mdï¼Œæ–¹ä¾¿ä½ åç»­ç†è§£ã€‚

ç°åœ¨ï¼Œæˆ‘éœ€è¦åœ¨Inferenceï¼ŒTrainingè¿™ä¸¤ä¸ªtabé¡µä¹‹å¤–ï¼Œå†å¢åŠ ä¸€ä¸ªtabé¡µï¼Œåå­—å«Model Managementï¼Œå¸ƒå±€è·Ÿinferenceï¼Œtrainingç±»ä¼¼ï¼Œåˆ†ä¸¤å¤§å—ã€‚
1. å·¦è¾¹ä¸€å—æ˜¯æ¨¡å‹ä¸‹è½½ï¼Œç”¨æˆ·éœ€è¦å¡«å†™ï¼š
    - huggingfaceæ¨¡å‹IDï¼Œå¹¶ä¸”æ›¿æ¢/home/ubuntu/workspace/model-deployment-ui/templates/hf-download-template.yamlä¸­çš„HF_MODEL_IDï¼›
    - huggingface token ï¼ˆè·Ÿvllmä¸­çš„ä¸€æ ·æŠ˜å èµ·æ¥ï¼ŒåŒæ—¶å¦‚æœç”¨æˆ·å¡«å†™ï¼Œåˆ™æ„å»ºhf-download-templateä¸­çš„envï¼‰
2. å³è¾¹å±•ç¤ºå½“å‰storage class/pv/pvcé‡Œå£°æ˜çš„s3æ¡¶ä¸­çš„ä¿¡æ¯ï¼Œä½ å¯ä»¥é€šè¿‡æ¯”å¦‚aws S3 ls s3://æ¡¶åï¼Œæ¥è·å–ä¿¡æ¯



è¯·å¸®æˆ‘æ•´ä½“reviewä¸€ä¸‹è¿™ä¸ªé¡¹ç›®ã€‚çœ‹çœ‹æ˜¯å¦æœ‰ä¸åˆç†çš„åœ°æ–¹ï¼Œå…ˆå°½é‡ç½—åˆ—å‡ºæ¥ï¼Œä¸è¦æ”¹åŠ¨ä»»ä½•åœ°æ–¹ã€‚

> â€¢ å¤§é‡åŒæ­¥çš„kubectlè°ƒç”¨
â€¢ WebSocketå¹¿æ’­æ²¡æœ‰èŠ‚æµ
â€¢ å‰ç«¯ç»„ä»¶é‡å¤æ¸²æŸ“
â€¢ å¤šä¸ªç»„ä»¶ä¸­é‡å¤çš„APIè°ƒç”¨é€»è¾‘
â€¢ ç›¸ä¼¼çš„é”™è¯¯å¤„ç†ä»£ç 
è¿™äº›åœ°æ–¹æˆ‘è§‰å¾—æ¯”è¾ƒé‡è¦ï¼Œèƒ½å…·ä½“ä¸€äº›å—ï¼Ÿåˆ—å‡ºé—®é¢˜ä»¥åŠä¿®æ”¹æ–¹æ¡ˆã€‚


kubectl get node hyperpod-i-025cd4f55f0d613b9 -o jsonpath='{.status.allocatable.nvidia\.com/gpu}'
kubectl get node hyperpod-i-025cd4f55f0d613b9 -o jsonpath='{.status.capacity.nvidia\.com/gpu}'


è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

ç°åœ¨æˆ‘éœ€è¦è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼š
1. Inferenceæ ‡ç­¾é¡µä¸­ï¼Œåœ¨Advanced Settingséœ€è¦å¢åŠ ä¸€ä¸ªdocker imageçš„å¡«å†™é€‰é¡¹ï¼Œé»˜è®¤å€¼ä¸ºï¼švllm/vllm-openai:latestï¼Œæ›¿æ¢vllm-template.yamlä¸­çš„
image: vllm/vllm-openai:latest

åœ¨Trainingæ ‡ç­¾é¡µçš„Training Configurationä¸­éœ€è¦å¢åŠ å¦‚ä¸‹åŠŸèƒ½ï¼š
1. ä½ å¯ä»¥å‚è€ƒ/home/ubuntu/workspace/model-deployment-ui/templates/smhp-training-operator-launch.yamlï¼Œå¹¶æ–°å»ºä¸€ä¸ªåˆå§‹æ¨¡æ¿ï¼›
2. éœ€è¦æä¾›ä¸€ä¸ªè¾“å…¥æ¡†ï¼Œå¡«å†™è®­ç»ƒä»»åŠ¡åç§°ï¼Œæ¥æ›¿æ¢yamlä¸­çš„name: gptnx-1b-fsdp-t1ï¼Œåˆå§‹é»˜è®¤å€¼ä¸ºhyperpodpytorchjob-1ï¼›
3. ç¬¬äºŒä¸ªè¾“å…¥æ¡†æ˜¯docker imageï¼Œé»˜è®¤å€¼ä¸º633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch24-ec2:latestï¼Œç”¨äºæ›¿æ¢æ¨¡æ¿æ–‡ä»¶ä¸­çš„imageå­—æ®µå€¼ï¼›
4. éœ€è¦æä¾›ä¸¤ä¸ªè¾“å…¥æ¡†ï¼Œåˆ†åˆ«æ›¿æ¢æ¨¡æ¿ä¸­çš„ï¼šnprocPerNode: "1"ï¼ˆæ³¨æ„ä¸ç®¡è¾“å…¥å¦‚ä½•ï¼Œè¿™ä¸ªå­—æ®µéœ€è¦æ˜¯å­—ç¬¦ä¸²çš„å½¢å¼ï¼‰ï¼Œä»¥åŠreplicasï¼Œé»˜è®¤å€¼éƒ½æ˜¯1ï¼›
5. éœ€è¦å¡«å†™gpuï¼Œefaçš„æ•°é‡ï¼Œç”¨äºæ›¿æ¢æ¨¡æ¿ä¸­çš„requests:
                  nvidia.com/gpu: 1
                  vpc.amazonaws.com/efa: 1
                limits:
                  nvidia.com/gpu: 1
                  vpc.amazonaws.com/efa: 1
ï¼Œgpuçš„é»˜è®¤å€¼ä¸º8ï¼Œefaçš„é»˜è®¤å€¼ä¸º16ï¼›
6. éœ€è¦æä¾›å…¥å£è„šæœ¬çš„è·¯å¾„ï¼Œæ›¿æ¢command: ["bash", "/s3/training_code/model-training-with-hyperpod-training-operator/fsdp-entry-smhp-op.sh"]ä¸­çš„.shéƒ¨åˆ†ï¼›
7. éœ€è¦æä¾›HyperPod training Operator logMonitoringConfigurationï¼Œ å¦‚æœå¡«å†™äº†ï¼Œå¯ä»¥æ”¾åœ¨yamlä¸­æœ€åæ³¨é‡Šçš„éƒ¨åˆ†ï¼š
    # logMonitoringConfiguration: 
    #   - name: "JobStart"
    #     logPattern: ".*Experiment configuration.*"
    #     expectedStartCutOffInSeconds: 120
    #   - name: "HighLossDetection"
    #     logPattern: ".*\\[train\\.py:\\d+\\] Batch \\d+ Loss: (\\d+\\.\\d+).*"
    #     metricThreshold: 1
    #     operator: "lteq"
    #     metricEvaluationDataPoints: 100
å†…å®¹æœ‰ç‚¹å¤šï¼Œæ˜ç™½äº†å—ï¼Ÿ




ç°åœ¨ï¼Œæˆ‘éœ€è¦åœ¨Training Monitorçš„éƒ¨åˆ†è¿›è¡Œä¸€äº›å¼€å‘ã€‚
1. ç¬¬ä¸€éƒ¨åˆ†ï¼šç›®å‰ï¼Œæˆ‘å¯ä»¥é€šè¿‡ï¼škubectl get hyperpodpytorchjob
NAME                   AGE
hyperpodpytorchjob-1   61s
çš„æ–¹å¼æ¥è·å–ä»»åŠ¡ï¼›åŒæ—¶æˆ‘ä¼šé€šè¿‡kubectl delete hyperpodpytorchjob hyperpodpytorchjob-1
hyperpodpytorchjob.sagemaker.amazonaws.com "hyperpodpytorchjob-1" deletedè¿™ä¸ªæ–¹å¼æ¥deleteã€‚å› æ­¤ï¼Œåœ¨uiä¸Šï¼Œæˆ‘å¸Œæœ›æœ‰ä¸€ä¸ªä¸‹æ‹‰æ¡†æ¥é€‰æ‹©å½“å‰é›†ç¾¤çš„hyperpodpytorchjobã€‚
2. ç¬¬äºŒéƒ¨åˆ†ï¼šå½“trainingå¯åŠ¨åï¼Œæˆ‘é€šå¸¸æ˜¯é€šè¿‡kubectl get podsæ¥çœ‹åˆ°trainingç›¸å…³çš„podsï¼Œå¹¶é€šè¿‡kubectl logs -f podname æ¥æŸ¥çœ‹æ—¥å¿—ã€‚å› æ­¤UIçš„ç¬¬äºŒéƒ¨åˆ†æˆ‘å¸Œæœ›èƒ½çœ‹åˆ°æŸä¸€ä¸ªhyperpodpytorchjobçš„å…³è”podsçš„æ—¥å¿—ï¼ˆä¸åŒpodçš„æ—¥å¿—å¯ä»¥ç”¨ä¸åŒçš„é¢œè‰²ï¼‰ã€‚
å½“æˆ‘ä»ç¬¬ä¸€éƒ¨åˆ†é€‰æ‹©äº†æŸä¸ªjobåï¼Œè¿™éƒ¨åˆ†å°±æ˜¾ç¤ºå‡ºè¿™ä¸ªjob ä¸­podsçš„æ—¥å¿—ã€‚
ä½ æ˜ç™½æˆ‘çš„éœ€æ±‚äº†å—ï¼Ÿ




è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

ç°åœ¨ï¼Œåœ¨Trainingæ ‡ç­¾é¡µä¸­ï¼Œéœ€è¦åšä¸€äº›è°ƒæ•´ï¼š
Training Configurationè·ŸTraining Job Monitorï¼Œå¸ƒå±€ä¸Šåº”è¯¥æ˜¯æ”¾åœ¨åŒä¸€è¡Œä¸­ï¼Œå°±è·Ÿinferenceä¸­çš„ä¸€æ ·ã€‚
ä½ æ˜ç™½äº†å—ï¼Ÿ

ç°åœ¨ï¼ŒSelect Training Jobçš„éƒ¨åˆ†ï¼Œæœ‰ä¸€ä¸ªDelete Jobçš„æŒ‰é’®ã€‚è¿™ä¸ªæˆ‘è§‰å¾—ï¼Œæ˜¯å¦å¯ä»¥æ”¾åˆ°å³ä¸‹æ–¹ï¼ŒApp Statusçš„éƒ¨åˆ†ã€‚ä½ å¯ä»¥åœ¨App Statusä¸­æ–°å»ºä¸€ä¸ªtab é¡µï¼Œåç§°æ˜¯HyperPod PytorchJobï¼Œç„¶åè·ŸDeploymentsä¸€æ ·ï¼Œå¯ä»¥ç”¨åˆ é™¤æŒ‰é’®åˆ é™¤ã€‚


å¾ˆå¥½ï¼Œç°åœ¨åªæœ‰ä¸€äº›å°çš„æ˜¾ç¤ºé—®é¢˜éœ€è¦ä¿®æ”¹ï¼š
1. App Statusçš„HyperPod PytorchJobï¼ŒStatusæ˜¯Runningæ—¶å€™ï¼ŒRunningçš„æ ‡ç­¾æ˜¯æœ‰ä¸ªåŠ¨æ€æ•ˆæœï¼Œæ˜¯å¦å¯ä»¥ä¸è¦è¿™ä¸ªåŠ¨æ€æ•ˆæœï¼Œæ ‡ç­¾çš„å½¢å¼/é¢œè‰²éƒ½æ²¡æœ‰é—®é¢˜ï¼›
2. App Statusçš„HyperPod PytorchJobï¼Œdeleteçš„é£æ ¼è·Ÿdeploymentsä¸­çš„deleteé£æ ¼ä¿æŒä¸€è‡´ï¼Œå®å¿ƒçº¢è‰²ï¼›
3. App Statusçš„Deploymentså‰é¢æ²¡æœ‰ä¸€ä¸ªå›¾æ ‡ï¼Œè€Œå…¶ä»–å‡ ä¸ªéƒ½æœ‰ï¼›

å¦å¤–ï¼ŒTraining Job Monitorä¸­ï¼Œå…³äºpodsæ—¥å¿—çš„start streamingæŒ‰é’®ï¼Œå¦‚æœæŒ‰é’®æœªç‚¹å‡»çš„æ—¶å€™ï¼Œå¯ä»¥æ˜¯ç™½è‰²ï¼Œç‚¹å‡»äº†ä¹‹åå†æ˜¾ç¤ºé¢œè‰²ã€‚å¯ä»¥å—ï¼Ÿ


æˆ‘éœ€è¦ä¿ç•™å®Œæ•´çš„æ—¥å¿—ï¼Œç”¨äºè°ƒè¯•ã€‚æˆ‘å¸Œæœ›çš„æ˜¯ï¼Œå‰ç«¯ä»ä¿æŒç°æœ‰çš„æ•ˆæœï¼Œä½†æ˜¯ä¸ºäº†é˜²æ­¢å†…å­˜é£é™©ï¼Œé™åˆ¶ä¸€ä¸‹æ¯ä¸ªpodæ—¥å¿—æ˜¾ç¤ºçš„æ•°é‡ã€‚åŒæ—¶ï¼Œæ‰€æœ‰æ—¥å¿—å­˜å‚¨äºåç«¯ã€‚æ¯”å¦‚åœ¨é¡¹ç›®è·¯å¾„/logs/hyperpodpytorchjob/hyperpodpytorchjob-1/podname.log... ã€‚ 
è¿™æ ·å¯è¡Œå—ï¼Ÿ


è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-ui/PROJECT_ANALYSIS.mdã€‚å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

ç›®å‰ï¼Œé¡¹ç›®ä¸­å¯èƒ½å­˜åœ¨ä¸€äº›bugï¼Œè¯·å¸®æˆ‘æ£€æŸ¥ä¸€ä¸‹ã€‚App Statusä¸­ï¼Œç‚¹å‡»refresh allæŒ‰é’®ï¼Œçœ‹ä¸Šå»å¹¶éæ‰€æœ‰ç»„ä»¶éƒ½è¢«åˆ·æ–°äº†ï¼Œè¯·æ£€æŸ¥ä¸€ä¸‹æ˜¯æ‰€æœ‰App Statuséƒ½ä¼šè¢«åˆ·æ–°å—ï¼Ÿ
å¦å¤–ï¼ŒApp Statusæ˜¯å¦ä¼šè‡ªåŠ¨åˆ·æ–°ï¼Œé—´éš”æ—¶é—´æ˜¯å¤šä¹…ï¼Ÿ


å¦å¤–å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨trainingæ ‡ç­¾é¡µä¸­ï¼Œå½“æˆ‘ç‚¹å‡»äº†Deploy Training JobæŒ‰é’®ä¹‹åï¼Œæˆ‘å‘ç°é¡µé¢å³ä¸Šè§’çš„Statusï¼Œä¼šä»ç»¿è‰²å˜æˆâ€œStatus: ğŸŸ  Offline (Refresh to reconnect)â€ï¼Œç„¶ååˆ·æ–°ç½‘é¡µå°±å˜å¥½äº†ã€‚è¿™æ˜¯ä»€ä¹ˆåŸå› ã€‚


curl -X GET http://k8s-default-vllms3qw-39d72983ea-fc2ff427ee8d9187.elb.us-west-2.amazonaws.com:8000/v1/models


å¥½çš„ï¼Œç°åœ¨è¯·å°†ä½ ä¸€äº›é‡è¦çš„è®¾è®¡æœºåˆ¶ï¼Œæ¯”å¦‚å…¨å±€åˆ·æ–°ï¼Œç»Ÿä¸€websocketç­‰ä¿¡æ¯ï¼Œæ•´ç†åæ›´æ–°åˆ°/home/ubuntu/workspace/model-deployment-ui/PROJECT_ANALYSIS.mdçš„åˆé€‚éƒ¨åˆ†ã€‚




è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-ui/PROJECT_ANALYSIS.mdï¼Œç°åœ¨æœ‰ä¸ªé—®é¢˜ï¼Œåœ¨å³ä¸‹æ–¹HyperPod PytorchJobä¸­ï¼Œç‚¹å‡»åˆ é™¤ä»»åŠ¡ï¼Œä¼šæœ‰å¤šæ¬¡xxx delete successfullyã€‚è¯·çœ‹ä¸€ä¸‹æ˜¯ä»€ä¹ˆåŸå› 

ç°åœ¨ï¼Œæˆ‘å¸Œæœ›åœ¨training tabé¡µçš„å³è¾¹ï¼Œå†å¢åŠ ä¸€ä¸ªæ–°çš„tabé¡µï¼Œåç§°ä¸ºtraining Historyã€‚è¿™ä¸ªé¡µé¢ä¸»è¦å±•ç¤ºå†å²çš„è®­ç»ƒæƒ…å†µã€‚ä¿¡æ¯æ¥è‡ªæ‰˜ç®¡mlflowçš„tracking serverã€‚å…·ä½“metricçš„è·å–ä½ å¯ä»¥å‚è€ƒï¼š/home/ubuntu/workspace/model-deployment-ui/mlflow/get_experiment_metrics.PytorchJobï¼Œç„¶åè·ŸDeploymentsä¸€æ ·ï¼Œå¯ä»¥ç”¨åˆ é™¤æŒ‰é’®åˆ é™¤ã€‚
è¿™ä¸ªpythonä¾èµ–äºpip install mlflow==3.0.0 sagemaker-mlflow==0.1.0 pandasã€‚è¯·è€ƒè™‘ä¸€ä¸‹å¦‚ä½•å®ç°ã€‚


source ~/uvenv/py312/bin/activate && uv pip install mlflow==3.0.0 sagemaker-mlflow==0.1.0 pandas

åœ¨Training Historyè¿™é‡Œï¼Œç›®å‰æ˜¾ç¤ºçš„è¡¨æ ¼ï¼Œä¿¡æ¯æ¥è‡ªäºï¼š/home/ubuntu/workspace/model-deployment-ui/mlflow/get_training_history.pyã€‚ç°åœ¨ï¼Œæ¯ä¸ªmlflow runä¸Šæœ‰tagï¼Œæˆ‘å¸Œæœ›å°†è¿™äº›tagä¿¡æ¯ä½œä¸ºæ–°çš„columnæ˜¾ç¤ºåœ¨è¡¨æ ¼ä¸Šã€‚åŒæ—¶ï¼Œå½“å‰çš„è¡¨æ ¼ä¸­ï¼Œå¯ä»¥å°†Epochå»æ‰ã€‚


 /home/ubuntu/workspace/hyperpod_elastic_agent-1.0.0 ç°åœ¨è¯·å‚è€ƒè¿™ä¸ªä»£ç ï¼Œè¿™æ˜¯æ–‡æ¡£ä¸Šè¯´çš„å¯åŠ¨æ–¹å¼ï¼šhyperpodrun \
    --nnodes=${NNODES} --nproc-per-node=${NPROC_PER_NODE} \
    --server-host=0.0.0.0 --server-port=8080 \
    --tee=3 --log_dir=/tmp/hyperpod \
    --post-train-script="nohup python $LLAMA_FACTORY_PRJ/set_mlflow_tags.py > /tmp/hyperpod/mlflow_tags.log 2>&1 & exit 0" \
    $LLAMA_FACTORY_LAUNCHER \
        $LLAMA_FACTORY_PRJ/qwen06b_full_sft.yamlã€‚ä½†æ˜¯æˆ‘å‘ç°ï¼Œå½“æˆ‘ä½¿ç”¨äº†--post-train-scriptè¿™ä¸ªå‚æ•°åï¼Œä»»åŠ¡ä¼šåå¤é‡å¯ï¼Œä¸ç¬¦åˆé¢„æœŸã€‚è¯·çœ‹çœ‹æ˜¯å¦èƒ½
æ‰¾åˆ°ä»€ä¹ˆçº¿ç´¢ã€‚





è¯·äº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

vllm/vllm-openai:latest
lmsysorg/sglang:latest
vllm/vllm-openai:gptoss

python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000

å…³äºtraining historyçš„å±•ç¤ºï¼Œæˆ‘å‘ç°æ¯æ¬¡ç‚¹å‡»æ ‡ç­¾è¿›å…¥åï¼Œéƒ½è¦å®Œå…¨ä»mlflow serverè·å–ï¼Œèƒ½å¦æ”¹æˆç‚¹å‡»åˆ·æ–°æ—¶æ‰è·å–ï¼Ÿ

ç°åœ¨ï¼Œå…³äºæ¨¡å‹éƒ¨ç½²éƒ¨åˆ†ã€‚æˆ‘å¸Œæœ›å°†Docker Imageçš„è¾“å…¥éƒ¨åˆ†åšä¸€äº›ä¿®æ”¹ã€‚é¦–å…ˆï¼Œå°†å…¶ä»advanced settingä¸­æ‹¿å‡ºã€‚
å¦å¤–ï¼ŒDocker Imageæ˜¯å¦å¯ä»¥æ”¹æˆæŸç§ç»„ä»¶ï¼Œæ—¢å¯ä»¥é€‰æ‹©å‡ ä¸ªå›ºå®šé€‰é¡¹ï¼švllm/vllm-openai:latest
lmsysorg/sglang:latest
vllm/vllm-openai:gptossï¼Œåˆå¯ä»¥å…è®¸è‡ªå·±è¾“å…¥ï¼Ÿ
å¦å¤–ï¼Œå¯¹äºç°åœ¨vllm entry pointçš„æ ¡éªŒï¼Œå¯ä»¥å¢åŠ ä¸€ä¸ªpython3 -m sglang.launch_server


å¦å¤–ï¼ŒVLLM/SGLang Commandçš„é¢„å¡«å……éƒ¨åˆ†ï¼Œéœ€è¦åšä¸ªè°ƒæ•´ã€‚å½“é€‰æ‹©äº†vllmçš„ï¼ŒæŒ‰ç…§å½“å‰çš„å¡«å……ï¼Œå½“é€‰æ‹©äº†sglangçš„å®¹å™¨ï¼Œåˆ™ä½¿ç”¨ï¼špython3 -m sglang.launch_server \
--model-path Qwen/Qwen3-0.6B \
--tp-size 1 \
--host 0.0.0.0 \
--port 8000 \
--trust-remote-codeï¼›å½“å®¹å™¨æ—¶vllmçš„gptossæ—¶ï¼Œåˆ™ä½¿ç”¨:python3 -m vllm.entrypoints.openai.api_server \
--model openai/gpt-oss:120b \
--tensor-parallel-size 2 \
--host 0.0.0.0 \
--port 8000 \
--trust-remote-code


/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.md è¯·åŸºäºè¿™ä¸ªè¯´æ˜ï¼Œå¿«é€Ÿäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ã€‚åªéœ€è¦ç†è§£ï¼Œæš‚æ—¶æ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

ç°åœ¨æˆ‘çš„å›¢é˜Ÿæœ‰ä¸¤ä¸ªä¸»è¦çš„éœ€æ±‚ï¼š
1. å›¢é˜Ÿçš„äº§å“ç»ç†ï¼Œä»–ä»¬éœ€è¦åªè¯»ç”±æˆ‘ç®¡ç†çš„mlflow tracking serverï¼Œä»–ä»¬ç”šè‡³å¾€å¾€æ²¡æœ‰eksé›†ç¾¤ï¼Œåªæœ‰åŸºæœ¬çš„pythonç¯å¢ƒï¼› 
2. å›¢é˜Ÿçš„MLå·¥ç¨‹å¸ˆï¼Œä»–ä»¬éœ€è¦åœ¨ä»–ä»¬è´¦å·çš„EKSé›†ç¾¤è¿›è¡Œè®­ç»ƒä»»åŠ¡ï¼Œå¹¶å°†å®éªŒè®°å½•ä¸Šä¼ åˆ°æˆ‘ç®¡ç†çš„mlflow tracking serverï¼›
é‚£ä¹ˆï¼Œå¯¹äº1æ¥è¯´ï¼Œæ˜¯å¦æœ‰æ›´ç®€å•çš„æ–¹å¼ï¼Ÿå¯¹äº2æ¥è¯´ï¼Œç›®å‰çš„é‰´æƒæ–¹å¼æ˜¯å¦å¯ä»¥å†åŒ–ç®€ï¼Ÿï¼ˆå¦‚æœä¸èƒ½åŒ–ç®€ä¹Ÿæ²¡å…³ç³»ï¼‰








è¯·åŸºäº/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.mdï¼Œäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ï¼š/home/ubuntu/workspace/model-deployment-uiã€‚
å¦‚æœä½ å®Œå…¨ç†è§£äº†ï¼Œè¯·å›ç­”ç†è§£ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚


/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.md è¯·åŸºäºè¿™ä¸ªè¯´æ˜ï¼Œå¿«é€Ÿäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ã€‚åªéœ€è¦ç†è§£ï¼Œæš‚æ—¶æ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚




åœ¨trainingæ ‡ç­¾é¡µçš„HyperPodPytorchJob Recipesä¸­ï¼Œç›®å‰æœ‰ä¸€ä¸ªå­é¡µé¢æ˜¯ï¼šLlamaFactory Recipeã€‚ç°åœ¨æˆ‘å¸Œæœ›åœ¨æ—è¾¹å¢åŠ ä¸€ä¸ªverl Recipeçš„æ ‡ç­¾é¡µã€‚

> è¯·æ£€æŸ¥ä¸€ä¸‹ï¼Œä¹‹å‰çš„æ”¹åŠ¨æ˜¯å¦ç”Ÿæ•ˆäº†ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿæ˜¯å¦æœ‰æ”¹å˜åŸæœ¬çš„åŠŸèƒ½ï¼Œæˆ–è€…å½±å“äº†å…¶ä»–åŠŸèƒ½ã€‚


æˆ‘åœ¨ec2æœ¬åœ°éƒ¨ç½²äº†ä¸€ä¸ªå‰ç«¯é¡µé¢ï¼Œå‚è€ƒ/home/ubuntu/workspace/model-deployment-ui/start.shã€‚å¦‚æœæˆ‘ç›´æ¥è¿è¡Œstart.shï¼Œé‚£ä¹ˆæ˜¯å¦å½“ec2æ–­å¼€sshè¿æ¥åï¼ŒæœåŠ¡ä¹Ÿå°±ä¸­æ–­äº†ï¼Ÿ
ä»è„šæœ¬å†…å®¹å¯ä»¥çœ‹å‡ºï¼Œå®ƒå¯åŠ¨äº†å‰ç«¯å’Œåç«¯æœåŠ¡ï¼ˆç«¯å£ 3000ã€3001ã€8081ï¼‰ï¼Œè¿™äº›è¿›ç¨‹éƒ½æ˜¯åœ¨å½“å‰ SSH ä¼šè¯ä¸­è¿è¡Œçš„ã€‚å½“ SSH è¿æ¥æ–­å¼€æ—¶ï¼Œä¼šè¯ç»“æŸï¼Œæ‰€æœ‰ç›¸å…³è¿›ç¨‹ä¹Ÿä¼šè¢«ç»ˆæ­¢ã€‚

## æ–¹æ¡ˆ1ï¼šä½¿ç”¨ nohup
bash
nohup ./start.sh > logs/app.log 2>&1 &

ç°åœ¨ï¼Œæˆ‘å¸Œæœ›å°†è¿™ä¸ªurlæš´éœ²åˆ°å…¬ç½‘ï¼Œæœ‰ä»€ä¹ˆç®€å•ï¼Œå®‰å…¨çš„æ–¹å¼å—ï¼Ÿ


å½“å‰é¡¹ç›®ä¸­çš„Training Historyéƒ¨åˆ†ï¼Œæ˜¯é€šè¿‡/home/ubuntu/workspace/model-deployment-ui/mlflow/get_training_history.pyæ¥è·å–ä¿¡æ¯çš„ã€‚ä½†æ˜¯çœ‹ä¸Šå»tracking server ä¿¡æ¯è¢«hard codeäº†ã€‚èƒ½å¦åœ¨uiä¸Šï¼Œæ¯”å¦‚refreshæŒ‰é’®æ—è¾¹ï¼Œå¢åŠ ä¸€ä¸ªé…ç½®æŒ‰é’®ï¼Œç„¶åè®©ç”¨æˆ·é…ç½®uriï¼ˆé»˜è®¤å€¼æˆ–è€…å‰ç«¯çš„é¢„å¡«å……å¯ä»¥æ˜¯å½“å‰çš„è¿™ä¸ªï¼‰ï¼Œå¯è¡Œå—


å¦å¤–ï¼Œåœ¨å½“å‰HyperPodPytorchJob Recipesä¸­ï¼Œæœ‰äº†ä¸¤ä¸ªrecipesï¼Œæˆ‘å¸Œæœ›åœ¨LlamaFactory Recipeä¹‹å‰å¢åŠ ä¸€ä¸ªTorch Recipeï¼Œç„¶åæ‰€æœ‰çš„åŠŸèƒ½éƒ½è·ŸLlamaFactory Recipeå®Œå…¨ä¸€æ ·ã€‚
åªæ˜¯Entry Script Pathæ”¹æˆEntry Python Script Pathï¼Œä»¥åŠå†åé¢å¢åŠ ä¸€ä¸ªPython script parametersï¼ˆé¢„å¡«å……ä¸ºlearning_rate 1e5 \ --batch_size 1ï¼‰ã€‚


TBDï¼šeiifccvjtlgtkeivvhculeeciigljftvgjcclfjclhgd
eiifccvjtlgtceueicbjdjhhtekbkjdbdlefncjtedlf

urlæš´éœ²ï¼Œå†…ç½‘ç™»é™†
è®­ç»ƒéƒ¨åˆ†å¢åŠ MLFlowå…¼å®¹æƒ…å†µï¼ˆæ²¡æœ‰arnçš„æƒ…å†µï¼‰
è®­ç»ƒéƒ¨åˆ†å¢åŠ é…æ–¹ï¼š
    - llamafactoryé…æ–¹è‡ªåŠ¨è§£ædatasetï¼Œcutofflenå¹¶æäº¤è‡³training history
    - å¢åŠ verlé…æ–¹


/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.md è¯·åŸºäºè¿™ä¸ªè¯´æ˜ï¼Œå¿«é€Ÿäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ã€‚åªéœ€è¦ç†è§£ï¼Œæš‚æ—¶æ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

åœ¨Training tabé¡µï¼ŒTorch Recipeä¸­çš„Entry Python Script Pathå†…å®¹ï¼Œä½ éœ€è¦


åœ¨Training tabé¡µä¸­çš„LlamaFactory Recipeï¼Œæˆ‘éœ€è¦åšä¸€äº›ä¿®æ”¹ï¼š
1. Entry Script Pathå»æ‰ï¼Œç„¶åå¢åŠ 3é¡¹è¾“å…¥ï¼Œåˆ†åˆ«æ›¿æ¢æ¨¡æ¿/home/ubuntu/workspace/model-deployment-ui/templates/hyperpod-training-lmf-template.yamlä¸­çš„å¯¹åº”å­—æ®µï¼š
    - a. LlamaFactory Recipe Run Path(é»˜è®¤å€¼å…ˆå†™/s3/training_code/model-training-with-hyperpod-training-operator/llama-factory-project/)ï¼Œæ›¿æ¢æ¨¡æ¿ä¸­çš„LMF_RECIPE_RUNPATH_PHï¼›
    - b. LlamaFactory config YAML File Nameï¼ˆé»˜è®¤å€¼å…ˆå†™qwen06b_full_sft_template.yamlï¼‰ï¼Œæ›¿æ¢æ¨¡æ¿ä¸­çš„LMF_RECIPE_YAMLFILE_PHï¼›
    - c. DeepSpeed Json Config File Name(é»˜è®¤å€¼å…ˆå†™deepspeed_conf/ds_z0_config.json)ï¼Œæ›¿æ¢æ¨¡æ¿ä¸­çš„DS_JSONFILE_PHã€‚

å¤–ï¼Œå½“ç‚¹å‡»launch trainingä¹‹åï¼Œå¡«å……templateå¾—åˆ°çš„yamlæ–‡ä»¶ï¼Œé¡µå­˜å‚¨åˆ°template/ä¸­ï¼Œæ¯”å¦‚template/training/lma_$timestamp.yamlã€‚


## æ–¹æ³•1: åœ¨Macç»ˆç«¯ä¸­æŸ¥çœ‹å…¬ç½‘IP

åœ¨ä½ çš„Macç»ˆç«¯ä¸­è¿è¡Œï¼š
curl ifconfig.me
https://ifconfig.me/


è¯·åˆ†æä¸‹è¿™ä¸ªé¡¹ç›®/home/ubuntu/workspace/_DEV/hyperpod_elastic_agent-1.0.0ï¼Œä¸ºä»€ä¹ˆæˆ‘åœ¨è¿è¡Œè¿‡ç¨‹ä¸­æ€»æ˜¯æœ‰å¦‚ä¸‹æŠ¥é”™ï¼š
21:20:16[hyperpodpytorchjob-2-pods-0]Traceback (most recent call last): File "/usr/local/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi result = await app( # type: ignore[func-returns-value] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__ return await self.app(scope, receive, send) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/fastapi/applications.py", line 1054, in __call__ await super().__call__(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/applications.py", line 113, in __call__ await self.middleware_stack(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__ raise exc File "/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__ await self.app(scope, receive, _send) File "/usr/local/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__ await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app raise exc File "/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app await app(scope, receive, sender) File "/usr/local/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__ await self.middleware_stack(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/routing.py", line 736, in app await route.handle(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle await self.app(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/routing.py", line 78, in app await wrap_app_handling_exceptions(app, request)(scope, receive, send) File "/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app raise exc File "/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
21:20:16[hyperpodpytorchjob-2-pods-0] await app(scope, receive, sender) File "/usr/local/lib/python3.12/site-packages/starlette/routing.py", line 75, in app response = await f(request) ^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/fastapi/routing.py", line 302, in app raw_response = await run_endpoint_function( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/fastapi/routing.py", line 215, in run_endpoint_function return await run_in_threadpool(dependant.call, **values) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/starlette/concurrency.py", line 38, in run_in_threadpool
21:20:16[hyperpodpytorchjob-2-pods-0] return await anyio.to_thread.run_sync(func) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/anyio/to_thread.py", line 56, in run_sync return await get_async_backend().run_sync_in_worker_thread( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 2476, in run_sync_in_worker_thread return await future ^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 967, in run result = context.run(func, *args) ^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/hyperpod_elastic_agent/server/server.py", line 70, in _api_status log_state, log_rule_names = self._agent.get_log_agent_state() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/usr/local/lib/python3.12/site-packages/hyperpod_elastic_agent/hyperpod_elastic_agent.py", line 417, in get_log_agent_state for log_eval_result in self._log_agent.log_eval_results:
21:20:16[hyperpodpytorchjob-2-pods-0] ^^^^^^^^^^^^^^^ AttributeError: 'HyperPodElasticAgent' object has no attribute '_log_agent' INFO: 10.1.92.10:35326 - "GET /status HTTP/1.1" 200 OK



# è¯»å–å€¼
yq '.key' file.yaml
yq '.database.host' file.yaml

# è®¾ç½®å€¼
yq '.key = "new_value"' -i file.yaml

# æ·»åŠ æ–°é”®
yq '.new_key = "value"' -i file.yaml

# åˆ é™¤é”®
yq 'del(.key)' -i file.yaml

# åˆ—å‡ºæ‰€æœ‰é”®
yq 'keys' file.yaml



if MLFLOW_TRACKING_URI å­˜åœ¨ä¸”ä¸æ˜¯"":
    yq '.report_to = "mlflow"' -i $LMF_RECIPE_YAML_FILE

    å¦‚æœ$LMF_RECIPE_YAML_FILEæœ‰run_nameï¼š
        run_name_value=è¯»å–run_nameçš„å€¼
        å†™å›run_name: run_name_value_$(date '+%Y%m%d_%H%M%S')
    elseï¼š
        å†™å…¥run_name: run_$(date '+%Y%m%d_%H%M%S')
else:
    yq 'del(.report_to)' -i $LMF_RECIPE_YAML_FILE
    yq 'del(.run_name)' -i $LMF_RECIPE_YAML_FILE


if $LMF_RECIPE_YAML_FILEä¸­æ²¡æœ‰dataset_dirï¼Œæˆ–è€…dataset_diræ˜¯dataï¼š
    å†™å…¥dataset_dir: $LMA_RECIPE_LLAMA_FACTORY_DIR/data


Training Job Monitorä¸­ï¼ŒæŸ¥çœ‹å®Œæ•´æ—¥å¿—ï¼Œçœ‹ä¸Šå»ä¸æ˜¯æœ€æ–°çš„ã€‚å¯èƒ½æ˜¯å†™æ—¥å¿—çš„æ—¶å€™åˆ°æœ¬åœ°ï¼Œå‘ç°æœ‰æ–‡ä»¶äº†å°±æ²¡æœ‰è¦†ç›–ã€‚å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°å­˜å‚¨æ—¥å¿—æ—¶ï¼ŒåŠ ä¸Štimestampç­‰ä¿¡æ¯ã€‚


ç›®å‰çš„Training History tabé¡µé¢ä¸­ï¼Œæ˜¾ç¤ºçš„è¡¨æ ¼éœ€è¦åšäº›ä¿®æ”¹ã€‚éœ€è¦å°†mlflow run ä¸Šé¢çš„æ‰€æœ‰tagéƒ½æ˜¾ç¤ºå‡ºæ¥ã€‚å¦‚æœè¡¨æ ¼å®½åº¦ä¸å¤Ÿï¼Œå¯ä»¥å»æ‰train lossä»¥åŠlossã€‚ä»mlflowè·å–ä¿¡æ¯çš„é€»è¾‘æ˜¯åœ¨ï¼š
/home/ubuntu/workspace/model-deployment-ui/mlflow/get_training_history.pyã€‚æˆ‘è¯´æ¸…æ¥šäº†å—



/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.md è¯·åŸºäºè¿™ä¸ªè¯´æ˜ï¼Œå¿«é€Ÿäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ã€‚åªéœ€è¦ç†è§£ï¼Œæš‚æ—¶æ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

è¯·å‚è€ƒæˆ‘ç›®å‰éƒ¨ç½²æ¨¡å‹çš„æ–¹å¼ï¼š/home/ubuntu/workspace/model-deployment-ui/templates/vllm-template.yamlã€‚ç›®å‰æ˜¯éƒ¨ç½²äº†nlbå—ï¼Ÿ


åŸºäº/home/ubuntu/workspace/model-deployment-ui/templates/vllm-template.yamlè¿™ä¸ªéƒ¨ç½²æ–¹å¼ï¼Œç°åœ¨æˆ‘æœ‰ä¸ªéœ€æ±‚ã€‚
æˆ‘çš„é›†ç¾¤æœ‰3ä¸ªgpuèŠ‚ç‚¹ï¼Œä»–ä»¬éƒ½éƒ¨ç½²äº†åŒä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ä¸­node0ï¼Œ1æ˜¯æ¥å…¥äº†service/nlb0ï¼Œé€šè¿‡url0å‘ä¸šåŠ¡Aæä¾›æœåŠ¡ï¼›node2æ¥å…¥äº†service/nlb1ï¼Œé€šè¿‡url1å‘ä¸šåŠ¡Bæä¾›æœåŠ¡ã€‚
ä¸šåŠ¡Aæµé‡ç¨³å®šï¼Œä¸šåŠ¡Bæµé‡ä¼šæœ‰æ³¢åŠ¨ï¼Œçªå¢ç­‰ã€‚ç°åœ¨æˆ‘éœ€è¦åˆ¤æ–­ä¸šåŠ¡Bçš„æµé‡æƒ…å†µï¼Œå½“æµé‡é«˜äºæŸä¸ªthresholdä¹‹åï¼Œæˆ‘éœ€è¦å°†ä¸šåŠ¡Açš„ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œåˆ†å‡ºä¸€ä¸ªç»™ä¸šåŠ¡Bã€‚
ä½†æ˜¯è¯·æ³¨æ„ï¼Œå› ä¸ºä»–ä»¬éƒ½éƒ¨ç½²ä¸€æ ·çš„æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä¸å¸Œæœ›ç±»ä¼¼auto scalingçš„æ–¹å¼ï¼Œæ¥ä»ä¸šåŠ¡Aä¸­ç¼©å®¹ï¼Œè®©ä¸šåŠ¡Bæ‰©å®¹ï¼Œå› ä¸ºè¿™ä¼šæ¶‰åŠåˆ°æ¨¡å‹çš„å¸è½½/åŠ è½½ï¼Œéå¸¸è€—æ—¶ã€‚æˆ‘å¸Œæœ›èƒ½ç›´æ¥æŠŠæœåŠ¡Açš„èŠ‚ç‚¹æ¥å…¥æœåŠ¡Bï¼Œè€ŒèŠ‚ç‚¹ä¸Šçš„æ¨¡å‹éƒ¨ç½²/podä¿æŒä¸å˜ã€‚
ä½ ç†è§£æˆ‘çš„éœ€æ±‚äº†å—ï¼Ÿæ˜¯å¦æœ‰åŠæ³•å¯ä»¥å®ç°è¿™ä¸ªéœ€æ±‚ï¼Ÿ


åœ¨trainingçš„torch recipeä¸­ï¼Œå½“æˆ‘ç‚¹å‡»launch training jobæ—¶ï¼Œä¼šæœ‰popupé”™è¯¯å¼¹å‡ºï¼šTraining launch failed: LlamaFactory Recipe Run Path is requiredã€‚è¿™å¯èƒ½æ˜¯ä¹‹å‰ä»llamafacto
ryå¤åˆ¶çš„æ—¶å€™æ²¡æœ‰å®Œå…¨ä¿®æ”¹ã€‚è¯·å¸®æˆ‘æ£€æŸ¥ä¸€ä¸‹ã€‚

=== å»ºè®®çš„è§£å†³æ–¹æ¡ˆ ===
1. å°† containerd æ•°æ®ç›®å½•è¿ç§»åˆ° NVME å­˜å‚¨
2. æˆ–è€…é…ç½® containerd ä½¿ç”¨ NVME ä½œä¸ºé•œåƒå­˜å‚¨
3. æ¸…ç†ç°æœ‰çš„é•œåƒç¼“å­˜


ç°åœ¨æˆ‘å¸Œæœ›åœ¨å½“å‰çš„eksé›†ç¾¤é…ç½®fsxï¼Œæˆ‘å·²ç»åˆ›å»ºäº†fsxï¼Œä½†æ˜¯ä¸ç¡®å®šå®‰å…¨ç»„ç­‰æ˜¯å¦å¯ç”¨ã€‚
è¿™äº›æ˜¯ä»fsxL consoleä¸Šæ‹¿åˆ°çš„ä¿¡æ¯ã€‚
fs-0833c5da9d5da866d
us-west-2c
mr6gnb4v
vpc-08a5d58ba21a52c77
subnet-0b29cab84f8755af7
fs-0833c5da9d5da866d.fsx.us-west-2.amazonaws.com



è¯·å¸®æˆ‘çœ‹ä¸€ä¸‹è¿™ä¸ªè·¯å¾„ä¸‹çš„mdï¼š/home/ubuntu/workspace/production-stack/tutorialsï¼Œ00/01æˆ‘éœ€è¦é…ç½®å“ªäº›ï¼Ÿæˆ‘å·²ç»æœ‰ä¸€ä¸ªå¯ä»¥è¿è¡Œçš„EKS GPUé›†ç¾¤äº†ã€‚
## å¿…éœ€é…ç½®çš„æ­¥éª¤

### 1. å®‰è£… KubeRay Operator (00-b)
ç”±äºä½ å·²æœ‰EKSé›†ç¾¤ï¼Œåªéœ€è¦å®‰è£…KubeRay Operatorï¼š

bash
# æ·»åŠ  KubeRay Helm ä»“åº“
helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm repo update

# å®‰è£… KubeRay Operator
helm install kuberay-operator kuberay/kuberay-operator --version 1.2.0

# éªŒè¯å®‰è£…
kubectl get pods -A | grep kuberay-operator


### 2. éƒ¨ç½² vLLM Production Stack (01)
é€‰æ‹©é€‚åˆçš„Helmå®‰è£…æ–¹å¼ï¼š

bash
# æ·»åŠ  vLLM Helm ä»“åº“
helm repo add vllm https://vllm-project.github.io/production-stack

# ä½¿ç”¨æœ€å°é…ç½®éƒ¨ç½²
helm install vllm vllm/vllm-stack -f tutorials/assets/values-01-minimal-example.yaml


## è·³è¿‡çš„æ­¥éª¤

00-a, 00-c, 00-install-kubernetes-env.md - è¿™äº›éƒ½æ˜¯Kubernetesç¯å¢ƒå®‰è£…ï¼Œä½ å·²æœ‰EKSé›†ç¾¤å¯ä»¥è·³è¿‡

01-b-minimal-helm-installation-amd.md - é™¤éä½ çš„EKSèŠ‚ç‚¹æ˜¯AMDæ¶æ„ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨ç‰ˆæœ¬å³å¯

## æ¨èçš„é…ç½®æµç¨‹

1. éªŒè¯EKSé›†ç¾¤çŠ¶æ€ï¼š
bash
kubectl get nodes
kubectl get pods -A


2. ç¡®è®¤GPUæ”¯æŒï¼š
bash
kubectl describe nodes | grep nvidia.com/gpu


3. å®‰è£…KubeRay Operator (æŒ‰ç…§00-bæ–‡æ¡£)

4. éƒ¨ç½²åŸºç¡€vLLMæœåŠ¡ (æŒ‰ç…§01æ–‡æ¡£)

5. æ ¹æ®éœ€æ±‚é…ç½®é«˜çº§åŠŸèƒ½ (02-basic-vllm-config.mdç­‰)



/home/ubuntu/workspace/model-deployment-ui/KUBECTL_OPTIMIZATION_ANALYSIS.md è¯·åŸºäºè¿™ä¸ªè¯´æ˜ï¼Œå¿«é€Ÿäº†è§£ä¸€ä¸‹æ•´ä¸ªé¡¹ç›®ã€‚åªéœ€è¦ç†è§£ï¼Œæš‚æ—¶æ— éœ€å›å¤å…¶ä»–å†…å®¹ã€‚

ç°åœ¨ï¼Œåœ¨å‰ç«¯é¡µé¢ï¼Œæˆ‘éœ€è¦åœ¨Torch Recipeä¹‹å‰å¢åŠ ä¸€ä¸ªScript Recipe tabé¡µé¢ï¼Œéœ€è¦å¡«å……/home/ubuntu/workspace/model-deployment-ui/templates/hyperpod-training-script-template.yamlï¼Œ
infraæ–¹é¢çš„å¡«å……è·Ÿå…¶ä»–recipeä¸€æ ·ï¼Œå…¶ä»–éœ€è¦æœ‰ä¸ªproject pathï¼Œå¡«å……SCRIPT_RECIPE_PROJECTPATH_PHï¼Œä»¥åŠè„šæœ¬å…¥å£ï¼Œå¡«å……SCRIPT_RECIPE_ENTRYPATH_PHã€‚


